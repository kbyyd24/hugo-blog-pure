[{"categories":null,"content":"surface 升级到 Windows 11 之后，开始出现莫名其妙的屏闪。在更新驱动也不能改善之后，我还是选择了重装系统。虽然保留了个人数据，但是所有的应用都没有了，不得不重新安装。这里就记录一下那些不管重装多少次系统我都会安装的软件。 Windows 应用 ","date":"2022-02-01","objectID":"/windows-setup/:0:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"Windows Terminal 首先是安装 Windows Terminal。作为一个程序员，当然是无法离开命令行的。而 Windows Terminal 作为巨硬自己开发的工具，是我在 Windows 上体验过的最棒的命令行工具。 可以直接从微软商店下载正式版，也可以从 GitHub 上面下载 preview 版本，甚至可以下载源码自己编译。 打开后，使用快捷键 Ctrl-, 就能打开图形化的设置页面。为了能有更好的体验，建议做出下面这些修改 修改 启动 \u003e 默认中端程序，选择 Windows Terminal 在 powershell 的配置中，把光标修改成实心框。默认的条形光标在 vim 的 visual 模式下非常难以辨认选择了哪些字符 ","date":"2022-02-01","objectID":"/windows-setup/:1:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"scoop 既然讲到了 Windows Terminal，那么就继续介绍命令行的工具。 这里要介绍的就是 scoop 这个软件包管理工具。类似于 mac 下的 homebrew，scoop 可以帮助我们管理很多的软件包。我们可以通过它来安装 git、curl 等常用的 Linux 命令。甚至一些 Modern Unix 工具也能通过它来安装。 另外，scoop 还能用来安装 java，并且可以切换多个版本。这在一定程度上弥补了 sdkman 无法安装在 Windows 上的遗憾。 除了 scoop，我们在 Windows 上还有一个可选方案就是 chocolatey。 由于重装系统时选择的是保留个人文件，而 scoop 的文件都保存在 ~/scoop 目录中，所以会遇到 “执行 scoop 命令提示命令不存在，安装 scoop 又会提示 scoop 已经安装” 的问题。这就需要手动删除一下 ~/scoop 目录重新安装才能解决 ","date":"2022-02-01","objectID":"/windows-setup/:2:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"oh-my-posh oh-my-posh 是一个主题引擎，可以为 bash、zsh 等配置漂亮的主题。当然，powershell 也能够配置。 我们按照官网的介绍，执行下面的命令就可以安装 Install-Module oh-my-posh -Scope CurrentUser 接着，编辑 $PROFIEL 文件，加入一下内容，就可以在下一次启动 powershell 的时候看到美化后的命令行 Import-Module oh-my-posh oh-my-posh --init --shell pwsh --config C:\\Users\\kbyyd\\.oh-my-posh\\themes/powerlevel10k_modern.omp.json | Invoke-Expression ","date":"2022-02-01","objectID":"/windows-setup/:3:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"VS Code 不必多说，VS Code 绝对是必不可少的编辑器，甚至这篇博客的大部分内容都是在 VS Code 里面完成的。并且在 WSL 中可以直接使用 code 命令打开文件或文件夹，十分方便。 ","date":"2022-02-01","objectID":"/windows-setup/:4:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"Obsidian Obsidian 是我用了半年的笔记软件，融合了现代笔记软件的许多优秀功能，比如对卡片记忆法的支持、笔记双联等。 我在众多竞品中选择它，还有这么几个原因： 基于 markdown 语法，几乎没有学习成本。如果能支持 asciidoc 就更好了 🐶 基于文件系统，所有的内容都保存在我的本地而不是云端，以后要迁移也很方便 可以利用云盘同步，不需要为多设备同步付费 强大的插件系统，除了做笔记，还能用来写日记、做日程安排、做任务管理等 由于 iOS 和 iCloud 的限制，我目前的使用体验算不算完美，但也非常不错了。我现在利用它的 annotator 插件，在 iPad mini 上阅读 PDF 还能做点笔记，然后在 Windows 或者 macOS 下面把这些内容整理到存储在 OneDrive 上的记录知识的 Vault 中。这个 Vault 就是我的知识库了。然后利用 syncthing 同步到 iCloud，就可以在 iOS 上看到整理后的笔记；利用 Folder Sync 同步 OneDrive 就可以在 Android 上阅读同样的内容。 总的来说，Obsidian 非常好的满足了我记录笔记、查阅笔记的需求。如果你不满意现在的笔记软件，Obsidian 值得一试。 ","date":"2022-02-01","objectID":"/windows-setup/:5:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"PowerToys PowerToys 又是一个巨硬开发的开源工具，它为 Windows 提供了一些小功能来方便我们的使用。 比如使用 FancyZones 来自定义窗口的布局、利用键盘管理器来修改键位和组合键、利用 PowerToys Run 来完成一些快速指令等。 当然，这些功能还比较简单，应付一些复杂的需求可能就无法应对。 比如键盘管理器就过于简单，我们可以用 AutoHotKey 来替换它，完成一些复杂功能。 再比如 PowerToys Run，它只能支持软件中列出来的插件，如果想要扩展，我们可以使用 Wox 来自定义各种扩展。 ","date":"2022-02-01","objectID":"/windows-setup/:6:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"CopyQ CopyQ 是一个粘贴板管理工具，可以存储复制过的所有文本和图片。在需要粘贴时，可以搜索所有内容寻找想要的内容。 这还只是它的基础功能。它还可以对所有复制的内容进行编辑、分组等操作，还有更多功能我没有探索过，但这却是是一个强大的软件，比 Windows 自带的 ⊞-v 强到不知道哪里去了。 ","date":"2022-02-01","objectID":"/windows-setup/:7:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"Enable AppContainer Loopback 科学上网是一个合格程序员的必备技能，但是在 Windows 上却有一个困难。那就是按照传统方式安装的程序可以成功使用配置好的科学上网工具上网，但是通过微软商店安装的应用就做不到。 这背后的原因是通过微软商店安装的 UWP 应用运行在一个叫做 AppContainer 的进程中，无法发送流量到本地，也就无法使用本地的代理了。 如果不嫌麻烦的话，可以考虑使用巨硬官方提供的方法，一步步的去修改注册表。 或者也可以使用 Enable AppContainer Loopback 这个软件来快速完成这个工作。为了方便，我把下载地址也放到这里了。 WSL 因为本身是 Linux，所以这些软件也适用于 Linux，其中一部分还适用于 macOS，可以作为这些系统软件选择的参考。 WSL 的启用和安装就不介绍了，网上已经有很详细的内容，我们直接来看我们要安装什么东西。 ","date":"2022-02-01","objectID":"/windows-setup/:8:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"升级 Ubuntu 我的 WSL 选择的是 Ubuntu（后悔了），被锁定到了 20.04 TLS 版本。而这个发布时间已经过去很久了，甚至新的 TLS 都快要来了，所以我还是决定先升级一下。 下面的步骤参考的是 https://askubuntu.com/a/1369686 这个回答。 sudo apt remove snapd # WSL 不支持 systemd，所以 snap 命令也无法成功执行，卸载即可 sudo apt update \u0026\u0026 sudo apt upgrade sudo sed -i 's/^Prompt=lts/Prompt=normal/g' /etc/update-manager/release-upgrades sudo do-release-upgrade 完成之后，WSL 会自动重启，Ubuntu 也就升级到了最新的 21.10 版本。 ","date":"2022-02-01","objectID":"/windows-setup/:9:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"Homebrew Homebrew 现在支持安装在 Linux 上面了。尤其是我们安装的是 Ubuntu，通过 apt 命令安装的软件包往往不是最新版，而 homebrew 正好解决了这个问题。接下来要介绍的几个软件都能够方便的通过 homebrew 安装最新版本。否则还需要到它们的 release page 去下载 .deb 文件来安装。 ","date":"2022-02-01","objectID":"/windows-setup/:10:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"Modern Unix Modern Unix 是有人整理的一些现代化的 unix 命令，是对传统的 unix 命令的优化，提供更便捷的使用方式和更美观的输出，可以极大的提高我们的效率。 下面是我选择的几个命令。 ","date":"2022-02-01","objectID":"/windows-setup/:11:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"bat bat 是一个 cat 命令的替代品，它可以给输出的文件内容染色，获得更好的阅读体验。除此之外，它还会输出文件名和行号，非常方便阅读。 另外，如果输出的文件是被 git 管理的，那么它还能展示出文件的哪些行被删除、新增、修改。 ","date":"2022-02-01","objectID":"/windows-setup/:11:1","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"lsd lsd 是一个 ls 命令的替代品，它可以输出更美观的内容。 还有另一个类型的命令叫做 exa。我之所以选择 lsd 是因为它和 ls 命令的参数更加接近，所以更容易熟悉起来。 因为平时习惯了敲 l 或者 ls ，所以我会在 ~/.zshrc 中配置一下 alias alias l=\"lsd -alh\" alias ls=\"lsd\" ","date":"2022-02-01","objectID":"/windows-setup/:11:2","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"delta delta 是一个 diff 命令的替代品，提供更容易阅读的输出。并且按照 readme 的介绍对 git 进行配置之后，git diff 也可以使用 delta 来进行输出。 不过遗憾的是，它似乎不能和我非常喜欢的工具 tig 结合起来。 ","date":"2022-02-01","objectID":"/windows-setup/:11:3","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"fd fd 是一个 find 命令的替代品，并且比 find 命令高效的多，可以更快地得到搜索结果。 ","date":"2022-02-01","objectID":"/windows-setup/:11:4","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"ripgrep ripgrep 是一个文本搜索工具，可以快速搜索指定文件或目录下地文件内容。 它和上面地 fd 命令一样，默认情况下都会忽略掉隐藏文件或者 .gitignore 配置地文件。所以用这两个命令来搜索工程项目中地代码引用会十分方便。 比如在一个 Spring 项目中，我们发现 TicketService 引用了 TicketRepository，如果我们想要在命令行快速找到 TicketRepository的路径的话，就可以使用 fd '^TicketRepository' 来搜索文件。如果我们想知道哪些类引用了 TicketService 的话，就可以使用 rg '\\.TicketService$' 来搜索。 ","date":"2022-02-01","objectID":"/windows-setup/:11:5","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"mcfly 如果我们想要执行某个之前执行过的命令，我们可以采用下面这些方式 在 zsh 下可以敲出想要的命令的前面几个字符，然后按方向键上，找到想要的命令 通过 Ctrl-r 搜索历史 通过 history | grep 来查找 这些方式也不是不能用，只是不够高效。mcfly 则提供了另一种方式：通过 Ctrl-r 搜索历史，并且列出多个可能的结果，方便选择。这比传统方式只能看到一个结果高效多了，完全不用因为看不到想要的结果而担心敲错了内容。 ","date":"2022-02-01","objectID":"/windows-setup/:11:6","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"cheat \u0026 tldr 很多命令行的命令，如果不常用的话，我们很难记住应该怎么使用。而且上面介绍的这些命令也需要一个熟悉的过程。如果忘记了一个命令应该如何使用，我们一般可以求助于 --help 参数或者 man。但是这些内容往往是枯燥的文档，冗长且没有重点。 cheat 和 tldr 就可以解决上面的痛点。比如我们不熟悉 tar 命令，我们就可以通过 cheat tar 或者 tldr tar 来查看常用的用法。 这两个命令的不同点在于，tldr 有更多的资源，几乎所有的命令都可以通过它来找到。相比之下，cheat 命令经常找不到想要的命令。 但是 cheat 提供了更多的灵活性：我们可以通过 cheat -e \u003ccommand\u003e 来编辑某个命令的笔记，就像它的名字一样，是属于我自己的小抄。这个小抄的地址，是由配置文件 $CHEAT_CONFIG_PATH:~/.config/cheat/conf.yml 指定的。我们可以选择把这个小抄放到自己的云盘上，就可以在多个设备共享这份小抄。 而 tldr 想要做到这一点，就需要去提交 PR，通过之后才能被使用。并且这个内容不止对你自己生效，还对所有用户生效。 所以结合使用这两个命令会是一个不错的选择。在 cheat 命令无法找到提示时，使用 tldr 查看命令，并且把内容保存到 cheat 中，方便以后使用。 ","date":"2022-02-01","objectID":"/windows-setup/:11:7","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"oh-my-zsh 当我安装好 Linux 系统后，第一件事情就是安装 zsh 并用它替换掉 bash。因为 bash 的功能实在是太少，而 zsh 正好弥补了这一点。 chsh -s /usr/bin/zsh 以 zsh 的自动补全功能为例，它可以少敲很多字符，提高效率。比如想进入目录 src/main/resources，我不需要敲完整的路径，只需要敲 cd s/m/r 然后按下 tab 键就可以自动补全了。如果这个路径上有多个符合的路径，那 zsh 会在对应的位置停下，我们可以再添加几个字符之后再按 tab 来补全。 有了 zsh 自然时不能少了 oh-my-zsh 的，它主要提供了命令行美化和插件管理的功能。 ","date":"2022-02-01","objectID":"/windows-setup/:12:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"主题 主题的话我会推荐 powerlevel10k。它默认的展示信息足够详细，非常方便我们直接获取信息，而不需要执行命令。比如下面的截图中，我们就能看到当前 git repo 的状态、上一个命令消耗的时间和当前的时间。另外，它还能显示 aws profile、k8s config 等信息。 当然，想要这样的效果，还需要安装 powerlevel10k 支持的字体。这些字体需要安装到 Windows 下，然后在 Windows Terminal 的配置中选择想要的字体。 ","date":"2022-02-01","objectID":"/windows-setup/:12:1","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"插件 oh-my-zsh 提供了很多内置插件，也有其他插件可以选择安装。这里推荐几个我常用的插件。 git 这个插件提供了常用的 git 命令的 alias，熟悉之后能够更快的执行这些命令 z 通过 z 命令可以快速跳转访问过的目录 zsh-autosuggestions 根据键入的字符匹配上一次执行过的命令并提示出来，如果是想要的命令，可以直接按方向右键补全 zsh-syntax-highlighting 用红色和绿色提示想要执行的命令是否存在 gradle 为 gradle 项目生成 task 列表缓存，方便使用 tab 补全 ","date":"2022-02-01","objectID":"/windows-setup/:12:2","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"tmux 我是从去年开始入坑 tmux 的，因为它能解决我的一个痛点，那就是不得不重启电脑的时候，打开的命令行会话就会丢失，需要靠人力一个个找回来。 tmux 本身并不解决这个问题，它实际上只是一个终端管理器，会启动一个 server 来管理会话，使用 client 连接 server 来使用 会话。而在我的使用场景中，server 也运行在我的笔记本上面，所以重启电脑也会关闭 server，会话也就丢失了。 好在，tmux 支持安装插件，而 Tmux Resurrect 插件就可以满足会话保存和恢复的需求。安装插件后，我们可以在重启电脑前使用 prefix + Ctrl-s 保存当前会话，重启电脑后使用 prefix + Ctrl-r 恢复会话。 如果你觉得每次重启前后都需要按键来操作比较麻烦的话，tmux-continuum 插件可以帮助到你。它可以每15分钟自动保存一次会话，并且在 server 启动时自动恢复会话。这样就省去了每次重启前后的操作。 ","date":"2022-02-01","objectID":"/windows-setup/:13:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"oh-my-tmux oh-my-tmux 之于 tmux，就像 oh-my-zsh 之于 zsh 一样，为 tmux 做了很多美化和扩展的工作。如果你使用 tmux，那么 oh-my-tmux 绝对值得一试。 安装 oh-my-tmux 之后，可以直接使用 prefix + e 来编辑配置文件，退出时 tmux 会自动更新配置。 现在让我来配置一下 oh-my-tmux。 首先是修改 prefix。tmux 默认的 prefix 是 Ctrl-b，在我看来这并不容易按到。并且我很容易随手 Ctrl-d 退出会话，所以为了避免意外地退出，我干脆把 prefix 绑定到了 Ctrl-d。 set -g prefix 'C-d' unbind C-b bind 'C-d' send-prefix 接着是让鼠标可以直接滚动我的终端。tmux 中默认情况下我们是不能使用鼠标滚动查看那些滚出屏幕地内容的。所以我们需要做一点配置来让我们可以使用鼠标滚动屏幕。 set-option -g mouse on bind -n WheelUpPane if-shell -F -t = \"#{mouse_any_flag}\" \"send-keys -M\" \"if -Ft= '#{pane_in_mode}' 'send-keys -M' 'select-pane -t=; copy-mode -e; send-keys -M'\" bind -n WheelDownPane select-pane -t= \\; send-keys -M 最后是配置插件了，在使用 oh-my-tmux 后，插件的配置方式有点不一样，需要用下面的方式来配置。 set -g @tpm_plugins ' \\ tmux-plugins/tpm \\ tmux-plugins/tmux-sensible \\ tmux-plugins/tmux-resurrect \\ tmux-plugins/tmux-continuum \\ ' ","date":"2022-02-01","objectID":"/windows-setup/:13:1","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"vim 配置 我并不是一个 vim 党，但是常年在命令行活动，难免不和 vim 打交道。 首先是插件，我一定会选择的是 vim-airline 和 vim-airline-themes。它们是用来在 vim 的底部展示一行信息，包括了当前的模式、编辑的文件名、文件的状态、文件的语言、所处的行等信息。并且有很多的主题配色可供选择。毕竟好看是第一生产力，谁能忍受原始的 vim 那个简陋的界面呢。 接着要让我们的鼠标能够控制 vim 滚动，否则就得靠 Ctrl-e 和 Ctrl-y 来滚动 set mouse=aif has(\"mouse_sgr\") set ttymouse=sgrelse set ttymouse=xterm2end 最后是打通 vim 的注册器和系统的粘贴板，WSL 中的配置有所不同 autocmd TextYankPost * call system('echo '.shellescape(join(v:event.regcontents, \"\\\u003cCR\u003e\")).' | clip.exe')\" for macOS\"set clipboard=unnamed ","date":"2022-02-01","objectID":"/windows-setup/:14:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"direnv direnv 可以在我们进入到某个目录的时候自动帮我们设置一些环境变量，这对于用来工作的电脑来说非常实用。 我会在项目的根目录下创建一个 .envrc 文件，在里面可以配置 gradle 的缓存目录、kubernetes config 文件路径、aws profile 等环境变量。这样在进入这个目录或它的子目录后，都可以使用这些配置。而在离开这个目录的时候，又会自动 unset 这些环境变量，采用默认值。 这些就是我目前在使用的一部分工具，把它们记录在这里，下次重装系统就不用担心忘掉什么了。 ","date":"2022-02-01","objectID":"/windows-setup/:15:0","tags":["wsl"],"title":"重装了 Windows，这些东西又得装一遍","uri":"/windows-setup/"},{"categories":null,"content":"在我们日常的 TDD 开发中，永远绕不过去的就是要编写测试。而对于一个 Java 程序员，JUnit 似乎是一个不二的选择。它的确是一个十分优秀的工具，在大多数情况下都能够帮助我们完成测试的工作。 但是在开发过程中，我发现 JUnit 并不总是那么好用。它在一些情况下需要耗费挺多精力才能编写出让人满意的测试。 JUnit 不擅长的事情 一个让人满意的测试，应该能够清晰的体现被测试的目标、测试的目的以及测试的输入输出，并且应遵循 DRY 原则，尽可能的减少测试中的重复内容。 JUnit 可以通过设计测试方法名和组织方法内的代码的方式清晰的表达意图，也可以通过参数化测试来减少相同测试目的的测试代码重复。但是它在这些地方都做的不够好。 ","date":"2021-11-26","objectID":"/junit_alternative/:0:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"清晰表达测试的目的 在使用 JUnit 时，清晰的表达测试意图并不是总能做到的事情。这主要体现在两个方面。 ","date":"2021-11-26","objectID":"/junit_alternative/:1:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"如何命名测试方法 第一个体现就是在使用 Java 编写测试时，采用什么样的命名风格来命名测试。是为了代码风格的统一而选择驼峰？还是为了更高的可读性选择下划线？这个问题在不同的项目中有不同的实践，看起来是没有一个统一的认识。 而这个问题的根源是 JUnit 的测试名称是 Java 的方法名，而 Java 的方法名又不能在其中插入空格。所以除了下面要介绍的两种测试工具外，采用 Kotlin 来编写 JUnit 也是一种方式。 ","date":"2021-11-26","objectID":"/junit_alternative/:1:1","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"如何组织方法内的代码 第二个体现就是 JUnit 对测试方法内部如何编写没有强制的规定。这就意味着我可以在测试里面任意地组织代码，比如在一个测试方法里面对一个方法调用多次并验证每一次的结果，或者把调用测试目录的逻辑和准备数据的逻辑以及验证逻辑混合到一起。 总之这样的结果就是测试方法内的代码组织方式千奇百怪，每当阅读别人编写的测试的时候，总是要花上好几分钟才能知道这些代码到底在干什么。 对于这个问题，我个人会选择使用注释来标注一下 given 、 when 、 then，并且给 IDEA 设置了 live template 方便插入它们。 ","date":"2021-11-26","objectID":"/junit_alternative/:1:2","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"又不是不能用的参数化测试 如果说不能清晰地表达测试意图这个问题还有一些 workaround 可以绕过去的话，JUnit 那仅仅是能用的参数化测试功能就没有什么好办法可以绕过去了。 JUnit 提供了各种 Source 注解来为参数化测试提供数据，但是这个功能实在是太弱了，很难让人满意。 难以让人满意的第一个原因是，各种 Source 注解基本上只能支持 7 种基本类型再加上 String， Enum 和 Class 类型。如果想要使用其他类型的实例作为参数的话，就必须要使用 MethodSource 或者 ArgumentsSource 注解。 这就导致了第二个原因：这两个注解需要单独写一个静态或一个 ArgumentProvider 的实现，这就导致很难把测试参数写到测试代码旁边。并且 Arguments.of() 方法并不利于阅读测试参数。 MethodSource 要求使用静态方法，这在使用 Kotlin 编写 JUnit 时需要把这些方法写到 companion object 里面，并且加上 @JvmStatic 注解。因为 Kotlin 里面没有 static 关键字。 这两点导致测试的可读性下降。而按照“测试即文档”的原则，我们应该尽力去保证测试的可读性。 第三个原因则是来自 ParameterizedTest 注解。它的 name 字段可以使用参数的索引值来把参数填入模板中，生成更加可读的测试名称。 但是它的功能也仅限于此了。因为这个模板只能使用索引值，不能使用索引后再调用里面的方法或者字段。所以如果我们的参数是一个复杂对象，那么一定要重写 toString 方法才能得到满意的输出。但是这又违背了编写测试的原则之一——不能为了测试而添加实现代码。 如果我们一定要得到一个更加表意的测试名称，那么添加一个专用的测试参数也能做到。但是这又会导致 IDE 或者构建工具的警告，因为它们认为这个参数没有被使用。 总之，尽管 JUnit 可以解决绝大多数问题，但是在这么几个小地方却做的不是那么完美。 那么有没有什么工具可以作为 JUnit 的替代呢？当然是有的。下面我将按照我接触的顺序来介绍两种种测试框架。可以在 GitHub 上找到下面例子的完整代码。 使用 Spock 作为测试框架 Spock是一个用 Groovy 编写的测试框架，按照 given/when/then 的结构定义 dsl，能够让测试更加的语义化。它的一大特点是 Data Driven Test，可以方便的编写参数化测试。 我曾在两个项目上尝试过使用 Spock 作为测试框架，几乎没有遇到过无法解决的问题。 ","date":"2021-11-26","objectID":"/junit_alternative/:2:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"如何使用 Spock 由于 Spock 是使用 Groovy 来编写测试的，所以我们要使用它时，除了要引用它本身，还需要添加对 Groovy 的支持。以 gradle 为例： plugins { id 'groovy' } dependencies { testImplementation 'org.spockframework:spock-core:2.0-groovy-3.0' testImplementation 'org.spockframework:spock-spring:2.0-groovy-3.0' } 第二个依赖提供了一些对 Spring 的支持，比如可以使用 @SpringBean 注解来让被 mock 的对象注入到测试的容器中。 我们先看一个最简单的例子： class MarsRoverSpockTest extends Specification { // 1 def \"should return mars rover position and direction when mars rover report\"() { //2 given: // 3.1 def marsRover = MarsRoverFixture.buildMarsRover( position: new Position(1, 2), direction: Direction.EAST, ) when: // 3.2 def marsRoverInfo = marsRover.report() then: // 3.3 marsRoverInfo.position == new Position(1, 2) marsRoverInfo.direction == Direction.EAST } } 每一个测试都需要继承抽象类 Specification 可以使用字符串来命名测试 Spock 定义了一些 block，这里的 given 、 when 、 then 都是 block。 given block 负责测试的 setup 工作 when block 可以是任意代码，不过最好是对测试目标的调用。它总是和 then 一起出现 then block 用来断言。这里不需要任何的 assertion，只需要编写返回值是 boolean 的表达式即可 Spock 有非常友好的测试报告输出。如果我们把上面的断言特意改错，就能得到这样的测试输出： Condition not satisfied: movedMarsRover.position == movedPosition | | | | | | | Position(x=-2, y=2) | | false | Position(x=-1, y=2) MarsRover(position=Position(x=-1, y=2), direction=WEST) 在这个输出里面，我们可以清晰的看出表达式两端的值是什么，非常便于 debug。 接下来再看一个和 Spring 集成的简单例子： @SpringBootTest class MarsRoverServiceSpockTest extends Specification { @SpringBean MarsRoverRepository repository = Mock() // 1 @Autowired MarsRoverService service def \"should move mars rover forward success\"() { given: def id = \"id\" def marsRover = MarsRoverFixture.buildMarsRover( position: new Position(1, 2), direction: Direction.NORTH, ) repository.findById(id) \u003e\u003e marsRover // 2 when: service.forward(id, 3) then: 1 * repository.save({ it.position == new Position(1, 5) }) // 3 } } 使用 SpringBean 注解注入到测试的上下文中； Mock 是 Spock 提供的 mock 方法 为 mock 对象的方法调用设置返回值 验证 mock 对象的方法被调用的次数和参数 ","date":"2021-11-26","objectID":"/junit_alternative/:3:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"特点 语义化的结构 在前面的例子中，我们看到了 block 的概念。它可以帮助我们更好的组织代码结构，写出更加便于阅读的代码。其实在每一个 block 声明之后，我们还可以在添加一个字符串，达到注释的作用。比如： given: \"a mars rover at position 1,2 and direction is north\" 除了上面的例子里看到的，Spock 还提供了 cleanup 、 expect 、 where 这三个 block。详细信息可以看看它的文档 简洁的断言 在上面的例子中，我们看到 Spock 的断言十分简洁，不需要像使用 assertj 一样写很长的 assertThat(xxx).isEqualTo(yyy)，只需要一个返回 boolean 的表达式就可以了。 甚至可以把多行断言提取到一个方法中，返回他们与运算的结果。 使用 data table 构造参数化测试 对于参数化测试，我们再来看一个例子。 def \"should move mars rover from position 1,2 forward to #movedPosition.x,#movedPosition.y when direction is #direction and move length is #length\"() { given: def marsRover = MarsRoverFixture.buildMarsRover( position: new Position(1, 2), direction: direction, ) when: def movedMarsRover = marsRover.forward(length) then: movedMarsRover.position == movedPosition movedMarsRover.direction == direction where: direction | length || movedPosition Direction.EAST | 2 || new Position(3, 2) Direction.WEST | 3 || new Position(-2, 2) Direction.SOUTH | -1 || new Position(1, 3) Direction.NORTH | 1 || new Position(1, 3) } 我们可以看到代码的最后一段是一个 where block，这是 Spock 中用来定义数据测试的数据的地方。例子中的写法被称作 data table。尽管 Spock 还支持一些其他的写法，但是我个人认为 data table 是一个更加可读的写法，所以这也是我最常使用的写法。并且这个写法不需要手动调整格式，IDEA 支持自动 format，堪称完美。 我们还可以留意一下方法名。方法名中有几个以 # 开头的字符串，它们其实是在引用 data table 中定义的变量。这种通过变量名引用的方式可读性远远大于 JUnit 的索引值的方式。并且我们可以看到 #movedPosition.x 这样的表达式，它们可以直接使用这些对象中的字段值来生成方法名，不需要依赖于对象的 toString 方法。 它的测试输出也非常便于定位失败的测试数据。 使用 Groovy 编写测试 使用 Groovy 来编写测试，可以说既是优点，也是缺点。 优点是在于 Groovy 是动态语言，我们可以利用这一特性在测试中少写一些啰嗦的代码。并且在前面的例子中，断言里面获取的 marsRover.position 字段本身是 private 字段，测试仍然可以正常执行。这些都是由 Groovy 带来的灵活性。 缺点在于这是一个相对小众的语言。如果不是因为 Gradle，或许不会有多少人熟悉它的语法。这也会导致人们在选择它时会变得更加谨慎。 与 IDE 的完美集成 我一直使用的 IDEA 是能够完美适配它的。除了前面提到的 format data table ，最主要的是 IDEA 能像执行 JUnit 一样执行它，并且不需要任何的配置。 ","date":"2021-11-26","objectID":"/junit_alternative/:4:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"缺点 我在第一个项目里面使用 Spock 时，几乎没有发现它有什么缺点，以至于在后来的项目中总是在问 TL 能不能把它加到项目里来 🤪 但是后来在一个 Kotlin 项目中尝试使用它时，却遇到一些问题。 与 Kotlin 的集成问题 无法识别 Kotlin 的语法糖 Groovy 不能直接识别到 Kotlin 代码中的各种语法糖，这就让测试写起来有那么一点点不舒服。 比如命名参数。其实 Groovy 也支持命名参数，但是语法和 Kotlin 不同。这就显得有一点尴尬。不过这个问题可以通过为测试编写一些 fixutre 之类的代码来帮助处理这里问题。比如下面这个 Kotlin 类型 data class MarsRover( private val position: Position, private val direction: Direction, ) 我们可以为它编写一个 fixture，就能在测试里面也使用命名参数了 class MarsRoverFixture { @NamedVariant static MarsRover buildMarsRover(position, direction) { new MarsRover(position, direction) } } 其他的一些语法问题也基本都能绕过，本质思路就是把要测试的代码想象成编译后的 Java，这样就能找到绕过的办法。 没有对 final class 的 mock 支持 这是一个基本绕不过去的问题。Kotlin 里面的类型默认都是 final，不能再被继承。但是 Spock 的 mock 却需要为要 mock 的对象的类型创建一个子类。这就导致我们不能去 mock 那些类型。其实这个问题不是 Spock 特有的，Mockito 也有这个问题。只不过在使用 JUnit 时我们会选择用 MockK 作为 Kotlin 项目的 mock 工具，而不是 Mockito。 解决这个问题的策略有好几个： 尽可能不去 mock。这要求我们设计出更容易测试的代码，这样就可以避免在测试中使用 mock。 使用 Spring 的 Kotlin 插件 org.jetbrains.kotlin.plugin.spring[](org.jetbrains.kotlin.plugin.spring) 。它可以把 Spring 的 component 类都修改成 open，这样就能 mock 了 在写这篇文章的时候，发现一个很久没有更新的仓库 kotlin-test-runner，也许可以借鉴一下这里的思路来解决这个问题。 与 JUnit 的兼容问题 对于上一个问题，我们当时还有一个 workaround，那就是使用 JUnit5 + MockK 来编写那些需要 mock 的测试。但是那个时候的 Kotlin 版本还比较低，没有遇到和 JUnit 的兼容问题。 兼容问题是 JUnit 在编写 Spring 集成测试的时候，如果有 mock bean 的需求，需要使用 springmock 里面的 @MockkBean 注解。但是从 kotlin 1.5.30 开始，这个库就不能和 Spock 编写的 Spring 集成测试兼容，会出现 NPE 问题。这个问题在使用 Kotlin 对 Specification 子类进行反射时会出现。如果对这个问题感兴趣，可以看看这个 issue 。 Groovy 语言的学习成本 就像前面提到过的，使用 Groovy 还是有一些学习成本的。如果团队里没有熟悉它的人，可能会走一点弯路。 使用 Kotest 作为测试框架 Kotest 是在无意中发现的测试框架，还没有在实际的项目中实践过。所以这里只能分享一下如何使用，没有什么经验分享。 ","date":"2021-11-26","objectID":"/junit_alternative/:5:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"如何使用 Kotest Kotest 的测试目标是 Kotlin 代码，所以它除了支持 Java 以外，还支持用 Kotlin 编译的 JS。不过这里，我们就只试试 JVM 平台就好。 dependencies{ testImplementation 'io.kotest:kotest-runner-junit5:5.0.1' testImplementation 'io.kotest:kotest-assertions-core:5.0.1' } test { useJUnitPlatform() } 这样我们就能使用 Kotest 最基础的功能了。接着我们来看一个例子。 class MarsRoverKotestTest : BehaviorSpec({ given(\"a mars rover\") { val marsRover = MarsRover( position = Position(1, 2), direction = Direction.EAST, ) `when`(\"it report information\") { val (position, direction) = marsRover.report() then(\"get it's position and direction\") { position shouldBe Position(1, 2) direction shouldBe Direction.EAST } } } }) 这是一种 BDD 风格的测试。Kotest 使用 BehaviorSpec 类封装起来。 在 then 中，我们没有看到常见的 assertThat() 语句，取而代之的是 Kotest 的 assertion 库提供的方法。 接着我们再看一个 Spring 集成测试的例子，这需要多引入一个依赖 io.kotest.extensions:kotest-extensions-spring @SpringBootTest class MarsRoverServiceKotestTest( service: MarsRoverService, ) : FunSpec() { @MockkBean lateinit var repository: MarsRoverRepository override fun extensions() = listOf(SpringExtension) init { test(\"should move mars rover success\") { // given val id = \"id\" val marsRover = MarsRover( position = Position(1, 2), direction = Direction.NORTH, ) every { repository.findById(id) } returns marsRover val marsRoverSlot = slot\u003cMarsRover\u003e() every { repository.save(capture(marsRoverSlot)) } just Runs // when service.forward(id, 3) // then val movedMarsRover = marsRoverSlot.captured movedMarsRover.report().position shouldBe Position(1, 5) movedMarsRover.report().direction shouldBe Direction.NORTH } } } 在这个例子中，除了 override fun extensions() 那一行以外，其他部分的代码和 JUnit 的集成测试几乎没有区别。而这一行就好像 JUnit 中的 @ExtendWith(SpringExtension) 一样。我们甚至可以抽出所有的集成测试代码到单独的 sourceSet 中对 extensions 进行全局配置。Spring | Kotest ","date":"2021-11-26","objectID":"/junit_alternative/:6:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"特点 丰富的测试风格支持 除了上面的例子，我们还有很多的测试风格可以选择，这在它的文档中有介绍：Testing Styles | Kotest 简洁的断言 在上面的例子里面我们看到，Kotest 提供了自己的断言库，不需要再写冗长的 assertThat() 之类的语句 使用 Kotlin 编写，能与 Kotlin 项目完美结合 使用 Kotlin 来编写测试，可以使用到 Kotlin 里面的各种语法糖。这样就不用像 Spock 一样在语法切换中挣扎。 支持 MockK 同样的，因为 Kotest 的测试使用 Kotlin 编写，自然是支持 MockK 的。这样就能利用 MockK 的特性，支持对 final class 的 mock。 与 JUnit 兼容 因为 Kotest 是基于 JUnit 平台的，所以是能和 JUnit 兼容的，不会出现上面的 Spock 那样的问题。 对 data driven test 的支持 Kotest 提供了扩展来支持 data driven test。当然，不使用这个扩展也可以进行，比如用 list 构造好数据之后 foreach 创建测试。不过这里的例子我们还是使用这个扩展来演示。（需要添加依赖 io.kotest:kotest-framework-datatest ） class MarsRoverKotestTest : FunSpec({ context(\"data test\") { withData( nameFn = { (direction, length, position) -\u003e \"should move mars rover from 1,2 to ${position.x},${position.y}when direction is $directionand move length is $length\" }, Triple(Direction.EAST, 2, Position(3, 2)), Triple(Direction.WEST, 3, Position(-2, 2)), Triple(Direction.SOUTH, -1, Position(1, 3)), Triple(Direction.NORTH, 1, Position(1, 3)), ) { (direction, length, movedPosition) -\u003e val marsRover = MarsRover( position = Position(1, 2), direction = direction, ) val movedMarsRover = marsRover.forward(length) movedMarsRover.report().position shouldBe movedPosition movedMarsRover.report().direction shouldBe direction } } }) 虽然这个 data driven test 相对于 Spock 的 data table 来讲没有那么直观，但是对比 JUnit 的话，能够方便的自定义测试方法名，并且测试数据与测试代码放在一起，已经算是一个巨大的进步了。 ","date":"2021-11-26","objectID":"/junit_alternative/:7:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"缺点 因为没有在实际的项目中实践过，所以目前没有发现很多的缺点。 与 IDEA 和 Gradle 的集成不够完美 这个问题的表现是在 IDEA 里面无法执行单个测试方法。但是细究后发现，实际上是和 gradle 的集成不够好。 默认情况下，IDEA 会使用 gradle 来执行测试。执行单个测试的命令是 gradle test --tests \"xxx.Class.yyyMethod\"。对于 JUnit，这里的 class 和 method 是很直观的类名和方法名。但是 Kotest 的写法却不是编写类里面的方法，而是调用方法生成测试。所以 gradle 的这个命令就没有办法生效，也就没有办法只执行一个测试方法了。 在把 IDEA 的配置更新成使用 IDEA 来运行测试后，在 mac 上能够正常执行一个测试方法。但是在 Windows + WSL 的环境上却会出错。（看起来像是 IDEA 的问题，因为它尝试把 WSL 里面的 java 当作 Windows 的程序执行） 不要使用 Spek 前面介绍了两种值得一试的测试框架，这里再介绍一种不建议使用的框架。 当初想要尝试这个框架，是因为看到有网友说这是 Kotlin 版本的 Spock 🙃。但是实践下来并没有发现它有和 Spock 类似的功能，并且还出现了这些痛点： 与其他测试框架混合使用的问题 当与其他测试框架混合使用时，Spek 测试总是会先执行。哪怕我们在 IDEA 里面只想执行一个 JUnit 的单元测试，Spek 也会先把自己的所有测试跑完，然后才会执行到我们想要执行的测试。这就意味着在 Speck 测试编写了很多之后，执行其他测试就会等待很久。 不能编写 Spring 集成测试 我在写 demo 的时候发现，它的 IDEA 插件在 Windows 上面无法工作 如果这些痛点你都能忍，那我也不建议使用这个框架，毕竟上面已经有更好的选择了。 总结 现在我们有了两个 JUnit 以外的测试框架选择。当然它们也不是完美的，JUnit 仍然是那个最稳定、风险最低的那一个。但如果你想尝试一下这两个框架的话，可以考虑一下这些方面： 生产代码的编程语言 如果是 Kotlin，那么可以考虑 Kotest，不要考虑 Spock 如果是 Java，那么这两个都值得考虑 语言熟悉程度 Kotlin 明显是比 Groovy 更加流行，这个角度考虑的话 Kotest 是更优的选择 测试框架的流行程度（这方面我不知道有什么评价标准，只是作为参考） 两个框架在 GitHub 上的 star 数量半斤八两，一个 3.1k，一个 3.2k（JUnit 也才 4.9k） 在 MVNRepository 上，Spock 的 usage 明显高于 Kotest IDEA 的集成 Spock 在这方面完全没有问题 Kotest 需要安装插件，并且在 Windows + WSL 的模式下不能运行单个测试 Gradle 集成 Spock 完美集成 Kotest 不能执行单个测试 ","date":"2021-11-26","objectID":"/junit_alternative/:8:0","tags":["JUnit","Spock","Spek","Kotest","测试"],"title":"JUnit 不好用？也许你可以试试这些测试工具","uri":"/junit_alternative/"},{"categories":null,"content":"刚开始工作的时候就接触到 DDD，当时就被它规定的代码模式所吸引。几年下来，接触到不同的代码组织模式，更是深深的感受到一个“良好”的代码架构是多么重要。 Note “良好”的代码架构，应该让人在添加新代码时不繁琐和迷茫，应该让人在修改代码时不容易遗漏，应该让人能够准确的找到某个业务对应的代码。 以这样的眼光来看，传统的分层架构就难以做到这几点，而一些以领域为核心的架构则相对容易一些。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:0:0","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"传统的分层架构如何组织代码 我们先来看看分层架构是如何组织代码的 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:1:0","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"分层架构有哪几层 在传统的分层架构中，代码被分成了 API、Service、Repository 这样三层，每一层处理不同的事情。 API 层 API 层处理和 HTTP 协议交互的部分。如果使用 Spring 框架，那么也会把这一层叫做 Controller 层。这仅仅是因为 Spring 中使用 @Controller 作为注解而得名。 Repository 层 这一层主要负责和存储技术打交道。存储技术往往就是数据库。这一层也有很多的框架可以利用，比如 Hibernate、JOOQ、MyBatis。 Service 层 Service 层处理和业务相关的事情。虽然名义上是这样说，但实际上所有我们认为不应该交给 API 层和 Repository 层的代码，都会被放到 Service 层。这样的结果就是 Service 层中的代码逐渐变大，最终没人能够讲清楚一个 Service 层里的方法到底做了多少事情。 因为所有的代码都在 Service 层，所以 Service 层的代码就会变得冗长，很容易出现一个方法职责过多的问题。但是单一职责原则又会在程序员的脑海中回响，逼迫程序员做重构。而这时的重构往往就是抽方法。然后一个类中的方法就会越来越多，并且在一个类中处理了不同的事情。于是单一职责原则又开始回响，程序员又不得不再次重构。这时，往往就会出现“没有什么问题不是多加一层不能解决的”。于是在 Service 层中就出现和各种各样的层，当我们阅读代码的时候往往就会迷失在其中，逐渐搞不清楚自己要添加的代码应该放在哪里。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:1:1","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"层与层之间的依赖关系 在分层架构中，层级之间的依赖关系是向下的，如下图所示： 我们可以看到，在这样的代码组织方式中，依赖的最终指向是数据库。也就是说，数据库成为了系统的核心。但是我们开发一个系统，应该是围绕着要解决的问题来编写代码，数据库只是一个存储数据的地方而已。在这样的组织方式中，代码围绕这数据库而设计，当数据库发生变化时，代码就会发生变化。但是这些代码并没有把业务代码与数据库代码解耦，所以修改起来就像是在整理从口袋里拿出来的耳机线一样难受。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:1:2","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"以领域为核心的架构们如何组织代码 面对传统的分层架构的问题，人们提出了一些新的架构模式。这些模式中，有三种在今天被大家所熟知。它们分别是洋葱架构、六边形架构（又叫做端口-适配器架构）、整洁架构。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:2:0","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"三种以领域为核心的架构 接下来，我们先来看看这三种架构是什么。 Onion Architecture 洋葱架构的原文可以在原作者的博客中找到。作者一共用了四篇文章来解释洋葱架构是什么。 我们可以简单的总结一下洋葱架构的特点： 以 Object Model 为核心， Object Model 不依赖 Object Model 以外的任何概念 围绕着 Object Model 的是 Object Service（或者叫做 Repository），提供 Object Model 的查询和保存 围绕着 Object Service 的是 Application Service，负责对外提供接口，也会包含一些业务逻辑 洋葱架构要求了代码的依赖关系，如下图： 图中，蓝色箭头代表了依赖方向，橙色部分圈出的是应用的核心。而数据库之类的代码，则放到了 infrastructure 里面。 Port Adapter Architecture 我在之前的文章中介绍过六边形架构。它的特点是把与外部技术实现相关的代码隔离到适配器中，在应用核心代码中声明端口，这些适配器去实现端口，从而把与核心业务无关的技术实现从核心代码中解耦出来。 六边形架构与洋葱架构的相同点是，它们都把与核心业务无关的技术实现代码从应用的核心代码中解耦出来。 它们的不同点是，六边形架构只约束了如何把这些技术实现解耦出来，而没有约束如何组织核心业务代码。洋葱架构则对此做了一些约束，定义了 Application Service、Object Service 和 Object Model，并明确了它们之间的依赖关系。 Clean Arch 整洁架构是 Uncle Bob 总结了多种架构模式后提出的一种架构模式。 和洋葱架构一样，我们可以看到多个同心圆和指向内部的箭头。它们也代表了代码的核心程度和依赖关系。 在洋葱架构中： Entities 是业务实体，它们可以包含数据和行为 Use Cases 负责组织实体间的调用 Interface Adapters 负责提供 Restful API、CLI、数据库访问等与业务核心不相关的技术代码 Framework \u0026 Drivers 负责提供第三方的框架、库等不需要自己动手编写的代码 因为整洁架构是从前面两个以及其他架构设计中总结出来的，所以我们能发现它有它们的影子。与前两个架构一样，它们都强调要把和核心业务无关的代码隔离到应用核心代码之外。与前两个不一样的地方是核心代码的组织方式，洋葱架构有 Application Service、Object Service 和 Object Model，并且没有强调 Object Model 是否可以有行为；六边形架构没有对此做约束；整洁架构有 Use Cases 和 Entities，并且明确表示 Entities 是可以有自己的行为的。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:2:1","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"三种架构的前置条件 就像刚才说的，这三种架构都有一个共同特点，就是隔离与核心业务无关的技术代码。要实现这一点，就需要使用到依赖注入原则。 因为在这三种架构中，都是在核心代码里声明出要调用的接口，然后再在外部实现这个接口。实现代码中就出现了和具体技术相关的代码，比如 SQL 或 NoSql，而在接口声明中却对这些技术一无所知。这意味着随时可以替换掉实现代码，而不需要对核心业务代码做任何修改。 Note 这里说声明的是接口，其实已经与一些语言绑定了。实际上，只要能够支持依赖注入原则，这里并不关心声明的是接口还是什么东西，只要足够抽象，不包含实现细节就好。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:2:2","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"三种架构与 DDD 我时常会把 DDD 与这些架构模式放到一起思考。因为 DDD 实际上没有说到底应该如何写代码，这也导致了 DDD 的代码形式千变万化。实际上，我认为它们的关注点其实是不一样的。 三种架构的关注点 我认为这三种架构的关注点在于把技术细节隔离到业务核心代码之外，它们的共同点也在于此。正如标题说的一样，它们都是以领域为核心的的架构。至于领域里面如何组织，它们其实有不同的意见。所以我会认为它们更多的可取之处在于把与核心业务无关的代码隔离出去这种思想。 DDD 的关注点 领域内部的代码如何组织，实际上是 DDD 擅长的事情。在 DDD 中，Eric 介绍的模型驱动设计就是一种组织领域内代码的方式。 在模型驱动设计中，根据业务来设计模型，就会设计出 Entity、Value Object、Domain Service 等内容。这些构造块就可以直接对应到业务代码中。 所以，在我看来，一个合理的架构应该是这样的： 图中的蓝色箭头表示依赖的方向，橙色的圈表示业务代码的边界。 和前面的三种架构相比，我只是优化了一下组织业务代码的方式，提倡使用 DDD 来组织业务代码。而如何隔离具体技术实现的代码，仍然是使用依赖注入原则。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:2:3","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"如何实践这些架构 前面提到的三种架构，都在强调依赖关系：业务代码应该只关注业务，不和数据库之类与业务无关的代码打交道，它们不应该依赖于外部代码。 所以，要实践这些架构，就需要保证代码的依赖关系。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:3:0","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"保证依赖关系、 保证依赖关系有多种方式，这里介绍的是我想到的几种方式。 通过 Code Review 来审阅依赖关系 通过每日的 Code Review 来关注依赖关系，从而保证没有错误依赖。这是一种人工保证的方式，实践起来非常困难，容易出错。很可能因为关注依赖关系的人没有参与某一次 Code Review 导致依赖关系被破坏。而且有的团队不一定能够实践每日 Code Review。 不过在没有找到合适的技术解决依赖关系时，这也许是一种临时方案。 通过单元测试保证依赖关系 通过测试来保证依赖关系是一种不错的选择。 对于 Java 而言，可以考虑采用 ArchUnit 这个工具。它同简单的单元测试来扫描代码是否符合设置的依赖规则。 通过拆分 project 来保证依赖关系 Gradle 和 Maven 这种工具是支持拆分 subproeject 的，我们可以利用这个特性来保证依赖关系。 我们可以把业务核心代码放到名为 core 的 project 中；把提供 RESTful API 的代码放到名为 rest 的 project 中；把提供 MySQL 支持的代码放到名为 mysql 的 project 中。这样，我们就可以通过组合 rest 和 mysql 这两个 project 得到一个可部署的 project。 当我们想提供一个 CLI 版本的应用时，就可以添加一个名为 cli 的 project，再与 mysql 这个 project 组合成一个新的可发布的 project。 当我们想换掉 MySQL，提供 PostgreSQL 的支持时，就可以添加一个名为 postgresql 的 project，再组合成一个新的可部署的 project。 这种方式的优势是，每当业务代码更改时，可以同时发布所有发行版。并且为核心代码提供新的支持，不需要破坏原有的任何代码。 当然这种方式需要更高的技术成本，比如单独的 CI、团队成员对 subproject 的学习等等。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:3:1","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"可视化依赖关系 可视化代码中目前的依赖关系，可以帮助我们一眼了解项目目前的代码关系，从而了解代码关系是否符合设计，同时也能发现潜在的依赖问题。 不过这方面我暂时没有找到现成的工具，也许是一个可以尝试写代码实现的内容。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:3:2","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"总结 现有的几种架构模式都在讲把业务无关的技术实现代码从业务代码中解耦出来，通过严格的依赖关系要求，保证业务代码处于核心地位。 业务代码我们则可以使用考虑 DDD 中的模型驱动设计来设计对象的命名、职责、关系。 利用单元测试或者构建工具的 subproject 这样的功能，我们可以通过技术手段保证依赖关系，从而保证业务代码的核心地位。 有了这样的依赖关系，当我们添加、修改代码的时候，就能明确的知道应该把哪些代码写到哪些地方，而不会像在分层架构中那样迷茫，最后变成“多加一层就能解决”。 ","date":"2021-05-25","objectID":"/domain-centeric-architectures/:4:0","tags":["DDD","architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","onion-architecture","clean-arch","架构","六边形架构","端口与适配器架构","洋葱架构","整洁架构"],"title":"以领域为核心的架构们如何指导我们写代码","uri":"/domain-centeric-architectures/"},{"categories":null,"content":"发散式变化和霰弹式修改是 Martin Fowler 在《重构》中收录的两种代码坏味道。看到这两个名字时，我并不理解它们描述的是什么样的代码。也许我已经遇到过这样的代码，只是没有把它们叫做这两个名字而已。 今天我们就来看看这俩是什么样的坏味道。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:0:0","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"发散式变化 Divergent Change 发散式变化是指一个代码模块因为不同的原因在不同的方向上发生变化。这里的不同原因可以是不同的业务需求、不同技术的实现等。比如下面这个例子： fun sendTicket(sendToUserId: String, ticket: Ticket) { val emailAddress = httpClient.get('http://account.service/accounts/${sendToUserId}/emailAddress').body val message = \"\"\" xxx xxx ${ticket.price} xxx ${ticket.productions} xxx \"\"\" emailService.send(from = 'ticket@xxx.com', to = emailAddress, cc = null, title = 'Ticket', message = message) } 在这个只有几行代码的虚拟例子中，我们就能发现好几个关注点，比如处理根据 sendToUserId 拿到 emailAddress 的逻辑、组装邮件内容的逻辑和发送邮件的逻辑。这意味着每当一个关注点发生变化的时候，就需要修改这里的代码。比如获取用户的邮件地址的方式发生变化的时候、邮件内容需要修改的时候、发送邮件的方式变化的时候。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:1:0","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"为什么发散式变化是一个坏味道 上面的例子已经能够很好的回答这个问题。如果一段代码会因为不同的原因发生变化，那么意味着变化发生时，修改这部分的代码会变得困难。 困难之一是需要理解这里所有的代码才能做出正确的修改。 另一个困难的点是多个变化同时发生时，这里的代码很可能出现冲突。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:1:1","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"如何避免发散式变化 发散式变化的本质是耦合，把不同关注点的代码耦合到一个地方。所以要避免发散式变化，就需要识别出不同的关注点并解耦。 识别关注点需要搞清楚什么是业务、什么是技术、什么技术在处理什么问题。 用上面的例子来说，业务就是 \"给用户的邮箱发送 ticket，ticket 应该长成xxx样\" ；技术包括如何根据用户 ID 找到邮箱和如何发送邮件；使用 httpClient 解决了获取用户邮箱的问题、使用 emailService 解决了发送邮件的问题。所以应该把这三个关注点的代码实现分离到不同的地方。 解耦就是要把处理不同关注点的代码分开组织，放到不同的函数、类、文件中。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:1:2","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"如何修改发散式变化 不管多么小心，我们总是很难避免代码的坏味道，所以我们需要知道用什么样的方法可以修改这些坏味道。针对发散式变化，我们知道了核心是耦合，所以我们要做的就是解耦了。老马在书中给出了几种重构手法可以用来处理发散式变化：拆分阶段、搬移函数、提炼函数、提炼类 至于这几个重构手法如何操作，我想说懂得都懂🤪，不懂的，可以看看书。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:1:3","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"霰弹式修改 Shotgun Surgery 了解了发散式变化，我们再来看看霰弹式修改。 霰弹式修改是指每次发生某种变化，都需要到不同的地方做出修改。 这是与发散式变化截然相反的坏味道。一个是在一个地方出现了处理不同事情的代码，一个是处理一件事的代码被分散到了不同的地方。 同样，我们在想一个例子： class AccountServiceAdaptor : AccountServicePort { fun getAccountById(accountId: String) : HttpResponse\u003cAccount\u003e { return httpClient.get('http://account.service/accounts/${accountId}') } } class BusinessService { fun businessWork(accountId: String) { val accountResponse = accountServicePort.getAccountById(accountId) val account = accountResponse.takeIf { it.status == 200 }?.body ?: { throw SomeException() } // business works } } 在这个例子中，根据用户 ID 获取用户信息的代码被分散在了两个地方：AccountServieAdaptor 和 BusinessService 。那么当获取用户信息的方式发生变化时，这两个地方的代码都会发生变化。比如从缓存中获取用户信息，那么就不再需要处理 HTTP Status Code。 除了上面这个例子，重复的代码也有可能导致霰弹式修改。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:2:0","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"为什么霰弹式修改是一个坏味道 这个问题的答案应该是不言而喻的。当一个变化发生，却需要修改多个地方的代码时，意味着处理这个问题的代码不够内聚。这样的代码意味着当改变发生的时候，需要修改多个地方的代码甚至是重复的代码才能完成修改，这很容易导致遗漏，从而导致 bug。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:2:1","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"如何避免霰弹式修改 霰弹式修改的本质是内聚，没有把息息相关的代码放到一起做到高内聚。所以要避免霰弹式修改，也要识别出关注点，然后把处理同一个关注点的代码内聚到一起。 用上面的例子来说，businessWork 和 根据用户 ID 获取用户信息 是两个不同的关注点，所以我们应该把处理这两个关注点的代码分隔开，各自放到一起。 class AccountServiceAdaptor : AccountServicePort { fun getAccountById(accountId: String) : Account { return httpClient.get('http://account.service/accounts/${accountId}') .takeIf { it.status == 200 } ?.body ?: { throw SomeException() } } } class BusinessService { fun businessWork(accountId: String) { val account = accountServicePort.getAccountById(accountId) // business works } } ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:2:2","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"如何修改霰弹式修改 同样的，老马在书中也给出了修改的手法，这里也只是列出来：搬移函数、搬移字段、函数组合成类、函数组合成变换、拆分阶段、内联函数、内联类 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:2:3","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"总结 我们可以认为发散式变化在强调低耦合，把不相关的代码分离到不同的地方；霰弹式修改在强调高内聚，把相关的代码放到同一个地方。 它们的名字看起来有点隐晦，难以顾名思义。但是它们其实就是我们一直推崇的高内聚、低耦合。 ","date":"2021-04-25","objectID":"/divergent-change-and-shotgun-surgery/:3:0","tags":["refactor","重构","divergent change","发散式变化","shotgun surgery","霰弹式修改"],"title":"发散式变化与霰弹式修改","uri":"/divergent-change-and-shotgun-surgery/"},{"categories":null,"content":"在 Java 自带的反射库中，我们只能根据具体的一个完整类名去加载一个类。如果我们想要在一个 package 中扫描所有的符合条件的类，就需要自己写代码实现。 但是作为一个调包侠，让我自己写代码看处理起来非常常见的需求是一件很难忍受的事情。今天就来看看有什么包是我们可以直接调的。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:0:0","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"Reflections Reflections 是 github 上面的一个开源仓库，它主要提供的功能就是扫描。在它的 ReadMe 里，我们可以看到它能做什么 get all subtypes of some type get all types/members annotated with some annotation get all resources matching a regular expression get all methods with specific signature including parameters, parameter annotations and return type 很明显，这个库不仅能帮我们扫描 class，还能扫描类的成员、方法。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:1:0","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"扫描被自定义注解的类 接下来假设我们有一个叫做 @PrivateController 的注解，用来标记那些提供内部 API 的 controller。现在我们想要拿到所有被打上了这个注解的类，我们应该如何使用 Reflections 做到呢？ public Collection\u003cClass\u003c*\u003e\u003e getAllPrivateAPIs() { Reflections reflections = new Reflections(\"root.package\"); return reflections.getTypesAnnotatedWith(PrivateController.class); } 简单的两行代码，就完成了我们想要的功能。 除了根据被注解这个条件外，Reflections 还可以扫描某个类的子类，甚至根据一个表达式寻找符合要求的资源。不过这些操作都一样简单，阅读 ReadMe 就能够知道如何使用。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:1:1","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"利用 Spring 来扫描 Reflections 看起来很不错，不过有什么替代方案吗？ 我们知道 Spring 可以扫描所有的被 @Component 注解的类。那么 Spring 如何做到的呢？也许我们可以直接利用 Spring 达到我们的目的。 Spring 是可以通过 @ComponentScan 注解来指定扫描某个 package 下所有被 @Component 注解的类的。而这个过程则被实现在了 ClassPathScanningCandidateComponentProvider 这个类中。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:2:0","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"利用 TypeFilter 来做过滤 这个类的构造方法接收一个叫做 useDefaultFilters 的布尔值。那么问题来了，这里的 defaultFilters 是什么呢？ 简单浏览一下源码，就能发现 defaultFilters 包括的是 @Component、@ManagedBean、@Named 这三个注解。 Note ManagedBean 是 Java EE 6 中的 javax.annotation.ManagedBean Named 是 JSR-330 中的 javax.inject.Named 现在我们知道了三个 defaultFilter 包括哪些东西了，那么又有问题了，这里的 filter 是什么呢？ 刚才的三个注解，不是被直接使用，而是被用来创建了一个 AnnotationTypeFilter 实例，然后加入到了 includeFilters 集合中。与这个集合相对的，是 excludeFilters 集合。 这两个集合的职责，就是用来判断一个类是否符合要求的。如果一个类符合了 excludeFilters 中的条件，那么就不符合要求；接下来，如果符合了 includeFilters 中的条件，那么就符合要求。 接下来的例子中，我们将会使用到 AnnotationTypeFilter 这个类。它是 TypeFilter 的一个实现。而 TypeFilter 则有很多实现，包括 AssignableTypeFilter、AspectJTypeFilter、JooqTypeExcludeFilter 等。可以根据需要来选择或者自定义实现。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:2:1","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"如何使用 ClassPathScanningCandidateComponentProvider 我们还是使用上面的那个例子，正好对比一下两种方式的代码不同之处。 public Collection\u003cClass\u003c*\u003e\u003e getAllPrivateAPIs() { ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentPrivder(false); provider.addIncludeFilter(new AnnotationTypeFilter(PrivateController.class)); Collection\u003cBeanDefinition\u003e beanDefinitions = provider.findCandidateComponents(\"root.package\"); return beanDefinitions.stream() .map(BeanDefinition::beanClassName) .map(Class::forName) .collect(toList()); } ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:2:2","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"对比 我们可以从几个角度来对比一下上面提到的两个方案。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:3:0","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"代码量 Reflections 明显是更简单的选择，短短两行代码就能得到想要的结果。 Spring 使用起来则要麻烦一些，需要先做一点配置才能使用。并且不能直接得到想要的 Class 对象，而是得到的 BeanDefinition，然后再自己转换。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:3:1","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"职责 从 ReadMe 就可以看出，Reflections 就是用来做这种事情的，非常符合这里面临的需求。 Spring 的 ClassPathScanningCandidateComponentProvider 则不是用来做这种事情的，它只是恰好提供了我们需要的功能而已。故名思意，这个类的职责是提供 Component candidates ，是给 Spring Context 使用的。Spring 并没有保证这个类会不会被修改。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:3:2","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"可维护性 其实从职责的角度来看，Spring 的这种方式的可维护性要稍微低一点点。但是在代码量较少且封装良好的情况下，这么一点点的差别并不会有什么影响。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:3:3","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"总结 今天我们了解了如何扫描被注解的类的两种方式。但是我们没有研究这两种方式背后实现的逻辑。 作为一个合格的调包侠，如果不清楚背后的逻辑，那么很难自信的保证自己的代码是如何工作的。所以后面再单独写文章来看看这两种方式背后的逻辑吧。 ","date":"2021-03-21","objectID":"/how-to-scan-classes-in-java-package-with-annotation/:4:0","tags":["Java","Reflections","Spring","ClassPathScanningCandidateComponentProvider","scann classes"],"title":"要扫描 Java 的 package 中有某个注解的类，应该怎么做？","uri":"/how-to-scan-classes-in-java-package-with-annotation/"},{"categories":null,"content":"Json 的序列化和反序列化是我们常见的操作，很多库都提供了这样的能力帮助我们完成这样的操作。 但业务有时可能变得稍微有些复杂，需要将一个抽象类的子类实例序列化成一个 Json，也需要将一个 Json 反序列化成一个抽象类。而在反序列化的时候，就需要找到真正的子类。 Jackson 是 Java 中常用的一种进行 Json 序列化和反序列化的库，它能否面对这样的业务呢？ 业务假设 假设我们有这样的类结构关系： 代码写出来长这样： package cn.gaoyuexiang.practice.jackson.inheritance; @Data public abstract class Ticket { private TicketType type; } @Data @EqualsAndHashCode(callSuper=true) public class BugReportingTicket extends Ticket { private String productName; private String stepToReproduce; } @Data @EqualsAndHashCode(callSuper=true) public class QuestionTicket extends Ticket { private String description; } 可以在 https://github.com/kbyyd24/jackson-inheritance-practice 看到完整代码 接下来所有的序列化和反序列化操作，都是针对类型为 Ticket 的变量来执行的，而不考虑真实的类型。 序列化 序列化不会遇到任何挑战。因为不管变量的真实类型是什么，Jackson 总是会通过所有的 getter 方法来找到所有的属性和值，并序列化到 Json 中。 反序列化 反序列化就不同了，因为程序不知道 Json 应该对应到哪一个子类，所以我们会写出这样的代码： Ticket ticket = new ObjectMapper().readValue(json, Ticket.class); 而抽象类又不能创建出实例，所以这一行代码就会因为这个限制而出错。 为了让这一行代码成功执行，为 ticket 变量找到真正的类型，我们就需要使用 @JsonTypeInfo 注解。 这个注解可以用到属性上，也可以用到类上，影响的只是作用范围，对效果没什么影响。本文只有使用到类上的例子。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:0:0","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"指定 Json 中标识类型的值 使用这个注解时，需要使用到 use 属性，表示使用哪种信息来作为 Json 对应的实际子类的标识符。我们来看其中的几种。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:1:0","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"CLASS 标识 使用它时，我们的抽象类得写成这样： @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS ) @Data public abstract class Ticket { private TicketType type; } 这里的 CLASS 表示使用全限定类名来表示 Json 对应的类型，并且会在序列化的 Json 中插入一个 key 为 @class 的信息，值为类的全限定名。 比如下面的代码，就会得到这样的 Json： def ticket = new QuestionTicket() ticket.type = TicketType.QUESTION ticket.description = \"some description\" def json = new ObjectMapper.writeValueAsString(ticket) {\"@class\":\"cn.gaoyuexiang.practice.jackson.inheritance.QuestionTicket\",\"type\":\"QUESTION\",\"description\":\"some description\"} 显然，Json 中的 @class 并不是 QuestionTicket 对象的字段，而是我们的注解导致这个字段的出现。 这时我们使用这个 Json 来做反序列化，就能得到一个 Ticket 对象，并且实际类型是 QuestionTicket。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:1:1","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"MINIMAL_CLASS 标识 这个标识同样使用类名来作为 Id，但是与 CLASS 不同，它不使用全限定名，而是根据子类来计算出应该是使用单纯的类名，还是包含一部分的包名。并且，最终的值前面会加上一个 .。 使用这个标识时，使用 @c 作为 key 插入到 Json 中，上面的代码会得到这样的 Json {\"@c\":\".QuestionTicket\",\"type\":\"QUESTION\",\"description\":\"some description\"} ","date":"2020-10-02","objectID":"/jackson-inheritance/:1:2","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"NAME 标识 NAME 标识表示使用逻辑名称来标识 Json 对应的类型。默认情况下的逻辑名称是类名。 比如下面的代码会产生的 Json： @JsonTypeInfo( use = JsonTypeInfo.Id.NAME ) @Data public abstract class Ticket { private TicketType type; } {\"@type\":\"QuestionTicket\",\"type\":\"QUESTION\",\"description\":\"some description\"} 我们可以使用 @JsonSubTypes 注解来指定逻辑名称和子类的映射关系： @JsonTypeInfo( use = JsonTypeInfo.Id.NAME ) @JsonSubTypes({ @JsonSubTypes.Type(value = QuestionTicket.class, name = \"QUESTION\"), @JsonSubTypes.Type(value = BugReportingTicket.class, name = \"BUG_REPORTING\"), }) @Data public abstract class Ticket { private TicketType type; } {\"@type\":\"QUESTION\",\"type\":\"QUESTION\",\"description\":\"some description\"} @JsonSubTypes.Type 注解的 name 属性没有指定时，Jackson 会查找这个类上的 @JsonTypeName 注解的值用作 name ；如果也不存在，就会使用类名。 我们可以看到 @JsonSubTypes 注解帮助我们维护了一个逻辑上唯一的字符串和子类的映射关系，这样 Jackson 就可以根据这样的映射关系来进行序列化和反序列化了。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:1:3","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"指定标识符在 Json 中的表现形式 上面的三种标识符类型，都是在原本对象序列化的 Json 中插入了一个字段来完成的。这是因为 @JsonTypeInfo 的 include 属性默认值是 PROPERTY，也就是说是在 Json 中插入一条 property。 如果 Json 中恰好有一条 property 的名字和要插入的 property 重复，那么就可能看到 Json 中出现两个相同 key 的字段。这可能导致一些 Json 库识别这个 Json 时出现错误，因为 Json 规范没有规定 Json 中是否可以出现两个相同的 key，具体会表现为什么行为依赖于每个库的实现。 当然，除了 PROPERTY 还有其他的形式可以选择。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:2:0","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"WRAPPER_ARRAY 这种方式会将 Json 序列化成一个数组，第一个值是 Id，第二个值才是真正的对象序列化后的 Json。 @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.WRAPPER_ARRAY ) @JsonSubTypes({ @JsonSubTypes.Type(value = QuestionTicket.class, name = \"QUESTION\"), @JsonSubTypes.Type(value = BugReportingTicket.class, name = \"BUG_REPORTING\"), }) @Data public abstract class Ticket { private TicketType type; } [\"QUESTION\",{\"type\":\"QUESTION\",\"description\":\"some description\"}] ","date":"2020-10-02","objectID":"/jackson-inheritance/:2:1","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"WRAPPER_OBJECT 与 WRAPPER_ARRAY 类型，不过这种形式不是得到的数组，而是一个对象，其中的 key 就是 Id。 @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.WRAPPER_OBJECT ) @JsonSubTypes({ @JsonSubTypes.Type(value = QuestionTicket.class, name = \"QUESTION\"), @JsonSubTypes.Type(value = BugReportingTicket.class, name = \"BUG_REPORTING\"), }) @Data public abstract class Ticket { private TicketType type; } {\"QUESTION\":{\"type\":\"QUESTION\",\"description\":\"some description\"}} ","date":"2020-10-02","objectID":"/jackson-inheritance/:2:2","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"EXISTING_PROPERTY 顾名思义，这个形式表示将会使用原 Json 本来就有的一个字段。 使用这种形式时： 序列化将不会对原本的 Json 做任何修改 我们需要指定 property 属性，以表明使用原本 Json 中的哪一个字段来表示 Id 这两点表明，在使用这种形式时，原本的 Json 的这个字段自身就应该表示出对应的类型。 反序列化时，Jackson 会找到指定的 property，通过它的值和 JsonSubTypes 的配置来确定应该使用哪一个子类。 property 属性在任何时候都可以使用，它可以替换掉默认的 @class、@c 和 @type 字段。 @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.EXISTING_PROPERTY, property = \"type\" ) @JsonSubTypes({ @JsonSubTypes.Type(value = QuestionTicket.class, name = \"QUESTION\"), @JsonSubTypes.Type(value = BugReportingTicket.class, name = \"BUG_REPORTING\"), }) @Data public abstract class Ticket { private TicketType type; } {\"type\":\"QUESTION\",\"description\":\"some description\"} 另外，还有一种表现形式是 EXTERNAL_PROPERTY，可以参考这个问题 总结 我们了解了 Jackson 可以通过不同的标识符、以不同的形式来识别一个 Json 应该映射到哪一个类型，我们可以对如何使用做一个简单的判断。 无论是使用 CLASS 还是 MINIMAL_CLASS 类型，都会在 Json 中插入类的信息，这样既增加了 Json 和实现的耦合，也不利于重构，还导致了技术细节的泄露。 使用 NAME 这样的逻辑名称来标识 Json 类型看起来是比较好的做法，并且使用 Json 中本来就有的信息时，就完全去除了技术实现对 Json 的侵入，降低了耦合。 至于表现形式，则应该由 Json 本来的设计来决定。Jackson 已经给出了好几种支持，可以选择最接近设计的一种。 ","date":"2020-10-02","objectID":"/jackson-inheritance/:2:3","tags":["Jackson","Json","序列化","反序列化"],"title":"使用 Jackson 处理抽象类的序列化和反序列化","uri":"/jackson-inheritance/"},{"categories":null,"content":"过去我知道有圈复杂度这个概念，它代表了代码的复杂程度。但是却没有了解过它是怎么算出来的，哪些代码会影响它的结果。 今天我们就来看一下，圈复杂度这个概念。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:0:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"什么是圈复杂度 圈复杂度这个概念在 1976 年就已经被提出来了，比很多编程语言还要古老。 它的英文名是 Cyclomatic complexity，中文是圈复杂度、循环复杂度或者条件复杂度。 显然圈复杂度并不是一个好的翻译，因为第一次看到的时候，其实不知道它是个什么东西。但是另外两个名字也只能表示它的部分内容。 圈复杂度被用来判断程序的复杂程度，圈复杂度越高，则代表程序越复杂。 它的数值代表的是程序的线性独立路径的数量。 线性独立路径，又被称作线性无关路径，是一条至少包含一条在其他任何线性独立路径中从未有过的边的路径。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:1:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"如何计算圈复杂度 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:2:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"单结束点 对于单结束点的程序，圈复杂度的计算公式很简单，我们需要先画出程序的控制流图，用 M 代表圈复杂度： M = E - N + 2P 上面的： E：图中的边的数量 N：图中的节点数量 P：图中 connected component 的数量 connected component 是来自图论的概念，叫做元件，它是一个无向子图，上面的所有节点都不能到达子图以外的节点。 在程序设计的上下文里，这个值永远是 1。 我们可以看一下这个例子： 在这个图中，我们有 10 条边、9 个节点、1 个元件，所以 M = 10 - 9 + 2 = 3。 对于程序而言，因为只有一个进入点，所以上面的公式可以再做一步简化： M = π + 1 其中，π 代表程序中的决策点。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:2:1","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"多结束点 对于多个结束点（比如多个 return 语句）的程序，上面的公式就不太适用了，而需要一个新的公式： M = π - s + 2 其中 s 代表结束点的数量。我们可以发现这个公式同样适用于单结束点的程序。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:2:2","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"影响圈复杂度的因素 既然知道了计算圈复杂度的公式，那么我们就知道了影响它的因素：决策点和结束点。 对应到代码里，结束点就是 return 语句和 throw exception。 而决策点就很多了，不仅包括 if、case、for 等语句，三元表达式和 catch 语句同样也是决策点。 这也是为什么说循环复杂度和条件复杂度这两个翻译不够好的原因，因为决策点包含了循环和条件控制这两个内容。 想要降低圈复杂度，我们可以选择增加 return 语句，比如提前 return 或者 throw exception。 我们也可以选择减少 if 等语句的使用，通过抽方法、合并条件等重构手段达到这个目的。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:3:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"圈复杂度与代码质量 圈复杂度与代码质量息息相关。 因为越复杂的代码，越难以维护，一点小小的改动就可能造成程序运行失败。 所以在一些代码扫描工具中，比如 sonarqube，就要求方法的圈复杂度不能高于 10。 另外，越复杂的代码，意味着越难以测试。因为我们需要让测试覆盖到尽可能多的情况，圈复杂度越高，意味着需要覆盖的情况越多，测试也就越难写。 这可能就导致这段代码没有被完整的覆盖到，再加上较高的圈复杂度导致代码阅读的困难，就难以保证对它的修改不会破坏掉它的功能。 所以，为了增加代码质量，我们应该尽可能的减少圈复杂度。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:4:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"总结 我们了解了圈复杂度代表的是程序的控制流图的线性独立路径的数量，代表了程序的复杂程度。增加结束点和减少决策点都能减少圈复杂度。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:5:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"思考 尽管程序员可以修改代码降低各个程序函数的圈复杂度，但是整个程序的圈复杂度却不会改变，因为这是由业务决定的，而不是可以通过修改代码而改变的。 而且在程序设计中引用圈复杂度，本来就是为了能在函数级别降低复杂度，而不是为了降低业务复杂度（本来也做不到）。 所以不必纠结“方法的复杂度是下降了，但是整体的复杂度却没有”这种事情。 ","date":"2020-08-15","objectID":"/cyclomatic-complexity/:6:0","tags":null,"title":"圈复杂度","uri":"/cyclomatic-complexity/"},{"categories":null,"content":"Eureka 是 Spring Cloud Netflix 的服务注册与发现工具。一般情况下，它都能很好的工作，但有时却会出现一些匪夷所思的情况。 今天我们就来研究一下不当的配置导致的幽灵服务。 幽灵出现 我们先来给幽灵服务做一个定义：它们是 Eureka 注册表中的一些节点，但是它们实际上已经被关闭了，永远无法访问。 我们可以通过 GET /eureka/apps 获取 Eureka 中注册的节点信息，其中包含了节点的最后更新时间： lastRenewalTimestamp 。如果发现这个时间已经过去了很久，那么这个节点可能就是一个幽灵节点。 这其实是一个非常反常的现象，因为 Eureka client 在关闭时，通过 @PreDestroy 触发 DiscoveryClient.shutdown() 方法，向 Eureka Server 发送 shutdown 请求，从 Eureka Server 注册表中注销。 那么问题来了。 为什么这些服务没有注销？ 因为 @PreDestroy 是用来处理 SIGTERM 等信号的，所以只有在通过这些信息关闭 Eureka client 时，Eureka client 才会向 Eureka Server 发送 shutdown 请求。 但是如果是通过 kill -9 这样的指令，或者是 不优雅的 stop docker ，即发送 SIGKILL 信号，都不会触发 PreDestroy，那么这些 Eureka client 也就不会被注销了。 那么问题又来了。 难道 Eureka 不会清理过期的注册信息吗？ 其实是会的。 在 Eureka Server 的核心库 eureka-core 中，有一个 AbstractInstanceRegistry 类，其中实现了 evict() 方法，用来完成清理过期注册信息的工作。这个方法被一个 TimerTask 定时调用，默认间隔时间是 60s。 在这个方法中，Eureka server 会： 找到所有过期的 Lease (对应一个 Eureka client 节点) 通过本地注册的 Lease 数量和一个阈值 (后面还有戏份) 计算出一个 evictonLimit 数值 比较过期的 Lease 数量和 evictionLimit 的大小，选出小的那个数作为本次清理的数量 从过期的 List\u003cLease\u003e 中随机挑出 n 个 Lease，删除 (这里的 n 就是上一步的结果) 这里的阈值通过 eureka.server.renewal-percent-threshold 设置，默认值是 0.85。 后面限制每次清理节点数量的算法，只是一种性能优化的考虑。如果忽略掉这部分的逻辑，我们可以认为，一个节点长时间没有向 Eureka server 发送心跳请求，那么它的 Lease 就会过期，最终被定时任务清楚掉。 因为默认的租期时间是 90s，evict 周期是 60s，所以一个服务被不优雅的关闭后，最多经过 120s 就可以被清理出注册表。 但是 eureka 在这里却有一个 bug。Eureka server 在接收到心跳请求时，会把当前时间加上租期时间作为最后接收到心跳的时间。然后在清理时，又会在这个时间上再加一个租期时间来判断是否过期。所以一个服务过期时间总共是 180s，它被不优雅的关闭后最多经过 240s 就会被清理掉。 但是事情可能没有那么简单，别说 240s，就是过了两天，这些幽灵服务可能还在你的注册表里。 这就涉及到另一个问题，Eureka server self-preservation mode. 自我保护模式 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:0:0","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"概念 Eureka client 之间调用时，不会向 Eureka server 发送任何请求，而是根据本地维护的注册表，找到需要调用的服务，直接调用，也就是 peer to peer 的模式。本地的注册表，其实是从 Eureka server 拉取回来的，默认每 30s 拉取一次更新。 那么当 Eureka server 因为网络震荡没有接收到某些 Eureka client 发送的心跳请求时，并不意味着 Eureka client 之间的网络也出现了问题，Eureka client 之间可能仍然能够访问。 如果这个时候更新注册表，清理了只是因为网络震荡而没有发送心跳的节点，那么本可能成功的调用就会失败。 为了减少对服务的影响，Eureka 默认启动了自我保护模式，不会清理掉过期的注册信息。 官方详细的解释可以看这里。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:1:0","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"自我保护的逻辑 在上面介绍的 evict 方法的逻辑之前，eureka 会首先调用 isLeaseExpirationEnabled 方法，以判断是否要执行后续的清理逻辑。 这个方法首先会判断是否启用了自我保护模式，如果没有启用，那么就返回 true，否则会返回下面这个表达式的结果。 number-of-renews-per-min-threshold \u003e 0 \u0026\u0026 num-of-renews-in-last-min \u003e number-of-renews-per-min-threshold 那么接下来我们就看看这两个值是什么。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:2:0","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"Number of Renews in Last Minute 这个值的含义顾名思义，就是过去一分钟内，eureka server 接收到的心跳请求次数。这不是实时数据，而是每分钟更新一次。具体逻辑被 MeasureRate 实现。 由于 eureka client 默认配置的心跳间隔是 30s，所以这默认情况下的这个值就是 2*client-size。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:2:1","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"Number of Renews per Minute Threshold 这个值就相对复杂一些，表示的是触发自我保护模式的心跳阈值。根据表达式，就明白它的含义：当过去一分钟实际接收到的心跳总数小于等于这个心跳阈值时，就会触发自我保护模式，Eureka server 就不会清理注册信息。 接着，我们来看看这个阈值是如何计算出来的： (int) (expected-number-of-clients-sending-renews * (60.0 / expected-client-renewal-interval-seconds) * renewal-percent-threshold) 好了，这里又冒出来三个值，我们先来看看后两个。 Expected Client Renewal Interval Seconds 这是一个配置的值，默认 30s，通过 eureka.server.expected-client-renewal-interval-seconds 配置。 需要注意的是，这个值不是 client 真实的发送心跳的时间间隔，真实的心跳间隔是值 client 中配置的 eureka.instance.lease-renewal-interval-in-seconds，同样默认 30s。 但这两个值显然应该保持一致，因为上面的 number-of-renews-in-last-min 其实就是 (60 / lease-renewal-interval-in-seconds) * client-size，而这里也几乎是同样的逻辑（如果把 expected-number-of-clients-sending-renews 看作 client-size 的话）。 Renewal Percent Threshold 这个值在前面讲清理注册信息的时候已经讲过了，默认值 0.85。 Expected Number of Clients Sending Renews 根据名字，这个值代表的是期望的会发送心跳请求的 client 数量，也就是前面的 client-size。但是实际上的值却有可能不同。 这个值的一个更新时间是启动 eureka server 时。Eureka server 会尝试从配置的邻居 server 节点拉取注册表信息来配置。如果没有邻居节点，或者注册表中没有信息的话，就会被设置为默认值 1，通过 eureka.server.defaultOpenForTrafficCount 配置。 不是很明白为什么要设置默认值为 1。不过相关代码中，在注册与注销时，都只有在这个值大于 0 时才会更新。 这个值的另一个更新时间是一个定时任务，默认每 15min 执行一次。可以通过 eureka.server.renewal-threshold-update-interval-ms 修改这个时间。 最后，每当这个值被更新之后，都会更新我们的心跳阈值。 了解了上面的这些逻辑，我们了解到，如果符合 应有的心跳数 - 失去的心跳数 ≤ 心跳阈值，那么就会触发自我保护机制。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:2:2","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"处理一个幽灵服务的极限值 了解了前面这些，我不禁产生一个疑问：在默认配置的情况下，有多少个 client 之后，eureka server 才会清理掉过期的注册信息？出现多少个过期的注册信息，就会触发自我保护模式？ 这其实只是一个数学问题。假设集群中一共有 n 个 client，eureka server 能接受的不会触发自我保护模式的过期注册信息个数有 m 个，那么我们就可以有如下的推导： threshold = (n + default-open-for-traffic-count) * (60 / expect-client-renewal-interval-second) * renewal-percent-threshold // 带入默认的配置之后 =\u003e threshold = (n + 1) * (60 / 30) * 0.85 = 1.7(n + 1) // 因为每 30s 发送一次心跳，所以 n 个服务就应该有 2n 个心跳， m 个幽灵服务就会失去 2m 个心跳 2n - 2m ≤ 1.7(n + 1) =\u003e 2n - 1.7(n + 1) ≤ 2m =\u003e 0.15n - 0.85 ≤ m 所以，一个集群如果要能够处理一个过期的注册信息，也就是 m=1， 那么至少需要有多于 12.33 个 eureka client，也就是 13 个。 换句话说，在默认配置下，如果集群中只有不到 13 个服务，那么任何一个服务被不优雅的关闭，都会出现幽灵服务。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:3:0","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"验证 为了验证上面的结论，我写了个 demo 来验证一下。 为了保证每分钟的 evict 方法都用到最新的心跳阈值，我把 expected-number-of-clients-sending-renews 的更新时间设置为了 60s。 这个 demo 用了 docker-compose，验证的步骤需要启动 14 个 container，想要试试的同学需要给自己的 docker 多分配点资源。 12 个 client 会触发自我保护模式 话不多说，直接启动 docker-compose up -d eureka-clinet=12 稍等一会儿，访问 http://localhost:8080 就可以在 eureka 的页面上看到两个值： Renews threshold: 22 Renews (last min): 24 这两个值都符合上面的数学计算。 然后，随便杀掉一个 client (一定要使用 docker kill，避免 client 发送 shutdown 请求): docker kill eureka-self-preservation-verify_eureka-client_1 然后几分钟过去了，eureka server 的注册服务仍然有 12 个。 13 个 client 能够处理掉一个幽灵服务 还是上面的命令，只是这次我们启动 13 个 eureka-client 然后也是随便杀掉一个 client，等待结果。 被杀掉的节点被清理前： 被杀掉的节点被清理后： 还可以观察一下杀掉两个 client 的效果： 同时，也可以利用 GET /eureka/apps 查看注册信息，关注被杀掉的节点的 lastRenewalTimestamp 信息。这是它的最后注册时间，因为加了 90s，所以应该是一个未来的时间。用这个时间加上 90s，在这之后的第一次 evict 就应该清理掉这个节点信息，这可以在 eureka-server 的日志中看到。 总结 我们研究了 eureka 的自我保护模式，它的目的是避免在出现网络震荡时删除掉可能正常工作的节点信息。但如果配置不当，自我保护模式反而可能产生幽灵服务。 可以影响到自我保护模式的配置主要有两个： eureka.server.enable-self-preservation 默认为 true eureka.server.renewal-percent-threshold 默认为 0.85 根据实际的情况，我们可能需要直接关闭自我保护模式，或者找出一个更适合具体情况的 threshold。 ","date":"2020-07-25","objectID":"/eureka-self-preservation-mode/:3:1","tags":["Spring","Spring Cloud","Eureka","self preservation","自我保护模式"],"title":"Eureka 中的幽灵 —— Self Preservation 导致的幽灵服务","uri":"/eureka-self-preservation-mode/"},{"categories":null,"content":"使用 Spring Security ACL 的例子","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"在上一篇文章中，我们了解了 Spring Security ACL 的基本概念。 但是几乎没有涉及实现与使用的部分。这篇文章我们就来看一看如何在使用了 Spring 的项目中使用 Spring Security。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:0:0","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"导入 Jar 包 Spring Security ACL 的 GAV 是 org.springframework.security:spring-security-acl:\u003cversion\u003e， 被加入到了 Spring IO Platform bom 中，所以可以使用 Spring 的依赖管理插件来管理版本号。 但是 Spring 没有提供相应的 boot starter，使用的时候需要自己进行配置。 毕竟 ACL 中提供的默认实现很可能达不到需求，还不如自己动手。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:1:0","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"存储 ACL ACL 需要被持久化起来，否则系统重启后就丢掉了权限信息。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:2:0","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"基于 JDBC 的持久化 Spring Security ACL 提供了默认的 AclService 和 MutableAclService 实现：JdbcAclService 和 JdbcMutableAclService。 因为是基于 JDBC 的实现，所以就可能存在性能问题（绝大多数情况下不需要更新 ACL，但使用 JDBC 时总是会访问数据库，得到一模一样的 ACL）。 于是 ACL 又设计了一个缓存机制来减少对数据库的请求。缓存的接口是 AclCache，Spring 提供了基于 Eh-Cache 和 Spring Cache 的两个实现。 AclCache 接口被安排在了 org.springframework.security.acls.model 包中。 个人觉得这不是一个必须的元素，算不上核心概念，不应该和其他核心概念放到一起。 创建数据库 JdbcAclService 需要创建四张表，创建表的 SQL 已经在 jar 包里了（但是有坑），我们只是来看看这几张表分别保存了什么内容。 acl_sid id: 主键 principal: 一个布尔值，表示这个 sid 是不是 PrincipalSid，如果值为 false 则表示 sid 是 GrantedAuthoritySid 这个设计简直把扩展堵死了 🙄️ sid: Sid 实例中保存的字符串，也就是 Principal.getName() 或 GrantedAuthority.getAuthority() 或自己写代码生成的字符串 acl_class id: 主键 class: Domain Object 类型的全限定名 ObjectIdentity 的默认实现使用 全限定名+id 的方式确定一个 Domain Object class_id_type: 可选字段，保存 ObjectIdentity 中 identifier 的类型，默认是 Long 这就是坑的所在。Jar 包中给出的 SQL 有不同的数据库的版本，只有 PostgreSql 和 HSQL 的版本中有这个字段。 如果不创建这个字段，那么 ObjectIdentity 就只能支持 Long 类型的 id。 JdbcService 默认也不会查询这个字段，需要特别配置。 acl_object_identity id: 主键 object_id_class: 外键依赖 acl_class.id object_id_identity: ObjectIdentity.getIdentifer() 的值，与 object_id_class 组成唯一键 parent_object: 外键依赖 acl_object_identity.id，在 Acl 继承时使用 owner_sid: 外键依赖 acl_sid.id，是 Acl 实例的 owner，拥有修改 Acl 的权限 entries_inheriting: 布尔值，表示 child acl 是否要继承 parent acl 的 ACE 这张表里保存的都是 Acl 持有的信息，也就是说，这张表的一行记录对应了一个 Acl 实例。 acl_entry id: 主键 acl_object_identity: 外键依赖 acl_object_identity.id，表示属于哪一个 Acl ace_order: 表示这一条 ACE 在 Acl 中的顺序，和 acl_object_identity 组成唯一键 sid: 外键依赖 acl_sid.id，表示这条 ACE 的权限对应哪一个 Sid mask: 表示权限的 32 位二进制数字 granting: 布尔值，表示这条 ACE 是否生效 audit_success audit_failure: 这两条是用于审计的信息，对应了 AuditableAccessControlEntry 接口，本文不会涉及 这张表保存的是 ACE 的信息，一行记录对应了一个 ACE 实例。 配置 JdbcAclService 在动手之前，我们先来看看 JdbcAclService 都有什么依赖： 因为 ACL 没有 Spring Boot 的 auto configure，所以除了 DataSource，我们需要自己来配置这些依赖。 除非项目中不需要修改 ACL，否则都会选择创建一个 JdbcMutableAclService 而不是 JdbcAclService。 注入 JdbcMutableAclService @Bean public MutableAclService mutableAclService() { JdbcMutableAclService jdbcMutableAclService = new JdbcMutableAclService(datasource, lookupStrategy(), aclCache()); jdbcMutableAclService.setAclClassIdSupported(true); (1) jdbcMutableAclService.setClassIdentityQuery(\"SELECT @@IDENTITY\"); (2) jdbcMutableAclService.setSidIdentityQuery(\"SELECT @@IDENTITY\"); (3) return jdbcMutableAclService; } 如果要支持 acl_class.class_id_type 字段，则需要将 aclClassIdSupported 设置为 true。这样 JdbcAclService 在查询时和 JdbcMutableAclService 更新时才会考虑这个字段的值。 classIdentityQuery 会在创建 Acl 对象时用到，用来获取刚刚插入的 acl_class.id。默认值是 call identity()，这是 H2 数据库的方言，这里的例子是 MySql 的方言。 与 2 相同，只是用来获取刚刚插入的 acl_sid.id 接着，我们需要配置 LookupStrategy 和 AclCache。 注入 LookupStrategy LookupStratege 只有一个实现：BasicLookupStrategy @Bean public LookupStrategy lookupStrategy() { BasicLookupStrategy basicLookupStrategy = new BasicLookupStrategy(datasource, aclCache(), aclAuthorizationStrategy(), permissionGrantingStrategy()); basicLookupStrategy.setAclClassIdSupported(true); (1) return basicLookupStrategy; } BasicLookupStrategy 也会自己组装 sql，需要调用这个方法以支持 acl_class.class_id_type。 接着，我们先来看一下 AclAuthorizationStrategy 和 PermissionGratingStrategy 这两个简单一点的依赖。 注入 AclAuthorizationStrategy @Bean public AclAuthorizationStrategy aclAuthorizationStrategy() { return new AclAuthorizationStrategyImpl(new SimpleGrantedAuthority(\"owner\")); } AclAuthorizationStrategy 是用来判断当前的 Autentication 是否有权限修改 Acl 的接口，它只有 AclAuthorizationStrategyImpl 这一个实现。 这个接口规定了三种权限： change ownership 修改 Acl 的 owner change auditing 修改 Acl 的审计信息 change general 修改 ACE 实现中有三个 GrantedAuthority 属性，对应了上面的三种权限，表示对 Acl 进行某种操作时，Authentication 需要满足对应的 GrantedAuthority。 它的构造方法接受一个或三个 GrantedAuthority: 如果只有一个参数，那么三个权限都是这个 GrantedAuthority 如果有三个参数，那么就会按上面的顺序赋值给这三个权限 在判断权限时，如果是 Acl 的 owner，且不是在修改审计信息时，就可以直接获得权限；否则就需要 Authentication 具备对应的 GrantedAuthority。 注入 PermissionGrantingStrategy @Bean public PermissionGrantingStrategy permissionGrantingStrategy() { return new DefaultPermissionGrantingStrategy(new ConsoleAuditLogger()); } PermissionGrantingStrategy 抽象了 isGranted 方法，被 AclImpl 调用，是真正执行权限判断的地方。 这个接口只有这一个实现。 注入 AclCache ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:2:1","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"创建 ACL 创建 ACL 发生在创建 Domain Object 的时候。我们可以编写代码创建 Acl 对象： public void onCreate(Class\u003c?\u003e domainClass, String id, Authentication authentication) { ObjectIdentity objectIdentity = new ObjectIdentityImpl(domainClass, id); PrincipalSid sid = new PrincipalSid(authentication); MutableAcl acl = findOrCreate(objectIdentity); acl.insertAce(acl.getEntries().size(), BasePermission.ADMINISTRATION, sid, true); // ... (1) mutableAclService.updateAcl(acl); } private MutableAcl findOrCreate(ObjectIdentity objectIdentity) { try { return (MutableAcl) mutableAclService.readAclById(objectIdentity); (2) } catch (NotFoundException e) { return mutableAclService.createAcl(objectIdentity); } } 这里可以向 Acl 中插入任意需要的 ACE 这里的强制转换不会出错是因为 AclImpl 是 Acl 和 MutableAcl 的共有且唯一的实现 我们来梳理一下这段代码的逻辑： 根据 domainClass 和 id 创建 ObjectIdentity，它可以用来标识一个 domain object 根据当前的 Authentication 创建一个 PrincipalSid，它后续会被用作 ACE 的 sid 根据 ObjectIdentity 对象查找或创建 Acl 对象 创建时会使用当前的 Authentication 新建一个 PrincipalSid 对象，和这里的不是同一个对象 向 Acl 中插入 ACE 这里可添加当前用户的 ACE，也可以添加某种 GrantedAuthority 的权限 保存 Acl 接着我们来看一下添加 ACE 的细节，也就是 Acl.insertAce() 方法。 insertAce 这个方法的作用是向 Acl 实例的 List\u003cAccessControlEntry\u003e 中插入 ACE 实例。 第一个参数指定了这个 ACE 在列表中的索引值。 第二个参数指定这个 ACE 的权限是什么。这里我们使用了 BasePermission 这个实现提供的值。 但无论如何，Permission 接口的 getMask() 方法一定返回的是一个 32 位的二进制数字。（类型是 int，但取值范围是 0~232-1，这也就是所能设计的权限的个数。） 第三个参数是一个 Sid，表示这个 Sid 拥有对应的权限。 第四个参数是 granting，代表这条 ACE 是否生效，作用类似于 enable。 调用了这个方法之后，一个新的 ACE 就被加入到 Acl 中了。接下来的权限判断就可以使用到这个新的 ACE。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:2:2","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"更新 ACL 更新 ACL 可能更多的出现在协作软件中，比如邀请他人一起编写文档等操作。 添加新的 ACE 我们就以邀请协作作为例子，看看如何添加 ACE： public void onInvite(Class\u003c?\u003e domainClass, String id, String invitedUserPrincipalName) { ObjectIdentity objectIdentity = new ObjectIdentityImpl(domainClass, id); PrincipalSid sid = new PrincipalSid(invitedUserPrincipalName); (1) MutableAcl acl = findAcl(objectIdentity); acl.insertAce(acl.getEntries().size(), BasePermission.WRITE, sid, true); mutableAclService.updateAcl(acl); } private MutableAcl findAcl(ObjectIdentity objectIdentity) { return (MutableAcl) mutableAclService.readAclById(objectIdentity); } PrincipalSid 其实保存的是 Authentication.getName() 的返回值，本质上就是 principal name，所以可以直接使用一个字符串作为 principal name。 这里，我们给了一个具体的被邀请者一个 WRITE 权限。我们甚至可以给一组用户权限，只需要提供不同的 Sid。 GrantedAuthoritySid sid = new GrantedAuthoritySid(\"teamA\"); 更新已有的 ACE 还是前面的例子，假设某个协作者完成了自己的工作，不想误操作导致修改，希望将权限修改为只读。 那么这个时候就会使用到 MutableAcl.updateAce() 方法。 这个方法只有两个参数，第一个是要修改的 ACE 的索引，第二个是需要修改成的权限，那么我们的代码可能写出来是这个样子： List\u003cAccessControlEntry\u003e aces = acl.getEntries(); aces.stream() .filter(ace -\u003e ace.getSid().equals(sid)) .mapToInt(aces::indexOf) .forEach(idx -\u003e acl.updateAce(idx, permission)); 这样我们就能将任何匹配到 Sid 的 ACE 的权限修改为指定的 Permission。 删除已有的 ACE 还是前面的例子，假设我们协作完成了，需要关闭其他人的权限，那么我们就需要删除对应的 ACE。 个人更倾向于软删除，也就是将 ACE 的 granting 字段设置为 false。但是 Spring ACL 在模型设计上没有提供修改的方法，具体实现上，这个字段也被标记为 final。 删除 ACE 同样需要它的索引： List\u003cAccessControlEntry\u003e aces = acl.getEntries(); aces.stream() .filter(ace -\u003e ace.getSid().equals(sid)) .mapToInt(aces::indexOf) .forEach(acl::deleteAce); ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:2:3","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"删除 ACL 一个 ACL 关联到一个 domain object，当一个 domain object 被删除时，ACL 也应该被删除。 MutaleAclService 也定义了 deleteAcl 方法用来删除 ACL，它被 JdbcMutableAclService 实现。 mutableAclService.deleteAcl(acl, true); 其中的第二个参数表示是否要删除子 Acl。（因为 ACL 可以继承） 在具体的实现中，默认的 sql 会删除整个 ACL。所以如果想要实现软删除，那么不仅需要调用对应的 setter 方法来修改对应的 sql，还需要修改表结构来支持软删除。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:2:4","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"使用 ACL 保护业务调用 现在我们知道了如何操作 ACL，那么接下来看看如果使用 ACL 来判断请求是否有权限调用业务方法。 在上一篇文章中，我们介绍过 Spring ACL 提供的三种使用 ACL 的机制，接下来我们以 expression based access control 作为例子，看看如何使用 ACL 保护业务方法。 在这个例子中，我们使用 PostAuthorize 来保护业务方法： @PostAuthorize(\"hasPermission(returnObject, 'READ')\") public DomainObject find(String id) { //... return domainObject; } 这个例子中，业务方法返回的 domainObject 会被作为表达式中的 returnObject 传递给 hasPermission 方法，验证当前用户是否有指定的 READ 权限。 这里的 hasPermission 方法会调用到 SecurityExpressionRoot，最终到达 AclPermissionEvaluator.hasPermission() 方法。 在这个方法中，returnObject 会被交给 ObjectIdnetityRetrievalStrategy 接口，得到 ObjectIdentity，用来查找对应的 Acl 实例。 具体实现上，其实是通过反射得到 className 和 getId() 方法，然后调用这个方法得到 identity。所以这里的 returnObject 一定要有 getId() 方法。 我们传递的 'READ' 参数，就是 permission，但这里被当作了 Object。AclPermissionEvaluator 通过自己实现的 resolvePermission() 方法来处理不同类型的参数。 我们的 'READ' 会被当做字符串处理，交给 PermissionFactory 的默认实现处理。 默认实现中提供了两个 Map，分别保存了 permission name → BasePermission 和 mask int → BasePermission 的映射关系。 我们的 'READ' 也就这样被映射到了 BasePermission.READ。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:3:0","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"启用 Method Security 解释完了上面的方法，接下来就是要让这个表达式生效了。 Spring Security 默认没用启用 Method Security，而 PostAuthorize 则需要这个支持。 要启用这个功能很简单，我们只需要： enable pre/post security 创建 AclPermissionEvaluator 创建 MethodSecurityExpressionHanler，并将 AclPermissionEvaluator 设置为它的 PermissionEvaluator @Configuration @EnableGlobalMethodSecurity(prePostEnabled = true) public class MethodSecurityConfig { @Autowired private AclService aclService; @Bean public MethodSecurityExpressionHandler methodSecurityExpressionHandler() { MethodSecurityExpressionHandler handler = new DefaultMethodSecurityExpressionHandler(); handler.setPermissionEvaluator(new AclPermissionEvaluator(aclService)); (1) return handler; } } 我们可以给它提供一个 AclSerivce 而不是 MutableAclService，因为它不会修改 ACL 这样我们的表达式就能生效了，当一个没有 READ 权限的用户尝试访问 domain object 的时候，就会得到一个 403 响应。 这里就不再讲使用 AclEntryVoter 和 after invocation 的例子了，无论使用哪种方式，都需要自己进行相应的配置。读者如果感兴趣可以自己研究研究。 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:3:1","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"总结 这篇文章从代码的角度简单研究了如何使用 ACL。 相信你也看出来了，没有 Spring Boot auto configure 的支持，使用 ACL 需要些大量的配置代码。 这还不止。在研究源码时，发现 ACL 的实现其实比较简单，覆盖的场景看起来特别单一。 如果实际项目想要使用，应该需要花一些工作来提供项目和适配的实现。 举个例子，AclEntryVoter 的实现中，要求 MethodInvocation 的餐宿中一定要有 domain object，这一点可能就和一些项目的实践相违背。 尽管如此，我仍然认为 ACL 是一种不错的设计，很好的分离了业务和访问控制的关注点，抽象的接口也可以方便的进行扩展，值得去试一试。 查看系列文章： 点这里 ","date":"2020-07-18","objectID":"/how-to-use-spring-security-acl/:4:0","tags":["Spring","Spring Security","Spring Security ACL","Spring Security ACL demo"],"title":"如何使用 Spring Security ACL","uri":"/how-to-use-spring-security-acl/"},{"categories":null,"content":"介绍 Spring Security ACL 中核心的概念和组件","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Spring Security 提供了一个 ACL 模块，也就是 Access Control List，用来做访问控制。 目的是解决 谁对什么资源有什么权限 的问题。 这里的重点是具体的资源。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:0:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"面临的问题 我们通过 Spring Security 的 WebSecurityConfigurerAdapter.configure(HttpSecurity) 方法， 对 HttpSecurity 对象进行配置，只能精确到 API 层面，决定拥有哪些权限的用户可以访问哪些 API， 但是对于哪些用户对一个具体的资源有访问权限却无能为力。 举个🌰，对于一个多用户的博客系统，可能存在一个 API /my/drafts ， 可以查看当前用户的草稿，但是不能查看其他用户的草稿。 那么上面的 HttpSecurity 就无法完成这个需求。 为了解决这个问题，我们可能会有很多解决方法： 实现一个 AccessDecisionVoter，访问数据库中的 Blog 记录，然后判断当前的用户是否有权限访问 实现一个 AccessDecisionVoter，通过 Authentication 对象中的 GrantedAuthority[] 判断是否有权限访问 抛弃 Spring Security，自己实现一套权限控制机制 将访问控制的代码和业务代码组织到一起 这些方法也不是不能用，但都有各自的缺陷（Spring Security ACL 也有，只是比这些要好一点）： 第一个方案意味着在 AccessDecisionVoter 中会进行数据库访问，这会造成性能上的隐患 第二个方案在数据量上升后可能会面临 Authentication 对象无比巨大的问题 第三个方案就像自己创造一个加密算法一样，看起来厉害，但可能会有很多漏洞，毕竟没有经过实践检验 第四个方案是最轻量的，但是很容易破坏代码的单一职责原则，最后变成没人愿意维护的代码 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:1:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"ACL 核心概念 针对上面的缺点，Spring Security ACL 则是采用了面向 Domain Object 的方式， 抽象出了 Domain Object 这个概念，把 ACL 与业务代码解耦。 我们可以先来看一下 ACL 中的核心概念： 除了 Domain Object 和 Security Object，其他概念都是 ACL 中的接口。 这些接口都在 org.springframework.security.acls.model package 中。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Domain Object Domain Object 是对业务类实例的抽象，用 DDD 的话说，就是对领域模型实例的抽象。 一个 Domain Object 对应的是一个实例。 它可能是一篇博客，也可能是一条评论。 一个 Domain Object 被一个 ObjectIdentity 唯一标识。 因为有了这个抽象，我们就可以将 ACL 的代码和业务逻辑解耦，仅仅通过 ObjectIdentity 来标识 Acl 和 Domain Object 的关联关系。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:1","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Security Object Security Object 是对用户和角色的抽象。代表了一个用户，或一种角色，或一个权限组等。 往往是 Principal 和 GrantedAuthority 的抽象。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:2","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Acl Acl 类是 ACL 中的核心，也就是 Access Control List 的简称。 一个 Acl 实例拥有一个 ObjectIdentity，标识了一个 Domain Object。 一个 Acl 实例拥有一组 AccessControlEntity，顾名思义，它们就是这个访问控制列表中的每一个访问控制项。 一个 Acl 实例持有一个 Sid 对象，标识了这个 Acl 的 owner；owner 对这个 Acl 有完全的控制权。 所谓完全的控制权，也就是指可以修改、删除其中的信息，甚至 Acl 本身。 MutableAcl MutableAcl 是对 Acl 的扩展，提供了一些修改 Acl 的方法。 考虑到 Acl 可能更多的被使用到一些不会修改访问权限的调用中，所以它只提供了一些只读方法。 但有一些可能不太频繁的会修改访问权限的操作，比如创建、删除等，所以需要一些方法能够修改 Acl 实例。 所以扩展出了 MutableAcl 类，用来进行修改访问权限的操作。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:3","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"ObjectIdentity 前面已经提到过 ObjectIdentity 的作用，是用来标识 Domain Object， 就是一个脱离业务上下文后仍然唯一的 id。 实际上是 ACL 上下文中对 Domain Object 的抽象 能够做到这一点，是因为提供了两个方法：getType 和 getIdentifier。 type 用来标识 Domain Object 的类型，identifier 则是在 type 上下文中唯一的。 不同的 type 下，可以出现相同的 identifier，但 type 却需要全局唯一。 默认实现是使用 Domain Object 的 java 类全限定名作为 type。 identifier 则需要使用者自己实现。 要求是能根据 Domain Object 找到这个 identifier，否则 Acl 和 Domain Object 就会失去联系。 使用 Domain Object 的系统 id 会是一个不错的选择。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:4","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Sid Sid 是 Security Identity 的简称，是在 ACL 上下文中对 Security Object 的抽象。 就像 ObjectIdentity 对 Domain Object 的抽象一样。 为什么不能统一一下规范，叫 SecurityIdentity 呢 🙄️ 在实现上，有 PrincipalSid 和 GrantedAuthoritySid。 前者是对用户的抽象，后者是对角色、权限的抽象。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:5","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"AccessControlEntry AccessControlEntry 简称 ACE，组成了访问控制列表。 每一个 ACE 标识了一个 Sid 的权限，权限由 Permission 表示。 🌰： 如果一种角色对 Domain Object 有可读权限 Sid 会是表示这个角色的 GrantedAuthoritySid Permission 会表示 READ 权限 如果一个用户对 Domain Object 有可读、可写、可邀请协作的权限 会有多个 ACE 每个 ACE 的 Sid 都是指向这个用户的 PrincipalSid 每个 ACE 的 Permission 会不同，分别表示可读、可写、可邀请协作 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:6","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"Permission Permission 接口可能收到了 Linux 文件权限的启发，要求使用 32 位二进制数字来表示权限。 所以我们可以针对一个 Domain Object 设计出 232-1 种权限，应该足够使用了。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:7","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"小结 ACL 的核心就是 Acl 类，它将 Domain Object 和 对应的 Security Object 以及权限关联了起来。 其中，将 Security Object 和权限关联起来的类是 AccessControlEntry。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:2:8","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"ACL 权限验证逻辑 通过了解核心概念，我们知道了 ACL 的核心就是 Acl 类，那么进行权限验证的逻辑也就很明显了： 根据要访问的对象，得到 ObjectIdentity 实例 从 Authentication 中获取 Sid 根据 ObjectIdentity 找到对应的 Acl 判断 ACE 中是否有进行访问需要的 Permission 当上面的这个逻辑验证通过时，才会被允许访问 Domain Object，否则就会出现 AccessDeniedException。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:3:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"获取 ObjectIdentity 对象 我们先来看看第一步，获取 ObjectIdentity 对象。 前面介绍 ObjectIdentity 的时候推荐过使用 Domain Object 的 class 和系统 id 来作为 ObjectIdentity。 这样的好处就是我们可以根据 Domain Object 实例创建出 ObjectIdentity 来。 这个逻辑被抽象成了接口 ObjectIdentityRetrievalStrategy。 它只提供了一个 getObjectIdentity 方法：Object → ObjectIdentity。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:3:1","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"获取 Sid 对象 因为 Sid 代表的是用户和角色，而这些信息被保存在 Authentication 对象的 Principal 和 GrantedAuthority[] 中， 所以我们可以通过 Authentication 对象来获取 Sid。 这个逻辑同样被抽象成了接口 SidRetrivalStrategy。 它只提供了一个 getSids 方法：Authentication → List\u003cSid\u003e ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:3:2","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"获取 Acl 对象 ACL 提供了一个接口 AclService 用来获取 Acl 对象。 接口提供了多种方法，其中被这个逻辑使用到的是 readAclById(ObjectIdentity, List\u003cSid\u003e)。 其实不提供 List\u003cSid\u003e 也能查到对应的 Acl，但其中就会包含当前逻辑中不需要使用到的 ACE。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:3:3","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"判断权限 Acl 提供了 isGranted 方法用来判断当前的 List\u003cSid\u003e 是否有需要的权限。 在默认实现 AclImpl 中，判断的逻辑交给了接口 PermissionGrantingStrategy， 这样我们可以通过实现策略而不是 Acl 来达到重写验证逻辑的目的。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:3:4","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"ACL 验证逻辑的入口 前面描述的验证逻辑，被实现在了不同的类中。这些类是具体的 security 机制相关的类，每一个类都是针对具体 security 机制的 ACL 验证逻辑的实现。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:4:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"针对 pre invocation 在前面的文章中， 我们了解过 AccessDecisionVoter 是在实际调用发生前进行权限验证的接口。 ACL 中提供了实现 AclEntryVoter 来实现验证逻辑。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:4:1","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"针对 post invocation 在前面的另一篇文章中， 我们了解过 AfterInvocationProvider 是在实际调用发生后进行权限验证的接口。 ACL 中提供了 AbstractAclProvider 来实现验证逻辑。 它的两个子类则是针对不同的使用场景，分别实现权限验证和 collection filter 的逻辑。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:4:2","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"针对 expression based access control 对于使用表达式的地方，比如 @PostAuthority(\"hasPermission(returnObject, 'READ')\")， ACL 提供了 AclPermissionEvaluator 实现验证逻辑。 这个类实现了 PermissionEvaluator 接口。这是 Spring Security 中 hasPermission 表达式的解析接口。 了解完了这三个入口，我们就知道当需要在某个 security 机制中使用 ACL 时，需要创建出哪一个组件注入到 Spring 容器中。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:4:3","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"更新 Acl 前面的验证逻辑是使用场景更多的逻辑，也是对 Acl 进行只读操作的逻辑。 更新 Acl 并不是一个频繁的操作，但却是一个必要的操作。 遗憾的是，Spring Security ACL 没有为我们默认实现更新 Acl 的逻辑，我们需要自己实现。 好在为了支持更新操作，Spring Security ACL 给我们提供了 MutableAcl 和 MutableAclService 这两个接口，提供了更新 Acl 的方法。 以创建 Acl 为例，我们来看看应该写出什么样的代码。（创建 Acl 的场景一般是创建了新的资源时） public void createAcl(Object domainObject) { ObjectIdentity oid = new ObjectIdentityImpl(domainObject); Sid sid = new PrincipalSid(SecurityContextHolder.getSecurityContext().getAuthentication()); Permission p = BasePermission.ADMINISTRATION; MutableAcl acl = aclService.createAcl(oid); acl.insertAce(acl.getEntries().size(), p, sid, true); aclService.updateAcl(acl); } 对于更新 ACE、删除 Acl 等操作，MutableAcl 和 MutableAclService 都有相应的方法，只是和创建一样，都需要我们自己写代码调用。 这些代码应该和业务代码分隔开，这样才能满足 ACL 的初衷，避免写出难以维护的代码。 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:5:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"总结 本文介绍了 Spring Security ACL 中的核心概念和它们之间的关系，那张概念图就是最好的总结。 另外还介绍了使用 ACL 进行权限控制的逻辑和相关的组件，也就是那些 strategy 和 service 接口。 最后简单介绍了更新 Acl 的方法，更详细的内容会在另外的博客里以 demo 的形式呈现。 查看系列文章： 点这里 ","date":"2020-07-02","objectID":"/spring-security-acl-conception-and-component/:6:0","tags":["Spring","Spring Security","Spring Security ACL"],"title":"Spring Security ACL 核心概念和组件","uri":"/spring-security-acl-conception-and-component/"},{"categories":null,"content":"如何优雅的关闭 Docker container","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"我们平时在使用 Docker 的时候，一般会使用 ctrl+c 或者 docker stop 的方式关闭容器。 但有时候我们可能会遇到 ctrl+c 不生效，或者 docker stop 之后要等待 10s 的情况，就像这样： 也许你会觉得 10s 是一个可以忍受的时间， 但这样的问题真的只有 10s 这么简单吗？ 为什么有的时候不能立即关闭容器呢？ ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:0:0","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"docker stop 怎么关闭容器 首先我们来看一下这两个命令做了什么。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:0","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"ctrl+c 到底做了什么 我们能够使用 ctrl+c 的场景，是我们在使用 foreground 模式运行容器的时候。 这时我们按下 ctrl+c 就像在普通的 shell 中按下这个组合键一样，发送一个 SIGINT 信号给当前的进程，通知它终止运行。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:1","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"docker stop 做了什么 docker stop 命令是在对 detached 模式运行的容器发出停止命令时使用的，从发送信号上来讲，它将发送 SIGTERM 信号给容器，通知其结束运行。 SIGINT 一般用于关闭前台进程，SIGTERM 会要求进程自己正常退出。 当我们在 shell 中给进程发送 SIGTERM 和 SIGINT 信号的时候，这些进程往往都能正确的处理。 但是在 docker 中却不灵了。 这是因为在 docker 中，只会将 SIGTERM 等所有的 signal 信号发送给 PID 为 1 的进程，当我们 docker 中运行的进程的进程号不是 1 时，就不会收到这样的信号。 根据这篇文章的说法，只有 alpine 会出现这个问题，但从我搜到的资料和实验来看，并不是这样，而是所有的镜像都会有这个问题。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:2","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"为什么是 10s 其实这只是 docker 的默认设置，如果你愿意，等十年都可以。 文档链接：https://docs.docker.com/engine/reference/commandline/stop/ 如果达到上面的时间限制，docker 将会通过给内核发送 SIGKILL 从而强制结束容器。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:3","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"验证上面的回答 为了验证上面查到的这些结果，我写了一点 demo。 在 demo 的场景里，我们会在 ENTRYPOINT 配置运行一个 shell 脚本，在脚本中做一些准备工作后启动进程。 FROM openjdk:8-jre-alpine COPY ./build/libs/app.jar /app/app.jar COPY ./docker/entrypoint.sh /app/entrypoint.sh WORKDIR /app EXPOSE 8080 ENTRYPOINT [\"./entrypoint.sh\"] entrypoint.sh #!/bin/sh echo 'Do something' java -jar app.jar 后面还会用到这个例子 我们可以观察一下 docker stop 之后，shell 给我们的返回值 Figure 1. 注意红色方框 根据这张图，我们可以看到: docker stop 命令等待了 10s 才结束 结束的 docker container 返回了 137，表示进程是因为内核接收到了 SIGKILL 而结束的（zsh 给美化成了 KILL） ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:4","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"Kill 导致的问题 现在我们已经了解了 ctrl+c 为什么不生效和 docker stop 等待 10s 的原因了。 我们再来看看另一个问题： 强制关闭容器，真的就没问题吗？ 或许你能想到，很多进程在结束阶段会做一些清理工作：比如删除临时目录、执行 shutdown hook 等。 但是当进程被强制关闭时，这些任务就不会被执行，那么我们就可能得到一些并不期望的结果。 以 Eureka 为例。 Eureka client 在结束进程时，需要向 Eureka server 发送 shutdown 信号，以注销 client。 这本来没什么问题，因为 Eureka server 即使没有收到这样的信息，也会定期清理 client 信息。 但是 Eureka server 还有一个 self preservation 模式，以防止意外的网络事件导致大量的 client 下线。 这就有可能导致 Eureka 集群的注册表中出现大量的 client 信息，但它们其实已经关闭了。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:1:5","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"如何优雅的关闭容器 通过前面的内容，我们已经了解了容器没有被优雅关闭的原因和可能导致的问题，接下来，我们来看看如何解决。 前面提到的那篇文章中提到可以使用 tini 来解决，但我没有成功过。tini 的确做到了立即关闭进程，但是进程并没有执行 shutdown hook。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:2:0","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"使目标进程成为 PID 1 既然 docker 只会将 sigal 发送给 PID 1 的进程，那就让我们的进程成为 PID 1 的进程就好了。 docker 的 exec 与 shell 模式 Dockerfile 的 ENTRYPOINT 有两种写法，即 exec 和 shell： # exec form ENTRYPOINT [\"command\", \"param\"] # shell form ENTRYPOINT command param 两者的区别在于： exec 形式的命令会使用 PID 1 的进程 shell 形式的命令会被执行为 /bin/sh -c \u003ccommand\u003e，不会执行在 PID 1 上，也就不会收到 signal 所以，我们应该选择 exec 模式，让我们的程序成为 PID 1 进程。 ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"] 更详细的信息，可以查看官方文档。 exec 命令 exec 形式的 ENTRYPOINT 只能解决 无需任何准备工作就启动进程 的场景，而不能解决一些需要准备工作的复杂场景。 在这样的场景中，我们的 ENTRYPOINT 往往需要执行一个 shell 脚本： ENTRYPOINT [\"./entrypoint.sh\"] 然后在这个脚本中执行我们的准备工作，完成后再启动真正的进程。 比如上面的例子，做完准备后，启动 java 进程。 这时候，我们的 java 进程就无法成为 PID 1 进程。 我们可以看到，java 进程的 PID 是 7，也就无法优雅退出了。 为了解决这个问题，我们可以使用 exec 命令来解决。这个命令的作用就是使用新的进程替代原有的进程，并保持 PID 不变。 这就意味着我们可以在执行 java 命令的时候使用它，从而替换掉 PID 1 的 shell 脚本： entrypoint.sh #!/bin/sh echo \"Do something\" exec java -jar app.jar 我们再来看一下容器中的进程： 使用 exec 命令之后，我们无论是使用 ctrl+c 还是 docker stop 都能让进程接收到信号，执行相应的操作后退出： 这张图我们可以看到很多信息： docker stop 命令很快结束，没有等待十秒 容器退出收到的信号是 SIGTERM，不是 SIGKILL Spring 进程的最后一行日志是 shutdown hook 的日志 这些信息表明，java 进程收到了 docker stop 发送的 SIGTERM 信号，并且正确的触发了相关操作，最后退出程序。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:2:1","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"使用 trap exec 命令在这样的场景下算是一个比较完美的方案。 但如果你还想探索一下其他方式，或者你的容器中需要运行多个进程，那我们可以接着来看看 trap 命令。 trap 是用来设置陷阱、监听 signal 的 shell 命令，一般用来处理脚本收到的 signal，完成一些操作。 trap [-lp] [[arg] sigspec ...] 本文不介绍 lp 参数的含义 arg 代表接收到某个信号后要执行的操作，是一个 shell 命令 sigspec 表示监听的信号，可以是多个 举个🌰： trap 'echo \"Shutting Down\"' TERM #(1) 表示在接收到 SIGTERM 信号时输出 \"Shutting Down\" 添加 trap 简单了解了 trap 命令后，我们就可以来改造一下 entrypoint.sh： entrypoint.sh #!/bin/sh echo 'Do something' kill_jar() { echo 'Received TERM' kill \"$(ps -ef | grep java | grep app | awk '{print $1}')\" #(1) } trap 'kill_jar' TERM INT #(2) java -jar app.jar 找到执行的进程，使用 kill 命令向其发送 SIGTERM 在脚本中监听 SIGTERM 和 SIGINT 信号，然后执行 kill_jar 函数 上面的脚本看起来可以正常工作，但实际上不能。 这是因为在 bash 中，即使 trap 收到了信号，如果这个时候 bash 在等待一个命令结束的话， 那么 trap 就会等到这个命令结束才会被执行。 If Bash is waiting for a command to complete and receives a signal for which a trap has been set, the trap will not be executed until the command completes. — https://www.gnu.org/software/bash/manual/html_node/Signals.html#Signals 在我们的场景中，bash 就在等待 java 进程结束，才能执行 trap 中的命令。 但是 java 进程又需要 trap 来关闭才能结束，所以程序陷入了循环依赖，只能 docker stop 等待 10s。 后台运行 java 既然前面的问题是 bash 在等待 java 进程结束，那么我们就让它不等待就好了——后台执行 java： entrypoint.sh #!/bin/sh echo 'Do something' kill_jar() { echo 'Received TERM' kill \"$(ps -ef | grep java | grep app | awk '{print $1}')\" echo 'Process finished' } trap 'kill_jar' TERM INT java -jar app.jar \u0026 #(1) wait $! #(2) 后台执行 java 使用 wait 命令等待 java 进程结束，避免 entrypoint.sh 执行完成后容器直接退出 是不是觉得这样就 OK 了？ Naive，上面的这个脚本能够帮助我们立即结束容器，但并不会等待进程自己正常退出： 我们可以看到，kill_jar 方法中的 echo 被成功执行，但是却没有看到 Spring 的 shutdown hook 日志输出。 这说明容器没有等待程序正常退出就被关闭了。 这里其实有两个问题。 kill 的问题 第一个是 kill 命令并不会等待进程结束，它只负责向进程发送 SIG 信号。 至于程序如何处理、什么时候处理，则与它无瓜。 wait 的问题 第二个问题则是 wait 命令，在上面 bash 对 trap 的解释后面，还有一句话： When Bash is waiting for an asynchronous command via the wait builtin, the reception of a signal for which a trap has been set will cause the wait builtin to return immediately with an exit status greater than 128, immediately after which the trap is executed. — https://www.gnu.org/software/bash/manual/html_node/Signals.html#Signals 也就是说，虽然 bash 在等待 wait 结束，但是 wait 又被特殊处理了 ——trap 收到任何大于 128 的信号都会让 wait 命令结束，以执行 trap 中的方法。 综合以上两点，我们会发现 trap 在执行 kill_jar 时，entrypoint.sh 中的 wait 已经结束，不再等待 java 进程结束。 kill_jar 仅仅发送了 SIGTERM 信号，也不会等待 java 进程结束。 由此，我们就可以对脚本进行改进： entrypoint.sh #!/bin/sh echo 'Do something' kill_jar() { echo 'Received TERM' kill \"$(ps -ef | grep java | grep app | awk '{print $1}')\" wait $! #(1) echo 'Process finished' } trap 'kill_jar' TERM INT java -jar app.jar \u0026 wait $! 在 kill 后加了一行 wait，因为 kill 会返回进程号，所以这里也可以使用 $!。 这样，我们的 kill_jar 就会等到 java 进程完全退出后才会结束： 我们可以看到，Spring 的 shutdown hook 在 \"Process finished\" 之前输出，证明新加的 wait 命令发挥了作用。 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:2:2","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"总结 ctrl+c 与 docker stop 都只会向容器中 PID 1 进程发送信号 docker stop 默认等待 10s 没有关闭容器后，会向内核发送 SIGKILL 以强制关闭容器 解决方案： 直接启动进程时，使用 ENTRYPOINT 的 exec form 启动单一进程，并且需要一点准备工作时，使用 exec 命令 启动多个进程时，组合使用 trap、wait、kill 命令 ","date":"2020-06-18","objectID":"/graceful-shutdown-docker-container/:3:0","tags":["docker","linux","exec","trap","wait"],"title":"你的 docker stop，它优雅吗？","uri":"/graceful-shutdown-docker-container/"},{"categories":null,"content":"在前面的文章里，我们对 Spring Security 进行权限验证的组件有了大致的了解，我们首先来回顾并探究一下细节。 Figure 1. 本文涉及到的组件 ","date":"2020-06-13","objectID":"/spring-security-authorization/:0:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"FilterSecurityInterceptor 这是 AbstractSecurityInterceptor 的一个子类，并且实现了 Filter 接口，负责调用父类的 beforeInvocation()、afterInvocatio() 和 finallyInvocation() 方法以及一些 Servlet 相关的工作。 真正处理权限验证的代码，其实在父类中。 它存在的意义就是为了能在 Filter 中进行权限验证。 这个 Filter 默认总是被安排在 SecurityFilterChain 的最后，因为需要保证它在所有的身份认证相关的 Filter 之后。 ","date":"2020-06-13","objectID":"/spring-security-authorization/:1:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"AbstractSecurityInterceptor 这个类实现了真正的权限验证的逻辑，它有多个子类，是为了适配不同的技术而存在的，比如上面的 FilterSecurityInterceptor 就是为了适配 Servlet Filter 而存在的。 我们可以关注一下上面提到的三个方法，这是每个子类都会调用的。 子类的实现总是下面的套路： InterceptorStatusToken token = super.beforeInvocation(secureObject); // (1) try { // call target method, eg, filterChain.doFilter() // may get a returnedObject } final { super.finallyInvocation(token); } super.afterInvocation(token, returnedObject); secureObject 是一个方法调用，它的类型是 Object，但一般会看到 MethodInvocation 或者 FilterInvocation 这样的类型。 beforeInfocation 方法 这个方法的目标是调用 AccessDecisionManager.decide() 方法，完成 pre-invocation handling 操作。 在前面的概览中介绍过，AccessDecisionManager.decide() 方法有三个参数。其中的 secureObject 已经被子类传进来了。 那么在真正调用前，就会去获取 Authentication 对象和 Collection\u003cConfigAttribute\u003e 集合，然后进行 pre-invocation handling 操作。 后面会介绍 ConfigAttribute 如果调用时出现 AccessDecisionException，那么他将会被 ExceptionTranslationFilter 处理。 在通过权限验证之后，就会准备一个 InterceptorStatusToken 对象作为返回值。 在创建 token 之前，会尝试使用 RunAsManager 创建一个 Authentication 对象，如果这个对象不为 null，那么就会把它放入一个 SecurityContext，替换掉 SecurityContextHolder 中原有的那个。 原有的 SecurityContext 总是会被放到 token 中。 关于 RunAsManager ：这里的逻辑是替换掉 SecurityContextHolder 中的值，这样在目标方法中看到的 Authentication 对象就是这个 RunAsManager 创建的对象。在目标方法调用完成后，即 finallyInvocation 方法 中，会将原来的 SecurityContext 重新放回 SecurityContextHolder 中。 这样的目的是为了将认证与鉴权流程中的 Authentication 对象与业务方法中的区分开来。 在上面的这些步骤中，还会发出一些 ApplicationEvent，包括: PublicInvocationEvent、AuthorizationFailureEvent 和 AuthorizedEvent。 PublicInvocationEvent 只在 Collection\u003cConfigAttribute\u003e 为空的时候才会发生，而且这种时候不会调用 AccessDecisionManager。 afterInfocation 方法 afterInvocation 方法主要目的是为了根据 returnedObject 进行权限验证，这使用到了 AfterInvocationManager 这个接口，这是在概览里没有提到的，它被用来进行 after invocation handling。 在这个方法中，如果有必要的话，就会使用 AfterInvocationManager.decide() 方法来处理 returnedObject，得到一个新的结果作为 returntedObject。 这里的有必要是指： token != null afterInvocationManager 字段不为空 finallyInfocation 方法 这个方法接收 InterceptorStatusToken 作为参数，只做一件事情：将 token 中的 SecurityContext 对象放回 SecurityContextHolder 中。 这个操作有两个判断条件： token 不为 null token 的 contextHolderRefreshRequired 为 true。当 SecurityContextHolder 中的值在 beforeInvocation 中被替换时，这个值才为 true 权限验证的入口 FilterSecurityInterceptor 的介绍就到这里，接下来我们来看看 pre-invocation handling 和 after invocation handling 的内容，也就是 AccessDecisionManager 与 AfterInvocationManager。 ","date":"2020-06-13","objectID":"/spring-security-authorization/:1:1","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"AccessDecisionManager 这是在概览中介绍过的内容，这里可以快速的回顾一下。 AccessDecisionManager 是 pre-invocation handling 的入口。 它的三个具体实现会调用多个 AccessDecisionVoter 的实现，然后具体实现的策略来决定如何根据 voter 的结果来判断是否通过身份验证。 每一个 voter 都会根据当前的 Authentication 对象、secureObject 和 Collection\u003cConfigAttribute\u003e 来做出是否允许访问的选择。 AccessDecisionManager 的三个实现，其实就是三种根据 voter 结果来决定最终结果的策略，分别是 AffirmativeBased、ConsensusBased 和 UnanimousBased。策略顾名思义，就不解释了。 ","date":"2020-06-13","objectID":"/spring-security-authorization/:2:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"AfterInvocationManager 之前没有讲 after invocation handling 的部分，是觉得不重要，使用场景不多（其实是自己没遇到）。现在想讲一讲，是因为发现 spring-security-acl 使用到了 after invocation handling 的机制。那么我们就来看看 AfterInvocationManager 是怎么工作的。 acl 的部分涉及一些新的概念，准备单独写一篇。 通过这个图，我们可以清楚的了解到，AfterInvocationManager 也只是一个接口。 它的实现 AfterInvocationProviderManager 则是管理了“很多的” AfterInvocationProvider 来真正的执行权限验证的操作。 这里“很多的” AfterInvocationProvider 其实也就只有四个个实现，其中三个都是 acl，包括图里的这两个。 剩下的那个 PostInvocationAdviceProvider 其实也没有真正进行 authorization 操作，而是代理给了 PostInvocationAuthorizationAdvice 处理。 而这个 PostInvocationAuthorizationAdvice 也只有 ExpressionBasedPostInvocationAdvice 这一个实现，也就是基于 SpEL 表达式来进行 authorization 的实现。 而上面提到的所有的 manager 和 provider，都提供了 decide 方法用来做权限验证。 与 AccessDecisionManager.decide() 相比，这些方法多了一个 returnedObject 参数。 这既是因为它需要作为判断条件参与到决策过程中，也是因为它可能会在决策过程中被处理，然后返回一个新的 returnedObject 作为处理后的结果。 ","date":"2020-06-13","objectID":"/spring-security-authorization/:3:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"ConfigAttribute 这个类是用来存储我们的 Security 的配置的。 举个例子，下面的代码就会生成相应的 ConfigAttribute： @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .mvcMatchers(\"hello\") .hasAuthority(\"test\") .anyRequest() .authenticated(); } 上面的代码定义了： 访问 /hello 的请求需要具有 test 权限 其他任意请求，需要通过身份验证（不允许匿名访问） 这样我们就能得到这样的 ConfigAttribute： 这是 FilterSecurityInterceptor 的截图。 其中的 securityMetadataSource 存储了很多的 ConfigAttribute。 AbstractSecurityInterceptor 通过子类实现的 obtainSecurityMetadataSource 方法获取到它，然后通过它获取到本次使用的 Collection\u003cConfigAttribute\u003e。 截图中的 requestMap 保存了 RequestMatcher 与 Collection\u003cConfigAttribute\u003e 的关系。 当我们请求 /hello 时，就会得到第一个 Collection\u003cConfigAttribute\u003e，也就是包含了 hasAuthority('test') 的那一个。 当我们请求其他接口时，就会得到第二个。 接着，这些被获取到的 ConfigAttribute 就可以被后续的验证逻辑使用到。 ","date":"2020-06-13","objectID":"/spring-security-authorization/:4:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"总结 本文介绍了 Spring Security Authorization，并着重介绍了 FilterSecurityInterceptor 如何在 SecurityFilterChain 的最后使用 AccessDecisionManager 和 AfterInvocationManager 来实现 pre-invocation handling 和 after invocation handling。 对于 AccessDecisionManager 和 AfterInfocationManager，则没有详细介绍内部的逻辑，而是介绍了它们如何利用子类和其他接口来完成权限验证的。其内部具体的细节逻辑，读者可以自己研究。 查看系列文章： 点这里 ","date":"2020-06-13","objectID":"/spring-security-authorization/:5:0","tags":["Spring","Spring Security"],"title":"Spring Security 的权限验证","uri":"/spring-security-authorization/"},{"categories":null,"content":"本文介绍 Spring Security 的身份认证的内容，研究 Spring Security 自带的身份认证方式和添加自己的身份认证方式的方法。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:0:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"身份认证相关组件 在上一篇文章中，我们了解到了 Spring Security 会将 DelegatingFilterProxy 插入到 Servlet Filter Chain 中，然后将要过滤的请求通过 FilterChainProxy 代理给匹配的 SecurityFilterChain；这些 SecurityFilterChain 中包含着真正做安全相关工作的 Filter。 Figure 1. 后面提到的 Filter 都是红色方框中的 这些 Filter 中的一部分，他们的职责就是进行身份验证，比如 UsernamePasswordAuthenticationFilter；而他们中的大多数都有一个共同的父类 AbstractAuthenticationProcessingFilter。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:1:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"AbstractAuthenticationProcessFilter 这个类是很多身份认证的 Filter 的父类，它已经实现了 doFilter 方法，流程如下： 本文不涉及其中的 sessionStrategy 部分 doFilter 已经帮我们搭好了这个流程，我们只需要关心其中的几个被调用的方法(红绿蓝三个颜色框)就可以了。 attemptAuthentication 这是一个抽象方法。子类实现的时候，需要从 HttpServletRequest 中获取需要的信息，构建出一个 Authentication 实例，然后调用父类中的 AuthenticationManager.authenticate() 方法，对 Authentication 对象进行认证。 unsuccessfulAuthentication 这个方法已经被实现，子类也可以选择重写。根据父类的实现，这个方法将完成一下步骤： 清理 SecurityContextHolder 清除 RememberMeService 中的信息（默认使用 NullRememberMeService） 调用 AuthenticationFailureHandler.onAuthenticationFailure() 方法 默认使用的 AuthenticationFailureHandler 是 SimpleUrlAuthenticationFailureHandler，它的逻辑是： 如果没有配置 defaultFailureUrl （默认没有） 发送 401 响应 根据配置的布尔值 forwardToDestination （默认为 false）判断 使用 Servlet forward 到配置的 defaultFailureUrl 使用 HTTP redirect 到配置的 defaultFailureUrl successfulAuthentication 与 unsuccessfulAuthentication 方法一样，这个方法也已经实现，并且可以被重写，但其中的逻辑却恰好相反： 将 attemptAuthentication 返回的 Authentication 对象保存到 SecurityContextHolder 中 保存登陆信息到 RememberMeService 中 发布 InteractiveAuthenticationSuccessEvent 事件，这样可以被配置的 EventListener 处理 调用 AuthenticationSuccessHandler.onAuthenticationSuccess() 方法 默认使用的 AuthenticationSuccessHandler 是 SavedRequestAwareAuthenticationSuccessHandler，其实现就是一次重定向。我们可以看看它重定向到哪里: 当配置了 alwaysUseDefaultTargetUrl 或指定了 targetUrlParameter 且此参数存在的时候 如果配置了 alwaysUseDefaultTargetUrl 则重定向到 defaultTargetUrl，默认是 / 如果存在 targetUrlParameter（比如 redirect_uri 之类的比较常见的参数），则重定向到这个路径 如果存在 Referer，则重定向到这个地址 重定向到 / 从 RequestCache 中找到了保存的请求 重定向到请求中设置的重定向地址 如果还是没有满足条件，则进行第一步里的逻辑 关于 RequestCache：想象你正在访问一个需要认证的资源，这个时候网站会把你重定向到登陆页面；在你登陆成功后，又会重定向回刚才的资源。RequestCache 就是为了保存登陆之前的请求而设计的。在这里，默认使用基于 session 的实现。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:1:1","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"AuthenticationManager 在 AbstractAuthenticationProcessingFilter 中保存了一个 AuthenticationManager，它会在子类的 attemptAuthentication 方法中被使用。其职责是对 Filter 创建的 Authentication 对象进行身份验证，比如查询数据库匹配用户名密码、携带的 token 是否合法等。 ProviderManager 与 AuthenticationProvider 这是 AuthenticationManager 常用的实现。它没有实现任何认证逻辑，而是管理了一些 AuthenticationProvider，通过这些 provider 来实现真正的认证功能。 每个 AuthenticationProvider 实现一种特定类型的身份认证方式，比如用户名密码登陆、OIDC 登陆等。他们可以通过 Authentication 的具体类型来判断是否支持这种 Authentication 需要的认证方式。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:1:2","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"其他的一些 Filter 除了 AbstractAuthenticationProcessFilter，还有一些进行身份验证的 Filter，它们并没有继承这个类，而是基于 OncePerRequestFilter 自己实现了一套逻辑。这些 Filter 包括 AuthenticationFilter 、BasicAuthenticationFilter 、OAuth2AuthorizationCodeGrantFilter 等等。 由于它们不再是 AbstractAuthenticationProcessFilter，所以不会再被要求使用 AuthenticationManager。尽管这样，当我们选择使用 OncePerReuqestFilter 来实现自定义的身份认证时，仍然可以考虑使用 AuthenticationManager 这种方式。 个人觉得 AuthenticationManager 还算是个不错的设计，因为做到了职责分离。 甚至还有更加放飞自我的 DigestAuthenticationFilter，直接继承 GenericFilterBean，在实现上也是我行我素，这里就不探究了。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:1:3","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"ExceptionTranslationFilter 这个 Filter 不是用来进行身份验证的，而是用来处理认证授权过程中产生的异常的。它可以处理 AuthenticationException 和 AccessDeniedException，分别表示认证失败和授权失败。这篇文章只关心如何处理 AuthenticationException。 但是这个 Filter 默认被安排在 SecurityFilterChain 的倒数第二位，所以前面的 Filter 抛出的异常并不能被它捕获。但自定义的 Filter 可以加到它后面，这样就可以利用它来处理这两种异常。 最后一位是 FilterSecurityInterceptor，可能会抛出 AccessDeniedException。 处理 AuthenticationException ExceptionTranslationFilter 对 AuthenticationException 的处理分三步： 清理 SecurityContextHolder 中的身份信息 将当前的 request、response 保存到 RequestCache 中（用途可以回顾一下 successfulAuthentication 方法） 调用 AuthenticationEntryPoint.commence() 方法 其中的 AuthenticationEntryPoint 具体实例取决于你的配置，默认会用到 BasicAuthenticationEntryPoint。这个接口的职责就是通过 WWW-Authenticate header 告诉客户端使用哪种方式进行身份验证。 处理其他异常 对于 AuthenticationException 和 AccessDeniedException 之外的异常，ExceptionTranslationFilter 会将其转换成 ServletException 或 RuntimeException 抛出。 如果想要处理这些异常，需要自己添加 Filter 实现。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:1:4","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"Spring Security 自动配置的 FilterChainProxy 当我们启动 Spring 应用之后，会在日志里看到打印所有配置的 FilterChainProxy。 默认情况下，我们会看到这样的一条链： 这是引入 spring-boot-starter-security 之后自动配置的 FilterChainProxy，在引入更多的 security 相关的依赖和编写了相关配置之后，这个 filter chain 也会相应变化。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:2:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"几种内置的身份认证方式 接下来，我们以 UsernamePawwrodAuthenticationFilter 和 BasicAuthenticationFilter 为例，看看他们是如何实现身份认证的。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:3:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"UsernamePasswordAuthenticationFilter UsernamePasswordAuthenticationFilter 是一个 AbstractAuthenticationProcessingFilter 的子类，实现了 attemptAuthentication 方法，没有重写其他方法。所以用户认证成功后，会被重定向到一个地址，具体逻辑参考上面的 successfulAuthentication 方法。 attemptAuthentication attemptAuthentication 方法会从 HttpServletRequest.getParameter() 方法中获取用户名密码，从而进行身份验证。具体从哪里获取用户名密码，则可以被子类通过重写 obtainUsername() 和 obtainPassword() 方法修改。 之后，UsernamePasswordAuthenticationFilter 会构建出一个 UsernameAuthenticationToken，交给 AuthenticationManager 进行认证。 DaoAuthenticationProvider 这是 UsernamePasswordAuthenticationFilter 对应的 AuthenticationProvider，负责对 UsernameAuthenticationToken 进行认证。 它使用一个 UserDetailsService 来加载用户信息，使用 PasswordEncoder 来匹配用户的密码。 这两个接口具体使用哪一个实现，取决于具体的配置。比如 UserDetailsService 就有 in memory 和 JDBC 的实现。 UsernamePasswordAuthenticationFilter 是用于单独处理登录的 Filter，它不是用来在请求业务 API 时进行身份认证的 Filter。 事实上，所有继承了 AbstractAuthenticationProcessFilter 但没有重写 successfulAuthentication 方法的 Filter 都是这样的，它们会在登陆成功后重定向到登录前的地址或默认的地址。这也符合它的语义：进行身份认证流程，而不是业务请求的一部分。 ","date":"2020-06-07","objectID":"/spring-security-authentication/:3:1","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"BasicAuthenticationFilter 与 UsernamePasswordAuthenticationFilter 不同，BasicAuthenticationFilter 没有继承 AbstractAuthenticationProcessingFilter，而是直接继承 OncePerRequestFilter。因为它是被使用在请求业务 API 的请求上，而不是进行身份认证流程。 BasicAuthenticationFilter 的实现并不复杂，无非是从 Authorization header 中取出用户名密码，然后创建出 UsernameAuthenticationToken，接着调用 AuthenticationManager.authenticate() 方法。 之所以它也会使用 AuthenticationManager，应该是出于复用的考虑。这样它就可以使用和 UsernamePasswordAuthenticationFilter 一样的 AuthenticationProvider。 它与 UsernamePasswordAuthenticationFilter 的区别在于认证之后的行为。 无论认证成功与否，BasicAuthenticationFilter 都不会做出重定向的响应。 如果认证失败，则通过默认的 BasicAuthenticationEntryPoint 返回 401 响应 如果认证成功，则继续执行 filter chain，这样就能执行到真正的业务方法 ","date":"2020-06-07","objectID":"/spring-security-authentication/:3:2","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"如何添加自己的身份认证方式 前面介绍了两种不同的 Filter 实现，以及它们被使用的场景，现在我们知道了该选择哪一种方式去实现自定义的 Filter。但是，如何把它们加入到 SecurityFilterChain 中去处理身份认证呢？ ","date":"2020-06-07","objectID":"/spring-security-authentication/:4:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"配置 SecurityFilterChain 我们如果需要任何对 SecurityFilterChain 的配置，都需要扩展 WebSecurityConfigurerAdapter，实现自己的一个配置类。每创建这样的一个实现，都会创建一个 SecurityFilterChain 加入到 FilterChainProxy 中。 配置 requestMathcer 我们在前一篇文章提到过，FilterChainProxy 需要根据 url 来判断选择哪一个 SecurityFilterChain。我们需要将这个配置写到这个实现类中，比如： @Override protected void configure(HttpSecurity http) throws Exception { http.requestMatchers(matcher -\u003e matcher.mvcMatchers(\"/hello\")); } 这样，FilterChainProxy 就知道了对 /hello 的请求需要使用这个 SecurityFilterChain。 向 SecurityFilterChain 加入 Filter 现在有了对应的 SecurityFilterChain，我们就可以将自定义的 Filter 加入到这个 chain 中： @Override protected void configure(HttpSecurity http) throws Exception { http.addFilter(HelloFilter.class); } addFilter 方法也有一些变体，可以控制 Filter 在 chain 中的位置，这里就不赘述了。 添加 AuthenticationProvider 与 Filter 一样，AuthenticationProvider 也是被安排到单独的 FilterChainProxy 中的，并且需要自己配置。如果你的自定义 Filter 需要 AuthenticationProvider 的话，同样需要配置： @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.authenticationProvider(HelloAuthenticationProvider.class); } ","date":"2020-06-07","objectID":"/spring-security-authentication/:4:1","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"总结 这篇文章比较详细的梳理了 AbstractAuthenticationProcessingFilter 及其子类 UsernamePasswordAuthenticationFilter 的实现和 BasicAuthenticationFilter 的实现，了解了需要实现自定义身份验证的 Filter 时应该选择哪种方式： 只是进行身份验证，完成后进行重定向，而不调用业务方法，那么就继承 AbstractAuthenticationProcessFilter 需要调用业务方法，身份验证是为了保护业务，那么就继承 OncePerRequestFilter，完全控制认证的流程 当然，这不是一个强制的限制，你仍然可以通过重写 AbstractAuthenticationProcessFilter.successfulAuthentication() 方法来修改重定向的行为。 另外，也了解到了实现完 Filter 后，需要实现 WebSecurityConfigurerAdapter，将 Filter 加入到 SecurityFilterChain 中。 查看系列文章： 点这里 ","date":"2020-06-07","objectID":"/spring-security-authentication/:5:0","tags":["Spring","Spring Security"],"title":"Spring Security 中的身份认证","uri":"/spring-security-authentication/"},{"categories":null,"content":"Spring Security Servlet ","date":"2020-05-31","objectID":"/series-index/:0:0","tags":null,"title":"话题系列","uri":"/series-index/"},{"categories":null,"content":"Spring Security 系列的文章列表： Spring Security 概览 Spring Security 中的身份认证 Spring Security 中的权限验证 Spring Security ACL 核心概念和组件 如何使用 Spring Security ACL ","date":"2020-05-31","objectID":"/series/spring-security-servlet/:0:0","tags":null,"title":"Spring Security","uri":"/series/spring-security-servlet/"},{"categories":null,"content":"Spring Security 是 Spring 框架中用于实现 Security 相关需求的项目。我们可以通过使用这个框架来实现项目中的安全需求。 今天这篇文章将会讨论 Spring Security Servlet 是如何工作的。 之所以将内容限定到 Servlet，是因为现在 Spring Security 已经开始支持 Reactive Web Server，因为底层的技术不同，当然需要分开讨论。 == Spring Security 在哪里生效 我们知道，在 Servlet 中，一次请求会经过这样的阶段: client -\u003e servlet container -\u003e filter -\u003e servlet 而 Spring MVC 虽然引入了一些其他概念，但整体流程差别不大： image::filter-and-interceptor.png[role=“center”] Spring Security 则是通过实现了 Filter 来实现的 Security 功能。这样一来，只要使用了 Servlet Container，就可以使用 Spring Security，不需要关心有没有使用 Spring Web 或别的 Spring 项目。 === DelegatingFilterProxy 这是 Spring Security 实现的一个 Servlet Filter。它被加入到 Servlet Filter Chain 中，将 filter 的任务桥接给 Spring Context 管理的 bean。 ==== FilterChainProxy 这是被 DelegatingFilterProxy 封装的一个 Filter，其实也是一个代理。这个类维护了一个 List\u003cSecurityFilterChain\u003e，它会将请求代理给这个 list 进行 filter 的工作。 但这个代理不是遍历整个 list，而是通过 RequestMatcher 来判断是否要使用这一个 SecurityFilterChain。我们配置时写的 mvcMatchers 之类的方法就会影响到这里的判断。 ===== SecurityFilterChain 这个接口的实现维护了一个 Filter 列表，这些 Filter 是真正进行 filter 工作的类，比如 CorsFilter、UsernamePasswordAuthenticationFilter 等。 上面提到的 RequestMatcher 是这个接口的默认实现使用的。 综上，我们可以得到一个 big picture： image::multi-securityfilterchain.png[role=“center”] === 处理 Security Exception 这里说的 Security Exception，其实只有两种：AuthenticationException 和 AccessDeniedException。它们会在 ExceptionTranslationFilter 中被处理，而这个 Filter 往往被安排在 SecurityFilterChain 的最后。 ==== AuthenticationException 这个异常代表身份认证失败。ExceptionTranslationFilter 会调用 startAuthentication 方法处理它，其流程是： 清理 SecurityContextHolder 中的身份信息（后面的身份认证内容会涉及） 将当前请求保存到 RequestCache 中，当用户通过身份验证后，会从其中取出当前请求，继续业务流程 调用 AuthenticationEntryPoint，要求用户提供身份信息。方式可以是重定向到登陆页面，也可以是返回携带 WWW-Authenticate header 的 HTTP 响应 ==== AccessDeniedException 这个异常代表权限验证失败，意味着当前用户的身份已确认，但被服务拒绝了请求。 ExceptionTranslationFilter 会将这个异常交给 AccessDeniedHanlder 处理。默认的实现会重定向到 /error，并得到一个 403 响应。 了解了 Spring Security 在哪里生效之后，我们再来看看两个重要的问题：身份认证和权限验证。 == 身份认证 === SecurityContextHolder SecurityContextHolder 是保存身份信息的地方，默认通过 ThreadLocal 的方式保存 SecurityContext。可以通过静态方法 SecurityContextHolder.getSecurityContext() 获取当前线程的 SecurityContext。 SecurityContextHolder.getSecurityContext() 方法虽然是静态的，可以在任何地方调用。但个人不建议这么做，而是应该作为参数传递给使用到的方法，避免当前的 SecurityContext 成为隐式输入。 SecurityContext 是一个接口，提供 getAuthentication 方法获取当前用户信息；setAuthentication 设置当前用户信息。 Authentication 也是一个接口，它的实现保存了当前用户的信息。在身份验证的流程中，总是在围绕着 Authentication 操作 —— 通过 Principal 和 Credentials 判断用户身份、通过调用 setAuthenticated 方法保存身份认证是否通过的结果。 另外，在身份验证成功后，Authentication 中还保存了 GrantedAuthority 的集合，表示当前用户的角色和权限，用于后续的权限验证操作。 image::securitycontextholder.png[role=“center”] === AuthenticationManager AuthenticationManager 提供了 authenticate() 方法用于进行身份验证，但并不是它自己完成，而是通过 AuthenticationProvider 完成。 AuthenticationProvider 提供 support(Authentication) 方法用于判断是否能够验证这种类型的 Authentication。 在 AuthenticationManager 的实现 ProviderManager 中保存了 List\u003cAuthenticationProvider\u003e。它会按顺序调用支持当前 Authentication 类型的 AuthenticationProvider 的 authenticate 方法，直到身份验证成功（返回值 non-null）或全部失败。 在这个过程中出现的 AuthenticationException 将会被上面提到的 ExceptionTranslationFilter 处理。 === AbstractAuthenticationProcessingFilter AbstractAuthenticationProcessingFilter.doFilter() 方法实现了身份验证的流程，包括成功和失败的处理。 它提供了一个抽象方法 attemptAuthentication() 用于身份验证。子类可以调用它的 authenticationManager 来实现 authenticate 的功能。 整体流程如图： image::abstractauthenticationprocessingfilter.png[role=“center”] 其中的 1 \u0026 2 都在 attemptAuthentication() 方法中完成，需要子类实现。 3 通过 successfulAuthentication() 方法实现，可以被子类重写。 4 中除 SessionAuthenticationStrategy 外都交给 unsuccessfulAuthentication() 方法处理，同样可以被子类重写。 考虑到越来越多的应用都是基于无状态的 RESTful API，所以 SessionAuthenticationStrategy 不会在本文涉及 == 权限验证 === 在 Servlet 中权限验证 Spring Security 权限验证的入口有很多处，关注到 Servlet 上的话，那就是 FilterSecurityInterceptor 这个 Filter。他会被配置到所有的 AbstractAuthenticationProcessingFilter 子类之后，这样他就能从 SecurityContextHodler 中得到 Authentication，用以进行权限验证。 ==== AccessDecisionManager 权限验证的过程，被交给 AccessDecisionManager 实现，他的 decide 方法接收三个参数： Authentication：这就是从 SecurityContextHolder 中拿到的对象 secureObject：这是一个 Object 类型，对于 FilterSecurityIntercepter 来说，会用 request、response 和 filterChain 创建一个 FilterInvocation 对象作为 secureObject Coll","date":"2020-05-31","objectID":"/spring-security-servlet-overview/:0:0","tags":["Spring","Spring Security"],"title":"Spring Security Servlet 概览","uri":"/spring-security-servlet-overview/"},{"categories":null,"content":"在上一篇文章中，我们在没有使用任何插件的情况下，练习了使用 Gradle 构建 Java 项目，最后得到一个脆弱的构建脚本和不符合约定的目录结构。 对此，Gradle 使用了插件来解决这些问题。 插件 Gradle 中的插件，可以给我们带来很多好处，包括： 添加 Task 添加领域对象 约定优于配置的实现 扩展 Gradle 的核心类 Gradle 将插件分为两类，Script Plugin \u0026 Binary Plugin。 那些写到单独的 gradle 文件中，并被 build.gradle 文件使用的脚本文件，就是 Script Plugin。常见的实践是将某一插件或某一方面的配置写到单独的文件中，比如 jacoco.gradle，然后通过下面的语法导入到 build.gradle 文件中： apply from: file(\"$projectDir/gradle/jacoco.gradle\") 而常见的 java、idea 这样的 core Plugin 和 org.springframework.boot 等可以在 https://plugins.gradle.org/ 找到的插件，就是 Binary Plugin，它们通过 plugins{} 语法块引入： plugins { id 'java' id 'org.springframework.boot' version '2.2.4.RELEASE' } 接下来，我们接着上一篇文章的例子，使用 Java Plugin 来改造我们的构建脚本。 改造 Hello World Java 插件的文档：https://docs.gradle.org/current/userguide/java_plugin.html ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:0:0","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"Import Java Plugin 如上所述，我们使用 Java Plugin 需要先导入它： plugins { id 'java' } 因为 Java 插件是 Gradle 提供的核心插件，它是和 Gradle 版本绑定的，所以不需要使用 version 参数。 ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:1:0","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"SourceSet 引入 Java 插件后，我们先来了解一个核心概念：SourceSet。这是 Java 插件引入的概念，每一个 SourceSet 都包含了一组相关的资源。默认情况下，一个 SourceSet 对应 src 目录下的一个目录，目录名称就是 SourceSet 的名称；目录下会有一个 java 目录和一个 resources 目录。根据约定，这两个目录分别是存放 java 文件的目录和存放配置等资源文件的目录。 SourceSet 还有更多的信息可以配置，参见：https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_source_sets Java 插件还默认配置好了两个 SourceSet，分别是 main \u0026 test。所以在使用 Java 插件后，无需任何配置，就可以得到约定的目录结构： ❯ tree src src ├── main │ ├── java │ └── resources └── test ├── java └── resources 所以，我们需要将 HelloWorld.java 从 src 目录移动到符合约定的 src/main/java 目录下： ❯ tree src src └── main └── java └── HelloWorld.java ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:2:0","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"Task ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:3:0","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"Java 插件引入的 Task 接着我们来看看 Task 需要做哪些修改。 Java 插件引入了下面的这些 Task，并且添加了依赖关系： 其中有四个 task 是由 base plugin 添加的：clean, check, assemble 和 build。 其中，check, assemble 和 build 是 lifecycle task，本身不执行任务，只是定义了执行它们时应该执行什么样的任务： check：聚合所有进行验证操作的 task ，比如测试 assemble：聚合所有会产生项目产出物的 task，比如打包 build：聚合前面两个 task 其他的 task 中，很容易发现，compileJava 与 compileTestJava、processResources 与 processTestResources、classes 与 testClasses 命名类似。实际上，每一对 task 表达的是同样的含义，只是一个针对 main sourceSet，一个针对 test sourceSet 而已。如果你创建了一个自定义的 SourceSet，那 Java 插件会自动的添加 compileSourceSetJava、processSourceSetResources 和 sourceSetClasses，其中的 sourceSet 就是 SourceSet.name。 compileJava：编译该 sourceSet下的 java 文件 processResource：将该 sourceSet 中的资源文件复制到 build 目录中 classes：准备打包和执行需要的 class 文件和资源文件 注意，执行测试是 test 任务，它没有因为添加 sourceSet 而自动添加 sourceSetTest 方法。因为自定义的 SourceSet 不一定是组件测试之类的不同类别的测试。所以，如果你添加了这样的 SourceSet，需要自己手动编写 Test 类型的测试 task。 ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:3:1","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"改进 Hello World 由上面的了解可知，Java 插件已经为我们添加了 compileJava 和 jar 这两个 task，所以我们不需要再创建这样的 task。但是我们还是可以对这些 task 进行配置。 比如，我们仍然希望控制 jar 产出的文件名，那我们的脚本就可以改成这样： // task compileJava(type: JavaCompile) { // source fileTree(\"$projectDir/src\") // include \"**/*.java\" // destinationDir = file(\"${buildDir}/classes\") // sourceCompatibility = '1.8' // targetCompatibility = '1.8' // classpath = files(\"${buildDir}/classes\", configurations.forHelloWorld) // } // tasks.create('jar', Jar) jar { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' // from compileJava.outputs // include \"**/*.class\" manifest { attributes(\"something\": \"value\") } // setDestinationDir file(\"$buildDir/lib\") } 其中注释的部分可以删除，这里仅仅作为修改前后的对比。 根据 assemble 的定义，我们的 fatJar 的输出应当看作项目的产出物，所以需要让 assemble 依赖于 fatJar ： assemble.dependsOn fatJar ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:3:2","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"Dependency Configuration ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:4:0","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"Java 插件引入的 Configuration 上一篇文章讲到，在 Gradle 中声明依赖，需要关联到 configuration。Java 插件也提前为我们设计了一些 configuration，他们的主要关系可以通过两幅图来表示。 与 main sourceSet 相关的： 其中： 灰色文字表示已经被废弃的 configuration 绿色表示用于声明依赖的 configuration 蓝灰色表示给 task 使用的 configuration 浅蓝色表示 task 由这个图，我们就能看出声明到不同 configuration 中的依赖最终会在什么地方使用到。 与 test sourceSet 相关的： 其中的字体和颜色与上一张图一致。 我们可以看到，除去 compile, implementation, runtime 和 rumtimeOnly，其他的 configuration 与上图几乎一致。这里画出他们，仅仅是为了展示出扩展关系而已。 如果你使用过以前版本的 Gradle，想必会比较好奇为什么 Compile 会被废弃。这其实是出于构建工具的性能的考虑，关闭掉不必要的传递依赖。 你也许也发现了，和 task 一样，有一些名称相近的 configuration，所以很自然的推测：添加了自定义的 SourceSet 后，Java 插件会自动的添加一些 configuration。这些 sourceSet configuration 都可以在 Java 插件的页面上找到。 ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:4:1","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"改进 Hello World 首先，我们可以直接使用 Java 插件提供的 implementation，而不需要自己创建任何 configuration: // configurations { // forHelloWorld // } dependencies { // forHelloWorld group: 'com.google.guava', name: 'guava', version: '28.2-jre' implementation group: 'com.google.guava', name: 'guava', version: '28.2-jre' } 同样，注释只是为了对比。 接着，我们的 fatJar 也不能再使用 forHelloWorld 这个 configuration，但也不能直接使用 implementation，而应该使用 runtimeClasspath 这个给 task 消费的、语义更符合我们使用目标的 configuration： task('fatJar', type: Jar) { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' archiveClassifier = 'boot' from compileJava // from configurations.forHelloWorld.collect { from configurations.rumtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) } manifest { attributes \"Main-Class\": \"HelloWorld\" } setDestinationDir file(\"$buildDir/libs\") } 总结 经过使用 Java 插件，并对构建脚本的修改，我们得到了更具有鲁棒性、实现了约定优于配置的构建脚本。 完整的脚本如下： plugins { id 'java' } repositories { mavenCentral() } dependencies { implementation group: 'com.google.guava', name: 'guava', version: '28.2-jre' } compileJava.doLast { println 'compile success!' } jar { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' manifest { attributes(\"something\": \"value\") } } task('fatJar', type: Jar) { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' archiveClassifier = 'boot' from compileJava from configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) } manifest { attributes \"Main-Class\": \"HelloWorld\" } setDestinationDir file(\"$buildDir/libs\") } assemble.dependsOn(fatJar) ","date":"2020-02-24","objectID":"/use-gradle-to-build-java-project/:4:2","tags":["gradle","java"],"title":"使用 Gradle 的 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project/"},{"categories":null,"content":"本文目标是探索在没有使用任何额外插件的情况下，如何使用 Gradle 构建一个 Java 项目，以此对比使用 Java 插件时得到的好处。 初始化项目 使用 Gradle Init 插件提供的 init task 来创建一个 Gradle 项目： gradle init --type basic --dsl groovy --project-name gradle-demo 运行完成后，我们将得到这些文件： ❯ tree . ├── build.gradle ├── gradle │ └── wrapper │ ├── gradle-wrapper.jar │ └── gradle-wrapper.properties ├── gradlew ├── gradlew.bat └── settings.gradle 接下来，我们将关注点放到 build.gradle 上面，这是接下来编写构建脚本的地方。 Hello World 首先，我们编写一个 Java 的 HelloWorld，做为业务代码的代表： public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello Wrold\"); } } 然后，将这个内容保存到 src/HelloWorld.java 文件中，不按照 maven 的约定来组织项目结构。 编译 Java 接着，我们需要给我们的构建脚本添加任务来编译刚才写的 Java 文件。这里就需要使用到 Task。关于 Task ，Gradle 上有比较详细的文档描述如何使用它：https://docs.gradle.org/current/dsl/org.gradle.api.Task.html#org.gradle.api.Task \u0026 https://docs.gradle.org/current/userguide/more_about_tasks.html。 现在，我们可以创建一个 JavaCompile 类型的 Task 对象，命名为 compileJava ： task compileJava(type: JavaCompile) { source fileTree(\"$projectDir/src\") include \"**/*.java\" destinationDir = file(\"${buildDir}/classes\") sourceCompatibility = '1.8' targetCompatibility = '1.8' classpath = files(\"${buildDir}/classes\") } 在上面的代码中，我们： 通过 source \u0026 include 方法指定了要被编译的文件所在的目录和文件的扩展名 通过 destinationDir 指定了编译后的 class 文件的存放目录 通过 sourceCompatibility \u0026 targetCompatibility 指定了源码的 Java 版本和 class 文件的版本 通过 classpath 指定了编译时使用的 classpath 那么，接下来我们就可以执行 compileJava 这个任务了： ❯ gradle compileJava ❯ tree build build ├── classes │ └── HelloWorld.class └── tmp └── compileJava ❯ cd build/classes ❯ java HelloWorld Hello World 我们可以看到，HelloWorld 已经编译成功，并且可以被正确执行。 添加第三方依赖 在实际的项目中，难免会使用到其他人开发的库。要使用别人开发的库，就需要添加依赖。在 Gradle 中添加依赖，需要做这样四个事情： 申明 repository 定义 configuration 申明 dependency 将 dependency 添加到 classpath ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:0:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"申明 repository 在 Gradle 中可以定义项目在哪些 repository 中寻找依赖，通过 dependencies 语法块申明： repositories { mavenCentral() maven { url 'https://maven.springframework.org/release' } } 因为 mavenCentral 和 jcenter 是比较常见的两个仓库，所以 Gradle 提供了函数可以直接使用。而其他的仓库则需要自己指定仓库的地址。 申明了 repository 之后，Gradle 才会知道在哪里寻找申明的依赖。 ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:1:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"定义 configuration 如果你使用过 maven 的话，也许 repository 和 dependency 都能理解，但对 configuration 却可能感到陌生。 Configuration 是一组为了完成一个具体目标的依赖的集合。那些需要使用依赖的地方，比如 Task，应该使用 configuration，而不是直接使用依赖。这个概念仅在依赖管理范围内适用。 Configuration 还可以扩展其他 configuration，被扩展的 configuration 中的依赖，都将被传递到扩展的 configuration 中。 我们可以来创建给 HelloWorld 程序使用的 configuration ： configurations { forHelloWorld } 定义 configuration 仅仅需要定义名字，不需要进行其他配置。如果需要扩展，可以使用 extendsFrom 方法： configurations { testHelloWorld.extendsFrom forHelloWorld } ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:2:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"申明 dependency 申明 dependency 需要使用到上一步的 configuration，将依赖关联到一个 configuration 中： dependencies { forHelloWorld 'com.google.guava:guava:28.2-jre' } 通过这样的申明，在 forHelloWorld 这个 configuration 中就存在了 guava 这个依赖。 ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:3:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"将 dependency 添加到 classpath 接下来，我们就需要将 guava 这个依赖添加到 compileJava 这个 task 的 classpath 中，这样我们在代码中使用的 guava 提供的代码就能在编译期被 JVM 识别到。 但就像在定义 configuration 中描述的那样，我们需要消费 configuration 以达到使用依赖的目的，而不能直接使用依赖。所以我们需要将 compileJava.classpath 修改成下面这样： classpath = files(\"${buildDir}/classes\", configurations.forHelloWorld) ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:4:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"修改 HelloWorld 完成上面四步之后，我们就可以在我们的代码中使用 guava 的代码了： import com.google.common.collect.ImmutableMap; public class HelloWrold { public static void main(String[] args) { ImmutableMap.of(\"Hello\", \"World\") .forEach((key, value) -\u003e System.out.println(key + \" \" + value)); } } 打包 前面已经了解过如何进行编译，接着我们来看看如何打包。 Java 打包好之后，往往有两种类型的 Jar： 一种是普通的 Jar，里面不包含自己的依赖，而是在 Jar 文件外的一个 metadata 文件申明依赖，比如 maven 中的 pom.xml 另一种被称作 fatJar (or uberJar) ，里面已经包含了所有的运行时需要的 class 文件和 resource 文件。 ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:5:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"创建普通的 Jar 文件 在这个练习中，我们就只关注 Jar 本身，不关心 metadata 文件。 在这里，我们自然是要创建一个 task，类型就使用 Jar ： tasks.create('jar', Jar) jar { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' from compileJava.outputs include \"**/*.class\" manifest { attributes(\"something\": \"value\") } setDestinationDir file(\"$buildDir/lib\") } 在这个例子中，我们： 指定了 archiveBaseName, archiveAppendix, archiveVersion 属性，他们和 archiveClassfier, archiveExtension 将决定最后打包好的 jar 文件名 使用 from 方法，指定要从 compileJava 的输出中拷贝文件，这样就隐式的添加了 jar 对 compileJava 的依赖 使用 include 要求仅复制 class 文件 可以使用 manifest 给 META-INF/MANIFEST.MF 文件添加信息 setDestinationDir 方法已经被标记为 deprecated 但没有替代的方法 接着，我们就可以使用 jar 进行打包： ❯ gradle jar ❯ tree build build ├── classes │ └── HelloWorld.class ├── lib │ └── base-name-appendix-0.0.1.jar └── tmp ├── compileJava └── jar └── MANIFEST.MF ❯ zipinfo build/lib/base-name-appendix-0.0.1.jar ❯ zipinfo build/lib/base-name-appendix-0.0.1.jar Archive: build/lib/base-name-appendix-0.0.1.jar Zip file size: 1165 bytes, number of entries: 3 drwxr-xr-x 2.0 unx 0 b- defN 20-Feb-22 23:14 META-INF/ -rw-r--r-- 2.0 unx 43 b- defN 20-Feb-22 23:14 META-INF/MANIFEST.MF -rw-r--r-- 2.0 unx 1635 b- defN 20-Feb-22 23:14 HelloWorld.class 3 files, 1678 bytes uncompressed, 825 bytes compressed: 50.8% ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:6:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"创建 fatJar 接着，同样使用 Jar 这个类型，我们创建一个 fatJar 任务： task('fatJar', type: Jar) { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' archiveClassifier = 'boot' from compileJava from configurations.forHelloWorld.collect { it.isDirectory() ? it : zipTree(it) } manifest { attributes \"Main-Class\": \"HelloWorld\" } setDestinationDir file(\"$buildDir/lib\") } 相比于 jar，我们的配置变更在于： 添加 archiveClassfier 以区别 fatJar 和 jar 产生的不同 jar 文件 使用 from 将 forHelloWorld configuration 的依赖全部解压后拷贝到 jar 文件 指定 Main-Class 属性，以便直接运行 jar 文件 然后我们再执行 fatJar : ❯ gradle fatJar ❯ tree build build ├── classes │ └── HelloWorld.class ├── lib │ ├── base-name-appendix-0.0.1-boot.jar │ └── base-name-appendix-0.0.1.jar └── tmp ├── compileJava ├── fatJar │ └── MANIFEST.MF └── jar └── MANIFEST.MF ❯ java -jar build/lib/base-name-appendix-0.0.1-boot.jar Hello World 总结 通过练习在不使用 Java Plugin 的情况下，使用 Gradle 来构建项目，实现了编译源码、依赖管理和打包的功能，并得到了如下完整的 gradle.build 文件： repositories { mavenCentral() } configurations { forHelloWorld } dependencies { forHelloWorld group: 'com.google.guava', name: 'guava', version: '28.2-jre' } task compileJava(type: JavaCompile) { source fileTree(\"$projectDir/src\") include \"**/*.java\" destinationDir = file(\"${buildDir}/classes\") sourceCompatibility = '1.8' targetCompatibility = '1.8' classpath = files(\"${buildDir}/classes\", configurations.forHelloWorld) } compileJava.doLast { println 'compile success!' } tasks.create('jar', Jar) jar { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' from compileJava.outputs include \"**/*.class\" manifest { attributes(\"something\": \"value\") } setDestinationDir file(\"$buildDir/lib\") } task('fatJar', type: Jar) { archiveBaseName = 'base-name' archiveAppendix = 'appendix' archiveVersion = '0.0.1' archiveClassifier = 'boot' from compileJava from configurations.forHelloWorld.collect { it.isDirectory() ? it : zipTree(it) } manifest { attributes \"Main-Class\": \"HelloWorld\" } setDestinationDir file(\"$buildDir/lib\") } 写了这么多构建脚本，仅仅完成了 Java Plugin 提供的一小点功能，伤害太明显。 完整的例子可以在这个 repo 的 commit 中找到 https://github.com/kbyyd24/gradle-practice ","date":"2020-02-22","objectID":"/use-gradle-to-build-java-project-without-plugin/:7:0","tags":["gradle","java"],"title":"使用 Gradle 但不使用 Java 插件构建 Java 项目","uri":"/use-gradle-to-build-java-project-without-plugin/"},{"categories":null,"content":"《微服务架构设计模式》这本书介绍了引入微服务架构后面临的挑战，提供了一些模式用于应对这些挑战。所有的模式都可以在 作者的网站上找到，书上只是使用了更加具体的例子和更详细的语言来阐述而已。 本文是个人的读书总结，不是系统的知识梳理，慎读。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:0:0","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"模式 为了解决特定条件下的问题的固定解决方案称为模式。 另一个词，模式语言 是指在特定领域内相关的模式的集合。 在模式这个概念中，还有一些关键点 需求 指必须解决的问题 描述了必须解决的问题和围绕这个问题的特定上下文 结果上下文 描述采用模式后可能带来的后果 好处 这个模式解决的问题 弊端 这个模式没有解决的问题 问题 这个模式带来的新问题 相关模式 指与其他模式的关系 前导模式 催生这个模式解决的需求的模式。换句话说，采用了一个模式的前导模式，可能还有没解决的问题或新引入的问题，这些问题可以通过这个模式解决。 后续模式 与前导模式相对应 替代模式 可以替换当前模式的模式 泛化模式 针对某种问题的一般性解决方案 特化模式 针对特定问题的解决方案 上面这些概念，化成思维导图： 了解了模式这个概念之后，再去看所有的微服务架构的设计模式，就能够建立起一个知识网络，把这些模式关联起来，知道什么样的问题可以使用哪个或哪些模式来解决。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:1:0","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"微服务的模式语言 作者在书中将微服务的模式语言分为了三类：应用相关、应用基础设施相关、基础设施相关，而这三类模式都是 微服务架构模式 的后续模式。这三类下还有更细粒度的划分，一点一点来看。 我不会完全按照作者的小类属于某个大类来划分，我会按照面临的问题来划分模式语言，有的模式可能出现在多个问题的解决方案中。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:0","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"服务拆分相关 这一小类被划分到了 应用相关 下，包括了两种模式：根据业务能力进行拆分 和 根据子域进行拆分。 这两个模式涉及到了 业务能力 和 子域 两概念，要理解这两个模式就要先理解这两个概念。 业务能力：能够为公司产生价值的商业活动。 子域：来自 DDD，将领域划分后的产物，一般认为一个子域就会被设计为一个服务。 业务能力这个词，对我来讲太抽象，个人理解是使用在没有采用 DDD 的环境中的替代子域的方案。而 DDD 至少实践过，也有理论支持。 这两个模式互为替代模式。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:1","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"进程间通信相关 将微服务拆分为单体后，面临的第一个问题就是服务间的访问问题，毕竟需要不同的服务相互协作，才能完成所有的业务。 要解决进程间通信的问题，首先要解决的问题就是进程在哪里。 服务发现 服务发现就是解决进程在哪里的问题，他的解决方案又可以分为 应用层服务发现 和 平台层服务发现。它们一个需要自己写代码或使用第三方类库来实现，一个需要部署平台来实现，而它们又都是两个模式的组合使用来实现的。 应用层服务发现 自注册模式 客户端发现模式 平台层服务发现 第三方注册模式 服务器发现模式 它们的典型代表分别是 Netflix 的 Eureka 和 Kubernates。这两种组合的模式互为替换模式。 服务间通信方式 解决了服务发现的问题之后，服务就能找到想要通信的服务在哪里，那么就要解决真正的通信问题。 这个问题又可以分为 同步的远程调用 和 异步的远程调用。 异步的方式，只有 消息模式 可以选择。 同步的方式，也只有 远程过程调用模式，但实现方式却多种多样。比如 REST、 gRPC 等等。但同步又会引入新的问题，那就是调用有可能失败。 针对失败的情况，可以使用 断路器模式 实现服务降级来应对。 同步的模式与异步的模式互为替换模式，一般会根据业务需求来进行选择。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:2","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"数据一致性问题 数据一致性问题算是分布式应用的一大痛点。目前的解决方案有 2PC 模式 和 Saga 模式。它们互为替换模式 其中的 Saga 模式 又可以采用 协同式 或 编排式 来实现。 书中有详细讲述如何实现 Saga 模式。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:3","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"查询相关 微服务引入的另一个问题是，查询的时候，很难进行连表查询，因为数据被划分到了不同的数据库实例中，无法连接。 针对这个问题可以使用 API 组合 或 CQRS 模式。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:4","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"外部 API 对于来自微服务系统外的请求，如果让它直接到达服务本身，就会让外部实现和服务产生紧耦合，没有做到封装。 为了解决这个问题，可以使用 API Gateway 模式 和 BFF 模式 来实现。 API Gateway 模式 是提供一个服务，对外只暴露这个服务，由这个服务来转发请求到真正的服务上。并且在这个 API Gateway 服务上，可以统一实现如认证授权、缓存等公共功能。 BFF 模式 是提供一个服务，编写一个 API 暴露到服务外部，屏蔽底层服务的 API 。外部服务调用的是 BFF 暴露的 API 。一般会针对不同类型的设备开发不同的 BFF 服务。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:5","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"服务安全性 服务间访问时，如何辨别是谁在请求，是微服务引入的问题之一。 解决方案是 访问令牌模式。这里的令牌中包含了用户信息，帮助服务判断当前的访问者是谁、有没有权限执行请求。常见的令牌是 JWT。 一般会选择使用 API Gateway 模式，在 API Gateway 服务上认证用户、颁发令牌。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:6","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"可配置性 引入微服务后，会发现配置管理是一个问题，所以引入了 外部化配置模式 来解决它。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:7","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"可观测性 这不仅仅是微服务的问题，只是在引入微服务后这个问题变得更大。针对不同的观测需求，需要采用不同的模式。 健康检查 API 模式：查看服务是否在正常运行。 日志聚合模式：在统一的地方查看所有服务的日志，而不需要到不同的地方查看。 分布式追踪模式：在追踪一个请求时，因为请求会跨域多个服务，为了追踪整条请求链路而设计。 应用程序指标模式：用于观测资源使用情况和告警。 异常追踪模式：将异常发送到特定的服务，该服务对异常进行警报、管理等工作。 审核日志记录模式：单体也需要。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:8","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"服务基底模式 考虑到所有的服务都有一些公共功能，比如日志、分布式追踪、服务发现等等，使用某种基底模式可以简化服务的开发工作。 微服务基底模式：将公共功能实现在框架上，在新加服务时，直接使用这个框架。 服务网格模式：服务网格是一个网络层，由它来实现服务发现、负载均衡等问题。 边车模式：边车是一个与服务同生同死的进程，由它来负责公共功能。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:9","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"部署模式 这里的四种部署模式和微服务没有关系，单体应用也可以使用这些模式，这没有什么强制性。 编程语言特定的发布包模式 发布为虚拟机模式 发布为容器模式 Serverless 部署模式 作者推荐的考虑顺序从下往上，但这仅仅是技术考虑的结果。实际选择时，还要考虑数据敏感性、部署环境技术限制等因素，Serverless 模式可能是最不会被选择的一个。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:10","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"测试相关 测试方面面临的挑战主要是如何验证服务本身是工作的，以及如何验证服务间的集成是工作的。 针对第一个问题，可以使用 服务组件测试模式 来解决。其中的组件是指一个服务。在这样的测试中，对外部的调用将会被 mock，测试的关注点在于服务本身是否工作。 针对第二个问题，可以使用 消费者驱动的契约测试模式 来解决。一份契约，既可以在消费者端称为 stub，又可以在服务者端做为测试用例。 值得注意的是，即使上面两个测试都通过了，你仍然需要一个针对整个系统的 E2E 测试 ，才能保证系统是正常工作的。因为从测试金字塔来看，上面两个测试是在 E2E 测试 的下一层。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:11","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":"重构的模式 毕竟微服务都是拆出来的。实践微服务时，往往面临的问题是将一个巨大的单体服务拆分为微服务。针对这样的问题，就要祭出大名鼎鼎的 绞杀着模式 。 使用这个模式需要考虑三种策略： 将新功能实现为服务 隔离表现层与后端（本质上是指责分离） 提取业务到服务中 当采用第一种和第三种策略时，都需要加入新的服务，那就需要准备好你的微服务需要的基础设施，也就是服务发现、API Gateway 等等一系列的模式都需要准备好，这样新加入的服务才能和单体一起工作。但这也不是绝对的，也可以逐渐演进，而不用一步到位。 ","date":"2020-02-01","objectID":"/microservice-arch-pattern/:2:12","tags":["微服务","microservice"],"title":"《微服务架构设计模式》读书总结","uri":"/microservice-arch-pattern/"},{"categories":null,"content":" 端口与适配器架构又被称为六边形架构，是一种约定代码设计的架构。它解决的是如何设计代码的问题，主要的关注点在于业务与技术的解耦。 第一次看到这个名字的时候，就有很多问题冒了出来： 它解决什么问题？ 什么是端口？什么是适配器？ 按照这个架构写出来的代码应该是什么样子？ 它为什么又被称作六边形架构？有什么特殊含义？ 带着这些问题，搜索了一些资料，算是有了下面的这些理解。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:0:0","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"为什么又被称作六边形架构 其实它被称作六边形架构，仅仅是因为它的作者 Dr. Alistair Cockburn 画成这样的而已，没有什么深意。这个架构和“六”这个数字以及“六边形”这个图形都没有任何关联。 那么，这个“六边形”又代表什么呢？ 这个“六边形”就是我们所说的“应用”，六边形内的东西，是与技术无关的业务代码。而这些业务代码如何设计，它并没有约定。你可以在内部使用 DDD 战术，也可以使用 “意大利面条式”的代码设计。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:1:0","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"一些概念 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:2:0","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"Actor Actor 是指那些与应用交互的组件，包括浏览器、命令行甚至其他的应用。所有这些东西都是在我们的“六边形”之外的。 Actor 也有分类，依据是与应用交互的方式： Drivers 又被称作 Primary Actors。这些 Actor 的特点是它们会调用应用，以完成业务目的。 在画六边形时，Driver 会被画到六边形的左边和上面。 Driven Actors 又被称作 Secondary Actors。这些 Actor 被应用调用，它们提供技术方面的功能以帮助应用完成业务逻辑。 在画六边形时，Driven Actor 会被画到六边形的右边和下边。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:2:1","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"Ports 端口处在“六边形”的边缘，用于应用和 Actors 的交互。 根据交互的 Actor 的不同，可以将端口分类为 Driver Port 和 Driven Port ，或者按照《微服务架构设计模式》的说法称作 入站端口 和 出站端口 。 它们被这样分类是因为： 入站端口提供了访问应用的 API 出站端口被应用调用，提供 SPI 供 Driven Actor 实现 端口描述了应用的业务功能，关注点仍然在业务上。 以 Driver Port 来说，可以提供一个 createOrder 端口提供下单的功能。 以 Driven Port 来说，可以提供一个 sendNotification 端口来要求一个发送通知的功能。 但 Actor 并不是直接和端口交互，而是通过 Adapter 来进行的。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:2:2","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"Adapters 适配器处在“六边形”之外，所有的 Actor 都是通过适配器来和 Port 交互，以达到和应用交互的效果。 适配器可以根据适配的端口的不同，分为 Driver Adapter 和 Driven Adapter （或 入站适配器 和 出站适配器 ）。 适配器就是从“六边形”中解耦出来的“技术”。 以 Driver Adapter 来说，可以实现 REST适配器 来提供 REST 风格的接口来调用端口使用应用；也可以实现 CLI适配器 来提供 CLI 接口来调用端口使用应用。 以 Driven Adapter 来说，可以实现一个 MySQL适配器 以使用 MySQL 持久化数据；也可以实现一个 MongoDB适配器 以使用 MongoDB 持久化数据；也可以实现一个 邮件通知适配器 来实现以邮件形式发送通知的功能。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:2:3","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"小结 以上就是端口与适配器架构的核心的概念。 我们可以看到，它的关注点仅仅在于如何将技术代码与业务代码分离开。所以它定义了 Actors、Ports 和 Adapters 这三个概念。而“六边形”并不是一个又特殊含义的东西，只有画出应用的边界和区别 Driver 与 Driven Actor 的作用。 这些概念中，只有 Ports 是属于应用的，其他两个概念都是在应用之外。 - 所有的业务代码，都被包含在了应用之中，而没有被泄漏到应用之外 - 所有需要与第三方交互的技术代码都被放到了 Adapters 里，也就是应用之外，没有侵入到业务代码中 解决的问题 与其他应用交互的代码的设计。 没解决的问题 业务代码怎么写。这需要使用其他的模式来解决，比如 DDD 的战术模式。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:2:4","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"如何实现 端口与适配器架构模式要求将应用与适配器分离开，这意味着实现这个模式的时候，需要对应用代码和技术代码做一些技术隔离。 对于 Java 而言，无论是使用 Maven 还是 Gradle 都能比较容易的分离 subproject。其他语言也可以使用类似的技术来实现。虽然不是特别麻烦，但总归是增加了系统的复杂度。 Java 的例子可以参考 这里。 ","date":"2020-01-05","objectID":"/hexagonal-architecture/:3:0","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"总结 总的来说，端口与适配器模式并不复杂，搞清楚三个关键概念和“六边形”没有意义这个点，就算立即到这个架构模式了。 参考连接： https://softwarecampament.wordpress.com/portsadapters/ ","date":"2020-01-05","objectID":"/hexagonal-architecture/:4:0","tags":["architecture","hexagonal-architecture","ports-and-adapters-pattern","ports-and-adapters-architecture","架构","六边形架构","端口与适配器架构"],"title":"端口与适配器架构","uri":"/hexagonal-architecture/"},{"categories":null,"content":"作为一个后端开发，我们经常遇到的一个问题就是需要配置 CORS，好让我们的前端能够访问到我们的 API，并且不让其他人访问。而在 Spring 中，我们见过很多种 CORS 的配置，很多资料都只是告诉我们可以这样配置、可以那样配置，但是这些配置有什么区别？ ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:0:0","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"CORS 是什么 首先我们要明确，CORS 是什么，以及规范是如何要求的。这里只是梳理一下流程，具体的规范请看 这里。 CORS 全称是 Cross-Origin Resource Sharing，直译过来就是跨域资源共享。要理解这个概念就需要知道域、资源和同源策略这三个概念。 域，指的是一个站点，由 protocal、host 和 port 三部分组成，其中 host 可以是域名，也可以是 ip ；port 如果没有指明，则是使用 protocal 的默认端口 资源，是指一个 URL 对应的内容，可以是一张图片、一种字体、一段 HTML 代码、一份 JSON 数据等等任何形式的任何内容 同源策略，指的是为了防止 XSS，浏览器、客户端应该仅请求与当前页面来自同一个域的资源，请求其他域的资源需要通过验证。 了解了这三个概念，我们就能理解为什么有 CORS 规范了：从站点 A 请求站点 B 的资源的时候，由于浏览器的同源策略的影响，这样的跨域请求将被禁止发送；为了让跨域请求能够正常发送，我们需要一套机制在不破坏同源策略的安全性的情况下、允许跨域请求正常发送，这样的机制就是 CORS。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:1:0","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"预检请求 在 CORS 中，定义了一种预检请求，即 preflight request，当实际请求不是一个 简单请求 时，会发起一次预检请求。预检请求是针对实际请求的 URL 发起一次 OPTIONS 请求，并带上下面三个 headers ： Origin：值为当前页面所在的域，用于告诉服务器当前请求的域。如果没有这个 header，服务器将不会进行 CORS 验证。 Access-Control-Request-Method：值为实际请求将会使用的方法 Access-Control-Request-Headers：值为实际请求将会使用的 header 集合 如果服务器端 CORS 验证失败，则会返回客户端错误，即 4xx 的状态码。 否则，将会请求成功，返回 200 的状态码，并带上下面这些 headers： Access-Control-Allow-Origin：允许请求的域，多数情况下，就是预检请求中的 Origin 的值 Access-Control-Allow-Credentials：一个布尔值，表示服务器是否允许使用 cookies Access-Control-Expose-Headers：实际请求中可以出现在响应中的 headers 集合 Access-Control-Max-Age：预检请求返回的规则可以被缓存的最长时间，超过这个时间，需要再次发起预检请求 Access-Control-Allow-Methods：实际请求中可以使用到的方法集合 浏览器会根据预检请求的响应，来决定是否发起实际请求。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:1:1","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"小结 到这里， 我们就知道了跨域请求会经历的故事： 访问另一个域的资源 有可能会发起一次预检请求（非简单请求，或超过了 Max-Age） 发起实际请求 接下来，我们看看在 Spring 中，我们是如何让 CORS 机制在我们的应用中生效的。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:1:2","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"几种配置的方式 Spring 提供了多种配置 CORS 的方式，有的方式针对单个 API，有的方式可以针对整个应用；有的方式在一些情况下是等效的，而在另一些情况下却又出现不同。我们这里例举几种典型的方式来看看应该如何配置。 假设我们有一个 API： @RestController class HelloController { @GetMapping(\"hello\") fun hello(): String { return \"Hello, CORS!\" } } ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:2:0","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"@CrossOrigin 注解 使用`@CorssOrigin` 注解需要引入 Spring Web 的依赖，该注解可以作用于方法或者类，可以针对这个方法或类对应的一个或多个 API 配置 CORS 规则： @RestController class HelloController { @GetMapping(\"hello\") @CrossOrigin(origins = [\"http://localhost:8080\"]) fun hello(): String { return \"Hello, CORS!\" } } ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:2:1","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"实现 WebMvcConfigurer.addCorsMappings 方法 WebMvcConfigurer 是一个接口，它同样来自于 Spring Web。我们可以通过实现它的 addCorsMappings 方法来针对全局 API 配置 CORS 规则： @Configuration @EnableWebMvc class MvcConfig: WebMvcConfigurer { override fun addCorsMappings(registry: CorsRegistry) { registry.addMapping(\"/hello\") .allowedOrigins(\"http://localhost:8080\") } } ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:2:2","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"注入 CorsFilter CorsFilter 同样来自于 Spring Web，但是实现 WebMvcConfigurer.addCorsMappings 方法并不会使用到这个类，具体原因我们后面来分析。我们可以通过注入一个 CorsFilter 来使用它： @Configuration class CORSConfiguration { @Bean fun corsFilter(): CorsFilter { val configuration = CorsConfiguration() configuration.allowedOrigins = listOf(\"http://localhost:8080\") val source = UrlBasedCorsConfigurationSource() source.registerCorsConfiguration(\"/hello\", configuration) return CorsFilter(source) } } 注入 CorsFilter 不止这一种方式，我们还可以通过注入一个 FilterRegistrationBean 来实现，这里就不给例子了。 在仅仅引入 Spring Web 的情况下，实现 WebMvcConfigurer.addCorsMappings 方法和注入 CorsFilter 这两种方式可以达到同样的效果，二选一即可。它们的区别会在引入 Spring Security 之后会展现出来，我们后面再来分析。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:2:3","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"Spring Security 中的配置 在引入了 Spring Security 之后，我们会发现前面的方法都不能正确的配置 CORS，每次 preflight request 都会得到一个 401 的状态码，表示请求没有被授权。这时，我们需要增加一点配置才能让 CORS 正常工作： @Configuration class SecurityConfig : WebSecurityConfigurerAdapter() { override fun configure(http: HttpSecurity?) { http?.cors() } } 或者，干脆不实现 WebMvcConfigurer.addCorsMappings 方法或者注入 CorsFilter ，而是注入一个 CorsConfigurationSource ，同样能与上面的代码配合，正确的配置 CORS： @Bean fun corsConfigurationSource(): CorsConfigurationSource { val configuration = CorsConfiguration() configuration.allowedOrigins = listOf(\"http://localhost:8080\") val source = UrlBasedCorsConfigurationSource() source.registerCorsConfiguration(\"/hello\", configuration) return source } 到此，我们已经看过了几种典型的例子了，完整的内容可以在 Demo 中查看，我们接下来看看 Spring 到底是如何实现 CORS 验证的。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:2:4","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"这些配置有什么区别 我们会主要分析实现 WebMvcConfigurer.addCorsMappings 方法和调用 HttpSecurity.cors 方法这两种方式是如何实现 CORS 的，但在进行之前，我们要先复习一下 Filter 与 Interceptor 的概念。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:3:0","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"Filter 与 Interceptor 上图很形象的说明了 Filter 与 Interceptor 的区别，一个作用在 DispatcherServlet 调用前，一个作用在调用后。 但实际上，它们本身并没有任何关系，是完全独立的概念。 Filter 由 Servlet 标准定义，要求 Filter 需要在 Servlet 被调用之前调用，作用顾名思义，就是用来过滤请求。在 Spring Web 应用中，DispatcherServlet 就是唯一的 Servlet 实现。 Interceptor 由 Spring 自己定义，由 DispatcherServlet 调用，可以定义在 Handler 调用前后的行为。这里的 Handler ，在多数情况下，就是我们的 Controller 中对应的方法。 对于 Filter 和 Interceptor 的复习就到这里，我们只需要知道它们会在什么时候被调用到，就能理解后面的内容了。 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:3:1","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"WebMvcConfigurer.addCorsMappings 方法做了什么 我们从 WebMvcConfigurer.addCorsMappings 方法的参数开始，先看看 CORS 配置是如何保存到 Spring 上下文中的，然后在了解一下 Spring 是如何使用的它们。 注入 CORS 配置 CorsRegistry 和 CorsRegistration WebMvcConfigurer.addCorsMappings 方法的参数 CorsRegistry 用于注册 CORS 配置，它的源码如下： public class CorsRegistry { private final List\u003cCorsRegistration\u003e registrations = new ArrayList\u003c\u003e(); public CorsRegistration addMapping(String pathPattern) { CorsRegistration registration = new CorsRegistration(pathPattern); this.registrations.add(registration); return registration; } protected Map\u003cString, CorsConfiguration\u003e getCorsConfigurations() { Map\u003cString, CorsConfiguration\u003e configs = new LinkedHashMap\u003c\u003e(this.registrations.size()); for (CorsRegistration registration : this.registrations) { configs.put(registration.getPathPattern(), registration.getCorsConfiguration()); } return configs; } } 我们发现这个类仅仅有两个方法： addMapping 接收一个 pathPattern，创建一个 CorsRegistration 实例，保存到列表后将其返回。在我们的代码中，这里的 pathPattern 就是 /hello getCorsConfigurations 方法将保存的 CORS 规则转换成 Map 后返回 CorsRegistration 这个类，同样很简单，我们看看它的部分源码： public class CorsRegistration { private final String pathPattern; private final CorsConfiguration config; public CorsRegistration(String pathPattern) { this.pathPattern = pathPattern; this.config = new CorsConfiguration().applyPermitDefaultValues(); } public CorsRegistration allowedOrigins(String... origins) { this.config.setAllowedOrigins(Arrays.asList(origins)); return this; } } 不难发现，这个类仅仅保存了一个 pathPattern 字符串和 CorsConfiguration，很好理解，它保存的是一个 pathPattern 对应的 CORS 规则。 在它的构造函数中，调用的 CorsConfiguration.applyPermitDefaultValues 方法则用于配置默认的 CORS 规则： allowedOrigins 默认为所有域 allowedMethods 默认为 GET 、HEAD 和 POST allowedHeaders 默认为所有 maxAge 默认为 30 分钟 exposedHeaders 默认为 null，也就是不暴露任何 header credentials 默认为 null 创建 CorsRegistration 后，我们可以通过它的 allowedOrigins、allowedMethods 等方法修改它的 CorsConfiguration，覆盖掉上面的默认值。 现在，我们已经通过 WebMvcConfigurer.addCorsMappings 方法配置好 CorsRegistry 了，接下来看看这些配置会在什么地方被注入到 Spring 上下文中。 WebMvcConfigurationSupport CorsRegistry.getCorsConfigurations 方法，会被 WebMvcConfigurationSupport.getConfigurations 方法调用，这个方法如下： protected final Map\u003cString, CorsConfiguration\u003e getCorsConfigurations() { if (this.corsConfigurations == null) { CorsRegistry registry = new CorsRegistry(); addCorsMappings(registry); this.corsConfigurations = registry.getCorsConfigurations(); } return this.corsConfigurations; } addCorsMappings(registry) 调用的是自己的方法，由子类 DelegatingWebMvcConfiguration 通过委托的方式调用到 WebMvcConfigurer.addCorsMappings 方法，我们的配置也由此被读取到。 getCorsConfigurations 是一个 protected 方法，是为了在扩展该类时，仍然能够直接获取到 CORS 配置。而这个方法在这个类里被四个地方调用到，这四个调用的地方，都是为了注册一个 HandlerMapping 到 Spring 容器中。每一个地方都会调用 mapping.setCorsConfigurations 方法来接收 CORS 配置，而这个 setCorsConfigurations 方法，则由 AbstractHandlerMapping 提供，CorsConfigurations 也被保存在这个抽象类中。 到此，我们的 CORS 配置借由 AbstractHandlerMapping 被注入到了多个 HandlerMapping 中，而这些 HandlerMapping 以 Spring 组件的形式被注册到了 Spring 容器中，当请求来临时，将会被调用。 获取 CORS 配置 还记得前面关于 Filter 和 Interceptor 那张图吗？当请求来到 Spring Web 时，一定会到达 DispatcherServlet 这个唯一的 Servlet。 在 DispatcherServlet.doDispatch 方法中，会调用所有 HandlerMapping.getHandler 方法。好巧不巧，这个方法又是由 AbstractHandlerMapping 实现的： @Override @Nullable public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { // 省略代码 if (CorsUtils.isCorsRequest(request)) { CorsConfiguration globalConfig = this.corsConfigurationSource.getCorsConfiguration(request); CorsConfiguration handlerConfig = getCorsConfiguration(handler, request); CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig); executionChain = getCorsHandlerExecutionChain(request, executionChain, config); } return executionChain; } 在这个方法中，关于 CORS 的部分都在这个 if 中。我们来看看最后这个 getCorsHandlerExecutionChain 做了什么： protected HandlerExecutionChain getCorsHandlerExecutionChain(HttpServletRequest request, HandlerExecutionChain chain, @Nullable CorsConfiguration config) { if (CorsUtils.isPreFlightRequest(request)) { HandlerInterceptor[] interceptors = chain.getInterceptors(); chain = ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:3:2","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"HtttpSecurity.cors 方法做了什么 在研究这个方法的行为之前，我们先来回想一下，我们调用这个方法解决的是什么问题。 前面我们通过某种方式配置好 CORS 后，引入 Spring Security，CORS 就失效了，直到调用这个方法后，CORS 规则才重新生效。 下面这些原因，导致了 preflight request 无法通过身份验证，从而导致 CORS 失效： preflight request 不会携带认证信息 Spring Security 通过 Filter 来进行身份验证 Interceptor 和 HttpRequestHanlder 在 DispatcherServlet 之后被调用 Spring Security 中的 Filter 优先级比我们注入的 CorsFilter 优先级高 接下来我们就来看看 HttpSecurity.cors 方法是如何解决这个问题的。 CorsConfigurer 如何配置 CORS 规则 HttpSecurity.cors 方法中其实只有一行代码： public CorsConfigurer\u003cHttpSecurity\u003e cors() throws Exception { return getOrApply(new CorsConfigurer\u003c\u003e()); } 这里调用的 getOrApply 方法会将 SecurityConfigurerAdapter 的子类实例加入到它的父类 AbstractConfiguredSecurityBuilder 维护的一个 Map 中，然后一个个的调用 configure 方法。所以，我们来关注一下 CorsConfigurer.configure 方法就好了。 @Override public void configure(H http) throws Exception { ApplicationContext context = http.getSharedObject(ApplicationContext.class); CorsFilter corsFilter = getCorsFilter(context); if (corsFilter == null) { throw new IllegalStateException( \"Please configure either a \" + CORS_FILTER_BEAN_NAME + \" bean or a \" + CORS_CONFIGURATION_SOURCE_BEAN_NAME + \"bean.\"); } http.addFilter(corsFilter); } 这段代码很好理解，就是在当前的 Spring Context 中找到一个 CorsFilter，然后将它加入到 http 对象的 filters 中。由上面的 HttpSecurity.cors 方法可知，这里的 http 对象实际类型就是 HttpSecurity。 getCorsFilter 方法做了什么 也许你会好奇，HttpSecurity 要如何保证 CorsFilter 一定在 Spring Security 的 Filters 之前调用。但是在研究这个之前，我们先来看看同样重要的 getCorsFilter 方法，这里可以解答我们前面的一些疑问。 private CorsFilter getCorsFilter(ApplicationContext context) { if (this.configurationSource != null) { return new CorsFilter(this.configurationSource); } boolean containsCorsFilter = context .containsBeanDefinition(CORS_FILTER_BEAN_NAME); if (containsCorsFilter) { return context.getBean(CORS_FILTER_BEAN_NAME, CorsFilter.class); } boolean containsCorsSource = context .containsBean(CORS_CONFIGURATION_SOURCE_BEAN_NAME); if (containsCorsSource) { CorsConfigurationSource configurationSource = context.getBean( CORS_CONFIGURATION_SOURCE_BEAN_NAME, CorsConfigurationSource.class); return new CorsFilter(configurationSource); } boolean mvcPresent = ClassUtils.isPresent(HANDLER_MAPPING_INTROSPECTOR, context.getClassLoader()); if (mvcPresent) { return MvcCorsFilter.getMvcCorsFilter(context); } return null; } 这是 CorsConfigurer 寻找 CorsFilter 的全部逻辑，我们用人话来说就是： CorsConfigurer 自己是否有配置 CorsConfigurationSource，如果有的话，就用它创建一个 CorsFilter。 在当前的上下文中，是否存在一个名为 corsFilter 的实例，如果有的话，就把他当作一个 CorsFilter 来用。 在当前的上下文中，是否存在一个名为 corsConfigurationSource 的 CorsConfigurationSource 实例，如果有的话，就用它创建一个 CorsFilter。 在当前上下文的类加载器中，是否存在类 HandlerMappingIntrospector，如果有的话，则通过 MvcCorsFilter 这个内部类创建一个 CorsFilter。 如果没有找到，那就返回一个 null，调用的地方最后会抛出异常，阻止 Spring 初始化。 上面的第 2、3、4 步能解答我们前面的配置为什么生效，以及它们的区别。 注册 CorsFilter 的方式，这个 Filter 最终会被直接注册到 Servlet container 中被使用到。 注册 CorsConfigurationSource 的方式，会用这个 source 创建一个 CorsFiltet 然后注册到 Servlet container 中被使用到。 而第四步的情况比较复杂。HandlerMappingIntrospector 是 Spring Web 提供的一个类，实现了 CorsConfigurationSource 接口，所以在 MvcCorsFilter 中，它被直接用于创建 CorsFilter。它实现的 getCorsConfiguration 方法，会经历： 遍历 HandlerMapping 调用 getHandler 方法得到 HandlerExecutionChain 从中找到 CorsConfigurationSource 的实例 调用这个实例的 getCorsConfiguration 方法，返回得到的 CorsConfiguration 所以得到的 CorsConfigurationSource 实例，实际上就是前面讲到的 CorsInterceptor 或者 PreFlightHandler。 所以第四步实际上匹配的是实现 WebMvcConfigurer.addCorsMappings 方法的方式。 由于在 CorsFilter 中每次处理请求时都会调用 CorsConfigurationSource.getCorsConfiguration 方法，而 DispatcherServlet 中也会每次调用 HandlerMapping.getHandler 方法，再加上这时的 HandlerExecutionChain 中还有 CorsInterceptor，所以使用这个方式相对于其他方式，做了很多重复的工作。所以 WebMvcConfigurer.addCorsMappings + HttpSecurity.cors 的方式降低了我们代码的效率，也许微乎其微，但能避免的情况下，还是不要使用。 HttpSecurity 中的 filters 属性 在 CorsConfigurer.configure 方法中调用的 HttpSecurity.addFilter 方法，由它的父类 HttpSecurityBuilder 声明，并约定了很多 Filter 的顺序。然而 CorsFilter 并不在其中。不过在 Spring Security 中，目前还只有 HttpSecurity 这一个实现，所以我们来看看这里的代码实现就知道 CorsFilter 会排在什么地方了。 public HttpSecurity addFilter(Filter filter) { Class\u003c? extends Filter\u003e filterClass = filter.getClass(); if (!comparator.isRegistered(filterClass)) { throw new IllegalArgum","date":"2019-06-14","objectID":"/how-spring-implement-cors/:3:3","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"总结 研究了 Spring 中 CORS 的代码，我们了解到了这样一些知识： 实现 WebMvcConfigurer.addCorsMappings 方法来进行的 CORS 配置，最后会在 Spring 的 Interceptor 或 Handler 中生效 注入 CorsFilter 的方式会让 CORS 验证在 Filter 中生效 引入 Spring Security 后，需要调用 HttpSecurity.cors 方法以保证 CorsFilter 会在身份验证相关的 Filter 之前执行 HttpSecurity.cors + WebMvcConfigurer.addCorsMappings 是一种相对低效的方式，会导致跨域请求分别在 Filter 和 Interceptor 层各经历一次 CORS 验证 HttpSecurity.cors + 注册 CorsFilter 与 HttpSecurity.cors + 注册 CorsConfigurationSource 在运行的时候是等效的 在 Spring 中，没有通过 CORS 验证的请求会得到状态码为 403 的响应 ","date":"2019-06-14","objectID":"/how-spring-implement-cors/:4:0","tags":["Spring","Spring Security","cors"],"title":"Spring 里那么多种 CORS 的配置方式，到底有什么区别","uri":"/how-spring-implement-cors/"},{"categories":null,"content":"概念 HTTP 协议中的 Origin Header 存在于请求中，用于指明当前请求来自于哪个站点。 ","date":"2019-05-23","objectID":"/http-headers-origin/:1:0","tags":["http","header","origin"],"title":"HTTP Headers 之 Origin","uri":"/http-headers-origin/"},{"categories":null,"content":"字段内容 Origin 仅仅包含站点信息，不包含任何路径信息。 ","date":"2019-05-23","objectID":"/http-headers-origin/:1:1","tags":["http","header","origin"],"title":"HTTP Headers 之 Origin","uri":"/http-headers-origin/"},{"categories":null,"content":"语法 Origin: \"\" Origin: \"\u003cschema\u003e://\u003chost\u003e[:port]\" // 例如 Origin: \"https://baidu.com\" // 错误示范，包含了路径信息 Origin: \"https://baidu.com/\" 参考： https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Origin ","date":"2019-05-23","objectID":"/http-headers-origin/:1:2","tags":["http","header","origin"],"title":"HTTP Headers 之 Origin","uri":"/http-headers-origin/"},{"categories":null,"content":"应用 ","date":"2019-05-23","objectID":"/http-headers-origin/:2:0","tags":["http","header","origin"],"title":"HTTP Headers 之 Origin","uri":"/http-headers-origin/"},{"categories":null,"content":"CORS 当我们的浏览器发出跨站请求时，行为正确的服务器会校验当前请求是不是来自被允许的站点。服务器就是通过 Origin 字段的值来进行的判断。 当服务器的配置出错时，比如配置成了 https://baidu.com/，则可能造成一些难以理解的问题。 比如有的浏览器（IE）能够请求成功，而有的浏览器却请求失败（Chrome）。这不是因为前一个浏览器行为正确，而是因为前一个浏览器发出请求时没有带上 Origin 而后一个浏览器带上了正确的 Origin。而在服务器端，因为没有 Origin Header，所以认为这不是一次 CORS 请求，所以没有进行 CORS 校验。这也反过来要求服务端强制请求带上 Origin Header，才能进一步保证服务器的安全性。 ","date":"2019-05-23","objectID":"/http-headers-origin/:2:1","tags":["http","header","origin"],"title":"HTTP Headers 之 Origin","uri":"/http-headers-origin/"},{"categories":null,"content":"JVM 提供了自动化的内存管理，使得开发者不需要编写内存回收的代码。但是，JVM 是如何工作的呢？是如何知道哪些内存应该被清理呢？又如何减小垃圾回收时产生的问题的影响呢？周志明的《深入理解 Java 虚拟机》可以给我们答案，本文主要针对垃圾收集算法做介绍。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:0:0","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"如何判断对象是否存活 JVM 要收集垃圾，那么久需要先判断哪些内容是垃圾，需要被收集。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:1:0","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"引用计数算法 该算法是指，给对象添加一个引用计数器，每当有一个地方引用它时，计数器加一；当引用失效时，计数器减一。 这个算法看上去很简单，但是有一个很大的问题，就是没有办法解决循环引用的问题。比如，A 引用了 B，B 引用了 C，C 又引用了 A。这样每个对象都被引用了一次，但是有可能 ABC 三个对象我们都不再需要，也就是它们都是垃圾。但这个算法却会认为它们都被引用了，所以它们都不是垃圾，也就永远都不会被回收了。 所以这个算法不应该被采用。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:1:1","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"可达性分析算法 这个算法的思路是，定义一系列称作 GC Roots 的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径成为引用链（Reference Chain），当一个对象到 GC Roots 没有任何引用链相连时，说明这个对象是不可达对象。 这个算法就解决了循环引用的问题，并且也是 JVM 中主流的实现。 JVM 中可以作为 GC Roots 的对象有： 虚拟机栈中引用的对象 方法区中静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 Native 方法引用的对象 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:1:2","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"如何清理垃圾 前面判断了一个对象是不是垃圾，接下来，就要看看我们应该如何清理这些垃圾了。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:2:0","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"标记-清除算法（Mark-Sweep） 这个算法顾名思义，就是先标记出需要回收的对象，然后清除它们。这个算法没有被采用，主要是两个原因： 效率低：无论是 标记 还是 清除，这两个过程的效率都很低下。 容易产生空间碎片：在 清除 过后，容易产生大量不连续的内存碎片，导致在分配大对象时找不到足够的连续空间来分配。 由此，人们基于这个算法进行改进，衍生出了后来的算法。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:2:1","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"复制算法（Copying） 将内存按容量分为大小相等的两块，每次只使用其中一块。垃圾回收时，将还存活的对象复制到另一块上面去，然后将刚才使用的这一块空间全部清除。 优点： 效率更高：每次都是对整个半块内存进行回收 避免了空间碎片问题 缺点： 能够使用的内存只有一半，利用率仅有 50%。 复制算法经过改良后，被广泛的使用到 新生代 的内存回收中。 改进 IBM 的研究表面，JVM 新生代中 98% 的对象都是“朝生夕死”的，所以没有必要按照 1:1 来划分内存空间。而在 HotSpot 的实现中，恰好是这样设计的。 HotSpot 将 新生代 分为一块较大的 Eden 空间和两块较小且大小相等的 Survivor 空间，默认比例是 8:1:1。 每次使用 Eden 和其中一块 Survivor 回收时，将 Eden 和 Survivor 中还存活的对象复制到另一块 Survivor 中去 清理掉 Eden 和刚才使用的那块 Survivor 如果 Survivor 没有足够的空间存放对象，那么这些对象需要通过内存担保机制存入老年代 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:2:2","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"标记-整理算法（Mark-Compact） 顾名思义，与 标记-清除 算法的区别是，该算法会让所有存活的对象都向一端移动，然后直接清理掉边界之外的内存。这样可以成功的避免内存碎片。 标记-整理 算法没有解决效率低的问题，所以显而易见，没有理由在 新生代 中用它替换 复制 算法。但由于 复制 算法按 1:1 划分时会浪费空间，划分 Eden \u0026 Survivor 又需要内存担保，所以在对象存活率较高的老年代中不适合使用。而 标记-整理/标记-清理 算法则更适合于这样的场景。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:2:3","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"分代收集算法 这并不是新的算法，而是根据对象存活周期的不同，将内存分为几块，分别采用不同的收集算法。 我们一般把 Java 堆 分为新生代和老年代。 新生代 研究表明，每次收集新生代内存时，都有大量对象死去，只有少量对象存活，所以采用改进的 复制 算法，付出少量对象的复制成本就可以完成收集。 老年代 老年代对象存活率高，没有额外空间进行内存担保，所以没有办法使用 复制 算法，就必须采用 Mark-Compact 或 Mark-Sweep 算法。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:2:4","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"总结 JVM 通过 可达性分析算法 标记需要被收集的对象，然后通过 Copying、 Mark-Sweep 和 Mark-Compact 算法的配合回收内存。 Copying 算法被改良后划分出一个 Eden 和两个 Survivor 区域，比例为 8:1:1，用于 新生代；Mark-Sweep 和 Mark-Compact 用于 老年代。 ","date":"2019-02-10","objectID":"/java-memory-collection-algorithms/:3:0","tags":["Java","JVM","GC","内存回收"],"title":"Java内存回收算法介绍","uri":"/java-memory-collection-algorithms/"},{"categories":null,"content":"个人十分喜欢 Hexo 博客框架下的 NexT 主题，所以自己的博客也就选择的这个主题。然而最近换了电脑之后傻眼了，git clone 之后发现 themes/next 是个空目录，github 上面也是同样的空目录，并且没有任何办法可以找回当时目录里的东西。回顾自己当时刚开始使用时的场景，一切要从 NexT 的官网说起。 ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:0:0","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"NexT 官方推荐的安装方式 Hexo 博客框架允许我们指定任何主题，被选定的主题需要在 ${root}/themes 目录下具有一个与指定名称相同的目录。比如，我们指定使用的主题名字叫做 next ，那么我们就需要在 ${root}/themes 目录下面存在一个叫做 next 的目录（本文不涉及此目录内容）。 现在我们有了一个想要使用的目录，假设就是 NexT 吧，自然会想到去官方网站看看有没有什么推荐的使用方式。于是 NexT 官网让我们这样安装： $ cd your-hexo-site $ git clone https://github.com/iissnan/hexo-theme-next themes/next 不仅是 NexT，还有很多其他的主题都是推荐的这样的安装方式。而这样的安装方式却会带来 git 仓库嵌套的问题。我们会发现我们的 Hexo 仓库不能识别到 ${root}/themes/next 仓库里面修改的内容，只能识别到它被修改了。这也就意味着，我们的 Hexo 仓库不知道 ${root}/themes/next 仓库里的内容。所以我们在克隆这个仓库之后，不知道这个主题下的文件从哪里来，有什么内容。 关于 git 嵌套仓库的问题，可以参考这里的讨论： https://github.com/swcarpentry/git-novice/issues/272 ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:1:0","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"正确的使用方式 现在，我们已经知道使用嵌套的 git 仓库是一个 anti-pattern，并且会导致我们丢失文件。那么正确的做法应该是什么呢？ ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:2:0","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"git submodule 天降神兵 首先解决“从哪里来”的问题。 因为我们需要使用到别人开发的主题，而且要做一些自定义修改，所以我们必须要拿到源码。这个时候我们就可以考虑使用 git submodule 了，这里正好满足使用 submodule 的情况。 git submodule 文档看这里： https://git-scm.com/book/en/v2/Git-Tools-Submodules 要使用 git submodule 我们就不能按照 NexT 这些主题官网提供的安装方式，而需要使用 git submodule 的方式： $ git submodule add git@github.com:iissnan/hexo-theme-next themes/next Cloning into '/I/wont/show/this/blog/themes/next'... remote: Enumerating objects: 12036, done. remote: Total 12036 (delta 0), reused 0 (delta 0), pack-reused 12036 Receiving objects: 100% (12036/12036), 12.95 MiB | 1.32 MiB/s, done. Resolving deltas: 100% (6967/6967), done. 现在，我们的 submodule 已经克隆好了，并且在我们的 Hexo 目录下面会多一个 .gitmodules 文件，我们可以看看里面的内容： $ cat .gitmodules [submodule \"themes/next\"] path = themes/next url = git@github.com:iissnan/hexo-theme-next 这个文件描述了当前仓库的 submodule 的信息，并且它会被加入到仓库中。因为我们在使用 git clone 的时候， git 不会帮我们把 submodule 一起克隆。 所以当其他人或我们在其他地方克隆这个仓库之后，就能够通过 .gitmodules 这个文件知道我们所需要的 submodule 能够从哪里获取到。 我们可以通过 git 的命令来获取所有的 submodule： $ git submodule update --init Submodule 'themes/next' (git@github.com:iissnan/hexo-theme-next) registered for path 'themes/next' Cloning into '/I/wont/show/this/blog/themes/next'... Submodule path 'themes/next': checked out '64302633a60c8f26ebdbad6f3c220e6d8a69723c' 这样，我们的 NexT 主题又能够回到我们本地的文件中。 ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:2:1","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"要好好保存自己的修改 然后解决“有什么内容”的问题。 submodule 解决了“从哪里来”，但是我们对主题做的自定义修改又保存在哪里呢？ 如果我们真的按照上面的步骤来操作的话，我们会发现在新设备上的主题配置和我们自定义的配置是不一样的。这样以来，我们新的设备上如果提交，我们自定义的主题配置信息就会丢失了。除非我们重新配置一下主题文件。 但我们是程序员，会永远遵循 DRY 原则，一定是什么地方出了差错。 前面的操作，我们把 NexT 官方仓库作为我们的 submodule，但因为我们不能把自定义的配置 push 到官方仓库中，所以我们对主题的自定义配置就只能留在本地。 为了保存我们的修改，我们可以将官方仓库 fork 到我们自己的账号下面，获得完全的控制权，然后将这个 fork 的仓库作为博客的 submodule。当我们对主题进行了配置的时候，我们需要将这些更新 push 到 fork 的仓库中，这样在下一次拉取 submodule 的时候就能够拿到我们自己的修改，而不会遇到丢失自定义配置的尴尬了。 ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:2:2","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"总结 所以，在 Hexo 中对自定义主题进行版本管理的正确操作就是： 找到主题的 Repo fork 到自己的账号下 将 fork 的仓库作为 Hexo 仓库的 submodule ","date":"2018-12-20","objectID":"/use-git-submodule-to-manage-your-hexo-theme/:3:0","tags":["hexo","git","submodule"],"title":"使用 git submodule 管理 Hexo 博客的主题","uri":"/use-git-submodule-to-manage-your-hexo-theme/"},{"categories":null,"content":"在学习 Spring Cloud 的时候，文档一开始就提到了一个概念：Twelve-Factor App。这勾起了我的好奇心，刚好有个网站用来解释这个东西，这里谈谈我的理解。 https://12factor.net ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:0:0","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"什么是 Twelve-Factor App 首先，twelve-factor app 是由 Heroku 提出的一种用于构建 software-as-a-service (SaaS) 应用的方法论。但不是所有的 SaaS 应用都是 twelve-factor app，只有符合一些特征，才是一个 twelve-factor app。 Use declarative formats for setup automation, to minimize time and cost for new developers joining the project; 通过声明的方式来自动化配置，以达到最小化项目新成员的时间成本和学习成本。这一点很好理解。 Have a clean contract with the underlying operating system, offering maximum portability between execution environments; 和底层系统之间有明确的契约，这样就能提供最大的可移植性。 类似于我们现在推崇的使用 docker 来输出我们的软件，这样我们只需要 docker 的环境就能运行我们的软件；而不是提供一个 jar 包或者前端打包的压缩文件，需要我们自行安装 java 或者 nginx 才能运行。 Are suitable for deployment on modern cloud platforms, obviating the need for servers and systems administration; 适用于部署到现代云计算平台，从而避免对服务器或系统管理员的需求。 其实，这样的要求就是在减小移植的成本。 Minimize divergence between development and production, enabling continuous deployment for maximum agility; 最小化开发环境和生产环境的差异，并启用持续部署来最大化敏捷实践。 减少开发环境和生产环境的差异，有利于我们提早发现问题和排查问题。持续部署作为一种敏捷实践，应该大力推行。 And can scale up without significant changes to tooling, architecture, or development practices. 可以在不对使用的工具、基础设施和开发实践进行重大改动的情况下进行“放大”。 这里的“放大”，可以指部署上的水平扩容或者功能上的改进等等。 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:1:0","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"为什么需要 Twelve-Factor App 在面对现代软件开发过程中，我们会遇到很多系统性的问题。Heroku 提出 Twelve-Factor App 的初衷，是为了增强我们对这些问题的认知，并提供专用的名次来命名这些问题，同时提出针对这些问题的广义上的解决方案。 这里的 广义上的 ，原文用词 conceptual ，可以理解为是抽象的解决方案，而不是具体的解决方案。 这些问题和解决方案，也不是随便找了一些人来闭门造车想出来的，而是邀请了直接或间接参与了大量应用的开发、部署、运维以及扩展的拥有丰富经验的 IT 从业人员参与讨论，得出了这些系统性的问题和对应的解决方案。 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:2:0","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"Twelve-Factor 是哪十二个 Twelve-Factor App 提出了十二个方面的解决方案，这也是它的名字的由来。而每一点，官网都提供了一句话的总结和详细的描述。这里暂时简单记录一下。 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:0","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"1.Codebase 一句话总结：一份基准代码，多次部署。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:1","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"2.Dependencies 一句话总结：显式的声明依赖。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:2","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"3.Config 一句话总结：在部署的环境中存储配置。 这里说的配置，是指在各个部署环境中不同的值。如果某个配置在不同的环境中是相同的，那么它就不应该是一个配置。所以，这里的配置一定会针对到具体的环境中，我们应该将它们与环境绑定起来。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:3","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"4.Backing services 一句话总结：将后端服务当作附加资源。 后端服务，不是平时所理解的 Web 应用的服务器那个后端，而是应用所调用到的其他服务，比如数据库、对象存储等等。当作附加资源，要求我们可以在不修改代码的情况下，可以切换所依赖的这些资源。比如，通过修改数据库的 URL、用户名等，切换到其他的数据库服务。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:4","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"5.Build, release run 一句话总结：严格分离构建、发布和运行的阶段。 由于每一个阶段对系统健壮性、信息完整度的要求并不一致，对不同的阶段采取不同的措施是很有必要的。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:5","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"6.Processes 一句话总结：以一个或多个无状态进程运行应用。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:6","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"7.Port binding 一句话总结：通过绑定端口来提供服务。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:7","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"8.Concurrency 一句话总结：通过进程模型来进行扩展。 在 twelve-factor app 中，进程是一等公民。Twelve-factor app 借鉴 unix 守护进程模型 来设计应用架构，将不同的工作分配给不同的进程类型来处理。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:8","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"9.Disposability 一句话总结：快速和优雅的启动、终止进程，可最大化健壮性。 易处理的进程，意味着进程可以瞬间完成启动和终止。这样有利于快速的对应用进行伸缩、部署新的代码或配置。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:9","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"10.Dev/prod parity 一句话总结：尽可能的保持 Dev、staging 和 production 环境一致。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:10","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"11.Logs 一句话总结：像对待事件流一样对待日志。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:11","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"12.Admin processes 一句话总结：后台管理任务当作一次性进程运行。 英文 中文 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:3:12","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"总结 可以将 Twelve-Factor App 理解为一种构建 cloud native 应用的指导原则（方法论），按照这样的原则设计并开发出来的应用，将会十分适用于部署到类似于 Heroku 这样的服务中。 ","date":"2018-09-27","objectID":"/what-is-12-factors-app/:4:0","tags":["Cloud Native","12 factor","Twelve-Factor"],"title":"什么是 Twelve-Factor App","uri":"/what-is-12-factors-app/"},{"categories":null,"content":"Spring Boot 为我们带来了自动配置的舒适体验，极大的提高了开发效率。当我们需要使用第三方库，而这些库没有实现 Spring Boot Auto-Configururation 时，我们为了避免在不同项目中的重复配置，可能需要为这些库提供 Auto-Configuration。 Spring Boot 为我们提供了自定义 auto-configuration 的方法，可以让我们方便的定义自己的 bean 如何注入容器，这样，我们可以将和 Spring 集成的代码集中起来，减少不必要的重复配置。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:0:0","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"Spring 如何实现自动配置 简书上的这篇文章 对 Spring Boot 如何实现自动配置做了比较详细的介绍。 简单来说，Spring Boot 提供了 spring-boot-autoconfigure 来实现自动配置，利用 @Conditional 注解判断需要配置的内容。Spring core 中的 SpringFactoriesLoader 类则会扫描 classpath 下所有的 JAR 包中 META-INF/spring.factories 里的内容。这个文件里包含了很多 Spring 里需要的内容，比如 ApplicationContextInitializer 、ApplicationListener 等类的子类信息，Spring 将会根据各自在生命周期中需要加载的时间进行加载。 spring boot autoconfigure 中的 spring.factories 部分内容 # Initializers org.springframework.context.ApplicationContextInitializer=\\ org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\\ org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener # Application Listeners org.springframework.context.ApplicationListener=\\ org.springframework.boot.autoconfigure.BackgroundPreinitializer # Auto Configuration Import Listeners org.springframework.boot.autoconfigure.AutoConfigurationImportListener=\\ org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener # Auto Configuration Import Filters org.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\\ org.springframework.boot.autoconfigure.condition.OnClassCondition # Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\ org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration # Failure analyzers org.springframework.boot.diagnostics.FailureAnalyzer=\\ org.springframework.boot.autoconfigure.diagnostics.analyzer.NoSuchBeanDefinitionFailureAnalyzer,\\ org.springframework.boot.autoconfigure.jdbc.DataSourceBeanCreationFailureAnalyzer,\\ org.springframework.boot.autoconfigure.jdbc.HikariDriverConfigurationFailureAnalyzer,\\ org.springframework.boot.autoconfigure.session.NonUniqueSessionRepositoryFailureAnalyzer # Template availability providers org.springframework.boot.autoconfigure.template.TemplateAvailabilityProvider=\\ org.springframework.boot.autoconfigure.freemarker.FreeMarkerTemplateAvailabilityProvider,\\\\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafTemplateAvailabilityProvider,\\ org.springframework.boot.autoconfigure.web.servlet.JspTemplateAvailabilityProvider 在 Spring Boot autoconfigure 中，提供了 AutoConfigurationImportSelector 类用来读取 spring.factories 中的 org.springframework.boot.autoconfigure.EnableAutoConfiguration 的值，然后 Spring 会加载这里读到的所有的 Configuration。这些被读取到的 Configuration 一定要是 Auto-configuration class，在 Spring 中，被 @Configuration 注解的类就符合这个条件。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:1:0","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"实现自己的 auto configuration 根据前面的理解，我们很容易想到如何来实现自己的 auto configuration。 编写自定义的配置类，并加上 @Configuration 注解 在 resources 目录下创建 META-INF/spring.factories 在 Spring 官网，也详细的讲述了如何实现自己的自动配置：https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-auto-configuration.html 我们来看一个实际的例子。github 上面有一个项目叫做 MariaDB4j。它可以为我们的 Java 项目提供一个 embedded mariaDB。然而在与 Spring Boot 集成的时候发现，如果要将 mariaDB4j 作为一个 bean 注入到我们的 Spring Context 里，我们需要确保在 mariaDB 启动之后，Spring 才会去加载 dataSource。我们不得不重写一个 dataSource 来注入 Spring。 @Bean @DependsOn(\"mariaDB4j\") #(1) public DataSource dataSource(DataSourceProperties dataSourceProperties) { return DataSourceBuilder.create() .driverClassName(dataSourceProperties.getDriverClassName()) .url(dataSourceProperties.getUrl()) .username(dataSourceProperties.getUsername()) .password(dataSourceProperties.getPassword()) .build(); } dataSource 需要等待 mariaDB4j 加载完成才能被加载 显然，我们不想在每次使用的时候都要写一份这样的代码，如果能够单纯的把 mariaDB4j 引入到我们的 classpath 中就能使用，那才是我们期望的用法。所以我选择了为它添加 auto-configuration 的方式。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:2:0","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"创建 MariaDB4jSpringConfiguration 将 MariaDB4j 源码拉下来之后，新建了一个名为 mariaDB4j-springboot 的 sub module。然后加上我们需要的依赖。 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003e${project.groupId}\u003c/groupId\u003e \u003cartifactId\u003emariaDB4j\u003c/artifactId\u003e \u003cversion\u003e${project.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-autoconfigure-processor\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 然后，添加我们的 Auto-configuration class @Configuration public class MariaDB4jSpringConfiguration { @Bean public MariaDB4jSpringService mariaDB4j() { return new MariaDB4jSpringService(); } } 接下来，我们需要测试这个配置类能够被成功的加载。这里的测试我还没有理解到，仅仅是按照官网的例子来写了一个测试： public class MariaDB4JSpringConfigurationTest { private final ApplicationContextRunner contextRunner = new ApplicationContextRunner() .withConfiguration(AutoConfigurations.of(MariaDB4jSpringConfiguration.class)); @Test public void shouldAutoConfigureEmbeddedMariaDB() { this.contextRunner.withUserConfiguration(MariaDB4jSpringConfiguration.class) .run(context -\u003e { assertThat(context).hasSingleBean(MariaDB4jSpringService.class); assertThat(context.getBean(MariaDB4jSpringService.class)) .isSameAs(context.getBean(MariaDB4jSpringConfiguration.class).mariaDB4j()); }); } } 这里的不理解在于，创建的这个 ApplicationContextRunner 类不能自动的去扫描 bean 而要让我们手动的去加载它。这样一来，就不能通过这个测试来验证这个类会被自动配置。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:2:1","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"创建 dataSource bean 在 mariaDB4j 启动完成后，我们就能创建 dataSource 了。因为 Spring 在 dataSource 创建后会尝试连接，所以一定要保证 mariaDB4j 先创建。 @Bean @DependsOn(\"mariaDB4j\") #(1) public DataSource dataSource(DataSourceProperties dataSourceProperties) { return DataSourceBuilder.create() .driverClassName(dataSourceProperties.getDriverClassName()) .url(dataSourceProperties.getUrl()) .username(dataSourceProperties.getUsername()) .password(dataSourceProperties.getPassword()) .build(); } 通过 bean 的名称来判断需要依赖哪个 bean ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:2:2","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"添加 spring.factories 有了这些，Spring 还不能自动配置我们的配置类，我们还需要在 src/main/resources/META-INF 下添加 spring.factories 文件，让 Spring 扫描这个文件后知道哪些类可以自动配置。 org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ ch.vorburger.mariadb4j.springboot.autoconfigure.MariaDB4jSpringConfiguration,\\ ch.vorburger.mariadb4j.springboot.autoconfigure.DataSourceAutoConfiguration 这里添加的类名一定是包含 package 的全名，否则 Spring 找不到指定的类。 现在，我们已经完成了自动配置的工作，打包好后就能使用了： 打包 mvn clean install 然后我们能够在本地 maven 仓库看到 ch/vorburger/mariaDB4j 目录下多了一个 mariaDB4j-springboot， 接下来就能使用了。 dependencies { testCompile(\"ch.vorburger.mariaDB4j:mariaDB4j-springboot:2.3.1-SNAPSHOT\") } 这次的实践提了一个 PR 到 https://github.com/vorburger/MariaDB4j/pull/153/(mariaDB4j)，也算是通过实践来学习了。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:2:3","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"总结 要实现自己的 auto-configuration 还算简单，只需要两步： 添加自己的 Configuration 类 将自己添加的配置类写到 META-INF/spring.factories 里 但是，这背后的实现却有些复杂，值得深入研究。这里会涉及到 Spring 的大量知识，包括各种包的依赖、spring.factories 的读取、@Conditional 注解等等，每一个都可以研究一下写一篇博客出来🐶🐶。 ","date":"2018-08-21","objectID":"/create-your-own-spring-auto-configuration/:3:0","tags":["spring","spring boot","auto configure"],"title":"创建自己的 Spring auto-configuration","uri":"/create-your-own-spring-auto-configuration/"},{"categories":null,"content":"还差几天就工作满一年了，记录一下一些不足以写成博客的经验吧。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:0:0","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"关于数据库访问层 做后端开发必定是要访问数据库的，经历的几个项目分别使用了 JDBCTemplate 和 QueryDSL ，一年的使用下来，让我对 JPA 越来越厌恶。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:1:0","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"QueryDSL QueryDSL 是基于 JPA 的，它为我们提供了一种类型安全的类似于 SQL 的 dsl。从语法上来看，确实能带来一定的价值，但相对于负面作用，我认为是得不偿失。 需要预编译 要使用 QueryDSL，需要先利用它的工具对 @Entity 注解的类进行编译，生成一个 Q 开头的类，用来实现便利的查询工作。虽然不是一个麻烦事，但确实在工作中造成了一些问题。比如在进行单元测试调试时，如果有对 @Entity 的修改，还得手动的执行一下编译任务。 生成的 SQL 有可能不符合预期 这是很有可能发生的事情，特别是对于新上手 QueryDSL 的人来说，更是摸不清头脑。这样就需要在每次修改了 DAO 类之后运行一下测试或者启动一下项目才能知道生成的 SQL 到底长什么样子。不过一般来讲，当你熟悉他的语法之后，这样的事情并不会造成太大的困扰。但即使是这样，我们仍然要在每次修改了 DAO 类之后查看一下生成的 SQL 是否符合我们的预期。因为我们就遇到过一些神奇的事情，得到的 SQL 并不符合预期，想来想去只能是 QueryDSL 的 bug。 自动生成 SQL 还会带来一个问题就是，开发者失去了对 SQL 的控制。我曾经也是“在代码里不要出现一句 SQL”的坚定支持者，但是当我经历了不得不对 SQL 进行定制时，才发现这种想法多么可笑。当然，QueryDSL 有办法通过 Expressions.stringTemplate() 方法来定制你想定制的那部分 SQL，但这和其他的 QueryDSL 组合起来时，总显得那么格(bu)格(gou)不(you)入(ya)。 JPA 事务对测试的影响 我们一般会在 Controller 的下一层，也就是 Service 这一层加上 @Transactional 注解，这样当我们的某个环节出错的时候，就能整个事务回滚，保证操作的原子性。对于这中间的这些 Service ，我们一般会采用没有 Spring 上下文的单元测试，这样可以让这部分的测试负担没有那么重。但是因为 JPA 的加入，情况变得复杂，就算我们的代码通过了这样的单元测试，也不能保证能够正确的执行，测试也就变得没有意义。一种解决办法是不再采用这种单元测试，而使用有 Spring 上下文的测试，这样能够将 JPA 的行为纳入测试中。 但我个人来讲，可能是由于对 JPA 不足够熟悉，不太愿意继续使用 JPA，而更愿意使用 JDBCTemplate。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:1:1","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"JDBCTemplate 其实一开始使用 JDBCTemplate 的时候我是抗拒的，那时我还是 JPA 的支持者，总觉得在代码里面插入 SQL 很不优雅。但是在业务复杂的项目中使用过 JPA 后发现，JDBCTemplate 真香。相比与 JPA，JDBCTemplate 有这些优势： 更轻量 这一点应该毋庸置疑，毕竟少了一大堆复杂的东西。 开发者能够完全控制 SQL 特别是当你需要特别的 SQL 的时候，会爱上这一点的。至于代码和数据库解耦这种事情，我的理解是我的数据库可以随意更换，但都是一种数据库，很难有从 MySQL 切换到 PostgreSQL 这种事发生。所以，代码里写 SQL 带来的耦合不一定会让你痛苦。 无需担心测试通过但不工作的情况 没有了 JPA ，我们的 Service 由我们全权控制，也就不会出现因为 JPA 事务导致通过测试的代码不能正确执行的情况。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:1:2","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"测试用的数据库 在我们的代码部署到 dev 环境之前，一定是会经历一个 CI 的过程的。在 CI 中，我们会运行我们写的所有测试，其中就包括针对 DAO 的测试，这些测试一定是要访问数据库的。那么在 pipeline 中执行测试时访问什么数据库就是一个问题。 过去，我们会考虑采用保存在内存中的 H2 数据库，它随我们的测试启动而启动，随测试结束而结束。看上去这简直是个完美的方案。然而，我们实践发现了一些问题，H2 不支持 MySQL 的全部语法，即使设置了 model=mysql。这使得我们不得不采用它支持的语法来绕过这些问题，甚至于需要迁移 JSON 格式的数据时会发现 migration 没法进行下去了，因为 H2 压根没有 JSON 的任何支持。最后，我们幸运的利用 flyway 支持的 Java Migration 绕过这个限制。 所以我们又重新思考有什么别的方法可以解决当前的问题。 其中一个方案是，在执行测试时，利用 Gradle 的插件启动一个 Docker 容器，测试完成后删掉这个容器。但是这样就要求我们的 pipeline 能够支持 Docker。并且，我们在调试 DAO 测试时也没有办法使用到这个容器中的数据库。不过这两个问题也有办法解决，这个方案也是值得尝试的。 另一个方向是同事前几天发现的 MariaDB4j ，单从这篇文章来看，对于使用 MySQL 的项目，确实是替代 H2 的不错方案。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:2:0","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"HATEOAS HATEOAS 是 Hypermedia As The Engine Of Application State 的缩写，处于 Richardson 博士提出的 REST Maturity Model 中的 Level 3，可以用来引导客户端探索服务端功能。 说实话，我对这个东西的理解不是很深刻，目前实践来看，更适合 Mobile 这样的单一入口的项目，而不适用于网页应用。这是因为整个应用需要一个入口，从这个入口进入才能获得更多的服务。或者，是我们的实践不好，应该支持任意入口进入都能获得完整的服务地图。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:3:0","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"DDD 领域驱动设计绝对是能够帮助你降低项目无谓复杂度的方法论，当然，前提是你的项目足够复杂。至于如何判断呢？我的理解是当你修改一个业务逻辑却需要到多个地方修改同样的逻辑、或者干脆不知道在哪里能够找到你想找的代码时，就可以考虑 DDD 了。 之所以着么推荐 DDD ，是因为在体验了没有采用 DDD 的项目需求变得日益复杂之后，才发现 DDD 的优势。他能够帮助开发者对代码有更加清晰的了解，也能够将业务表达在代码中。这样既可以将相同的业务代码集中，也能够帮助开发者更容易的找到代码。 以上就是一年工作的一点经验总结，看起来没什么进步，还是要多写博客才行啊。 ","date":"2018-08-02","objectID":"/biref-summary-of-one-year-of-work/:4:0","tags":null,"title":"码农一年小结","uri":"/biref-summary-of-one-year-of-work/"},{"categories":null,"content":"现在的项目上在尝试契约测试这个东西，使用了大半年，再加上业余的一些研究，简单的总结一下。 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:0:0","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"契约测试解决的问题 契约测试是在微服务实践中提出的，目的是为了避免在服务进行集成测试的时候，因为 consumer 对接口的期望和 provider 提供的接口的不一致导致的返工。因为这一反馈周期较长，返工的成本也就显得很高昂。 契约测试就是为了解决这样的问题。它可以约定接口接收到什么样的请求的时候，返回什么样的响应。这样，在集成测试之前，就能够知道这些服务在 API 层面上能不能正常工作。 这里强调在集成测试之前是想说明，契约测试不是用来替代集成测试的，而是测试方法的一种补充。 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:1:0","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"契约测试的工具 目前来看，契约测试有两大主流工具， Pact 和 Spring Cloud Contract。 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:2:0","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"Spring Cloud Contract 作为抱紧 Spring 大腿的契约测试工具，Spring Cloud Contract 成为了以 Spring Cloud 作为框架的微服务项目的首选，也是众多有微服务和契约测试经验的工程师的推荐。它主要有以下特点： 使用 Groovy 语言定义 DSL，利用了 Groovy 灵活的语法，可以帮助开发者写出满足需求的契约。（后来又支持了 yaml 的语法） Spring 提供了一系列的工具进行契约测试： 可以在 consumer 和 provider 的单元测试中进行契约测试 提供 Docker、jar 以及 Maven 包，用于启动 stub runner 独立的契约仓库管理契约 为契约生成独立的 jar 包，提供给 consumer 或 provider Spring Cloud Contract 的一个明显的缺点就是，只能支持能够使用 Spring 框架的语言，这也是抱紧 Spring 大腿的后果。为了解决这个问题，Spring Cloud Contract 又开始支持 yaml 格式的 DSL 。但是这种方式却无法为不能使用 Spring 的语言提供单元测试的方案，只能自己解决这个问题。 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:2:1","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"Pact Pact 是一个专注于契约测试的工具，与 Spring Cloud Contract 不同的是，Pact 一定是 Consumer Driven Contracts。所以我们会发现，Pact 会在 consumer 端的契约测试通过后，自动生成 json 格式的契约文件。然后，provider 可以通过文件系统、http 或者 Broker 的方式获取契约文件，进行契约测试。 另外，Pact 支持多种语言，没有 Spring Cloud Contract 的语言限制。这样不仅适用于使用不同语言的微服务之间，甚至可以用于Web项目中的接口约定。 Pact 还有一个杀手级功能： Broker 。它的基本功能是存储 consumer 生成的契约文件，provider 可以利用它来获取这些契约文件用于测试。但它还提供功能用于追踪哪些版本的契约通过了 provider 端的契约测试，这样在部署的时候就能清楚的哪些版本能够被部署。 不过对于 Java 语言的 provider （其他语言的支持我没有研究），Pact 没法提供像 Spring Cloud Contract 一样的单元测试支持，只能将整个 provider 启动起来进行 API测试 。 简单的总结一下这两个工具： Consumer 端： 都能提供 CDC 的支持，不过 Pact 是必须消费者驱动 Spring Cloud Contract 对消费者的技术选择有要求，否则只能使用 stub runner 或者自己通过 yaml 的方式来解决 Pact 不用写契约文件，是由 Pact 生成 Provider 端： 都能提供测试支持 Spring Cloud Contract 支持单元测试，但是同样也只支持 Spring 技术栈 Pact 只能进行 API测试 ，但不限定技术栈 契约文件管理： Spring Cloud Contract 需要手动写契约，自行管理 Pact 不用关心契约文件，由消费者的单元测试生成，生产者也不用关心契约文件的管理 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:2:2","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"契约测试的一点经验 契约测试，测的不是数据内容，而是接口返回的数据格式。 也就是说，当接口的输入为 1 或者 2 都能得到期望的结果而没异常产生时，就没有必要写两份契约。 注意管理契约测试的数据。 主要是针对像 Spring Cloud Contract 这样的 by sample 类型的契约测试工具，因为是按例子来写契约，就需要注意接口之间的业务关系，需要将契约中的数据串联起来。 契约测试会加大工作量 这是毋庸置疑的一点，因为我们把集成测试阶段的一部分事情放到了开发阶段，本身就加大了开发阶段的工作量。再加上契约测试带来的管理问题，总体而言，工作量是增加的。但不要因为这一点来质疑“将在集成测试阶段发现问题带来的长反馈周期缩短”这一初衷，只是，契约测试可能并不是一条最好的路。 契约测试能给开发者信心 当我们的测试能够通过契约测试的时候，说明在集成的时候不太会有什么问题。对于开发人员来讲，就不太需要担心在集成时发现接口数据格式对接不上这种问题了。之所以说不太需要担心，是因为有的契约测试新手可能会绕过这个测试（也可能是`CI`没有做好的原因），假装做好了，等到集成时才发现gg。 ","date":"2018-07-20","objectID":"/contracts-test-brief-summary/:3:0","tags":["Contracts-Test","Test"],"title":"契约测试小结","uri":"/contracts-test-brief-summary/"},{"categories":null,"content":"在做企业微信应用开发的时候，有使用第三方单点登录的需求。根据企业微信的文档，设置好登录授权发起域名和回调域名后，编写了一个Controller，用来重定向到微信的接口。但重定向后总是得到校验请求来源错误的提示信息。 之后，修改Controller，使用response返回一个\u003ca\u003e标签，链接地址指向微信接口。这次，点击链接，可以成功的看到二维码，并且能够成功扫码登录。 这样的情况说明使用重定向和使用超链接是有区别的。至少，浏览器的处理会不一样。 对浏览器而言，当接收到重定向的响应时，无论是具体的哪一个状态码，都会直接使用GET方法去请求Location字段中指定的URL。 而\u003ca\u003e标签则不同。当用户点击网页中的超链接时，浏览器还会在请求的headers中加上referer字段，内容为当前页面的URL。这样，为链接地址作出响应的服务器可以根据该字段判断这次访问是否来自期望的地址。经过实践推断，企业微信的服务器就是这样做的。 而Http中的这个字段又代表什么意义呢？Referer其实是一个拼写错误，本应该是Referrer，意味提供者。顾名思义，这个字段的作用是告诉服务器当前的请求是由谁提供给访问者的。对服务器而言，这个字段的内容既可以用来做安全验证，也可以做数据统计。 至于协议里具体的定义，可以查看下面两个链接： https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.36 https://en.wikipedia.org/wiki/HTTP_referer ","date":"2017-11-04","objectID":"/http-hearders-referer/:0:0","tags":null,"title":"Http Header 中的 referer 字段","uri":"/http-hearders-referer/"},{"categories":null,"content":"Java Validation简介 javax.validation包提供了大量用于验证数据的API工具，可以帮助开发者方便的检验程序的输入输出。最近刚刚接触到这方的知识，还只知道使用其提供的注解工具。 想要使用这个工具，需要导入这个包。以gradle为例： compile group: 'javax.validation', name: 'validation-api', version: '2.0.0.Final' 但这个包仅仅定义了规范的接口、注解等，并没有提供具体实现。常用的实现是由Hibernate提供的： compile group: 'org.hibernate.validator', name: 'hibernate-validator', version: '6.0.3.Final' compile group: 'org.hibernate', name: 'hibernate-validator-annotation-processor', version: '6.0.3.Final' ","date":"2017-10-25","objectID":"/custome_validation_in_java/:0:0","tags":null,"title":"定制Java中的Validation","uri":"/custome_validation_in_java/"},{"categories":null,"content":"注解使用示例 我们以@NotEmpty注解为例，这是由Hibernate提供的注解，而不是标准规范注解。 假设我们有一个Controller，需要接收一个POST请求，RequestBody为JSON，包含username和password两个字段。 首先定义我们的Body： @Data public class LoginCommand { @NotEmpty(message = \"username cannot be empty\") private String username; @NotEmpty(message = \"password cannot be empty\") private String password; } @Data注解由lombok提供，可以帮助我们创建出一个Java Bean。 @NotEmpty注解会在LoginCommand对象创建后验证被注解修饰的field的值，当username为null或\"\"时，会得到false，解析这个注解的Validator就会抛出异常，异常信息的message就是这里设置的值，最终Spring会把这样的异常以400响应返回给客户端。 接下来是我们的Controller： @RestController public class UserController { @PostMapping(\"login\") public String (@Valid @RequestBody LoginCommand command) { return \"23333\"; } } @Valid注解由javax.validation提供，一般使用在除基本类型和String之外的其他类型的对象上，以允许实现Validator的类能够对该对象进行递归的调用。 这样，当我们请求/login接口的时候，如果username或password为null或\"\"，则会得到400的HTTP响应。 如果希望系统学习，请移步：http://hibernate.org/validator/documentation/ 面临需求 现在我们面临一个需求，对于POST的数据，其中一个字段可以为\"\"或者是正常的字符串，而不能是null或\" \"。面对这样的情况，我们没法在Hibernate提供的库里找到需要的注解。这个时候，为了满足需求，我们需要定制我们的Validator以满足需求。 通过注解实现定制 解决方法来自：https://stackoverflow.com/a/43716689/6487869 最简单的方式，当然是不要写任何实现代码啦。仔细研究javax.validation和Hibernate提供的注解，我们找到了@Null、@NotEmpty和@NotBlank三个注解。发现组合这几个注解可以帮助我们构造出我们需要的注解。 EmptyOrNotBlank = NotBlank | Empty Empty = !(Null | NotEmpty) 结合前面stack overflow的回答，我们能够很容易的利用已有的注解构造出@Empty和@NotBlank这两个注解，并且不需要通过实现接口来完成逻辑代码。 @Empty注解 @ConstraintComposition(CompositionType.ALL_FALSE) @Null @NotEmpty @ReportAsSingleViolation @Target({FIELD, ANNOTATION_TYPE}) @Retention(RetentionPolicy.RUNTIME) @Constraint(validatedBy = {}) public @interface Empty { String message() default \"\"; Class\u003c?\u003e[] groups() default {}; Class\u003c? extends Payload\u003e[] payload() default {}; } @EmptyOrNotBlank注解 @ConstraintComposition(CompositionType.OR) @Empty @NotBlank @ReportAsSingleViolation @Target({ANNOTATION_TYPE, FIELD, METHOD, CONSTRUCTOR, PARAMETER}) @Retention(RUNTIME) @Constraint(validatedBy = {}) public @interface EmptyOrNotBlank { String message() default \"\"; Class\u003c?\u003e[] groups() default {}; Class\u003c? extends Payload\u003e[] payload() default {}; } 自定义Validator实现定制 解决方法来自：https://docs.jboss.org/hibernate/validator/5.0/reference/en-US/html/validator-customconstraints.html#validator-customconstraints-validator 另一种实现方式当然就是通过实现接口来做到啦，现在我们不用再去组合已有的注解，而是需要为@Constraint注解的validateBy指定用于验证的类。 于是我们的@EmptyOrNotBlank可以写成： @ReportAsSingleViolation @Target({ANNOTATION_TYPE, FIELD, METHOD, CONSTRUCTOR, PARAMETER}) @Retention(RUNTIME) @Constraint(validatedBy = EmptyOrNotBlankValidator.class) public @interface EmptyOrNotBlank { String message() default \"\"; Class\u003c?\u003e[] groups() default {}; Class\u003c? extends Payload\u003e[] payload() default {}; } 然后，我们需要编写EmptyOrNotBlankValidator这个类。 public class EmptyOrNotBlankValidator implements ConstraintValidator\u003cEmptyOrNotBlank, String\u003e { @Override public void initalize(EmptyOrNotBlank constraintAnnotation) {} @Override public boolean isValid(String object, ConstraintValidatorContext constraintContext) { if (object == null) { return false; } if (object.isEmpty()) { return true; } return !object.trim().isEmpty(); } } 完成之后，就能达到和使用组合注解的方法一样的效果了。对于一些复杂的、组合注解难以实现的验证，更加推荐实现ConstraintValidator的方式。 ","date":"2017-10-25","objectID":"/custome_validation_in_java/:1:0","tags":null,"title":"定制Java中的Validation","uri":"/custome_validation_in_java/"},{"categories":null,"content":"记录如何在 SpringMVC 中测试接口","date":"2017-06-04","objectID":"/springmvc%E4%B8%ADcontroller%E7%9A%84%E6%B5%8B%E8%AF%95/","tags":null,"title":"SpringMVC中Controller的测试","uri":"/springmvc%E4%B8%ADcontroller%E7%9A%84%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":" 参考 http://www.jianshu.com/p/ad7995332dd9 controller: @Controller @RequestMapping(\"/system\") public class SysMapController { @Autowired private FirstSysMapService firstSysMapService;//注入Service @RequestMapping(value = \"/first/map\", method = RequestMethod.GET, produces = {\"application/json;charset=UTF-8\"}) @ResponseBody//响应类型为`json` public List\u003cFirstSysMap\u003e getMap() { List\u003cFirstSysMap\u003e map = firstSysMapService.findMap(); return map; } } test: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({\"classpath:spring/spring-dao.xml\", \"classpath:spring/spring-service.xml\", \"classpath:spring/spring-web.xml\"}) @WebAppConfiguration//没搞懂，应该是注入`web`上下文的意思 public class SysMapControllerTest { @Autowired private SysMapController mapController; @Autowired private ServletContext context; private MockMvc mockMvc;//`springMVC`提供的`Controller`测试类，不需要再去单独依赖`mockito` @Before public void setup() { //应该是把`Controller`加入测试环境的意思 mockMvc = MockMvcBuilders.standaloneSetup(mapController).build(); } @Test public void getMap() throws Exception { //发起请求，`accpet`设定接收的数据类型 ResultActions actions = mockMvc.perform(MockMvcRequestBuilders.get(\"/system/first/map\") .accept(MediaType.APPLICATION_JSON_UTF8) ); //获取`response` MvcResult result = actions.andReturn(); String res = result.getResponse().getContentAsString(); System.out.println(\"-----------response:\" + res); } } ","date":"2017-06-04","objectID":"/springmvc%E4%B8%ADcontroller%E7%9A%84%E6%B5%8B%E8%AF%95/:0:0","tags":null,"title":"SpringMVC中Controller的测试","uri":"/springmvc%E4%B8%ADcontroller%E7%9A%84%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"函数对象 JavaScript中函数就是对象. 函数对象连接到Function.prototype. 当把一个函数当作构造函数(使用new关键字)使用时, 新创建的对象的原型就是该函数的prototype对象. 我们可以通过给prototype设置属性而达到让该类对象拥有同样的公共属性的目的. 新创建的对象有一个__proto__属性, 指向该函数的prototype对象. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:1:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"函数字面量 函数对象通过函数字面量来创建. 函数字面量可以出现在任何允许表达式出现的地方, 甚至可以被定义在函数内部. 内部函数处理可以访问自己的参数和变量, 他也能自由访问把它嵌套在其中的父函数的参数与变量. 通过函数字面量创建的函数对象包含一个连接到外部上下文的连接, 叫做闭包. var add = function (a, b) { return a + b; } //or function add (a, b) { return a + b; } ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:2:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"调用 每个函数除了声明的变量外, 还会接收两个附加的参数this和arguments. 调用运算符是跟在任何产生一个函数值的表达式之后的一对圆括号. 这就解释了立即执行函数的写法. 圆括号内包含参数. 参数个数不会导致运行时错误, 多余参数会被忽略, 参数缺失会被替换为undefined. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:3:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"方法调用模式 当一个函数被保存为一个对象的属性时, 称之为方法. 这种模式下this参数被绑定到该对象. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:3:1","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"函数调用模式 当一个函数不是一个对象的属性时, 他就是被当作函数来调用的. 这种模式下this被绑定到全局对象. 作者认为这是一个语言设计上的错误, 应该将其绑定到外部函数的this变量. 我们可以这样做: var foo = function () { var that = this; var helper = function () { //do something with var that } } ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:3:2","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"构造函数调用模式 使用new来调用一个函数时, 就会创建一个连接到该函数的prototype成员的新对象, 同时, this被绑定到这个新对象上. var Car = function () { this.wheels = 4; } var ford = new Car(); ford.__proto__ === Car.prototype //true ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:3:3","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"Apply调用模式 apply方法让我们构建一个参数数组传递给调用函数. 同时, 也可以指定this的值. 该方法有两个参数: 第一个: 绑定给this的值 第二个: 参数数组 ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:3:4","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"参数 前面提到过arguments参数, 函数可以通过访问此参数以访问所有参数, 包括多余参数. 然而, 因为语言的设计错误, 该参数只是一个\"类似数组\"的对象, 它拥有一个length属性, 但没有任何数组的方法. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:4:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"返回 使用return关键字返回, 如果没有指定, 则返回undefined. 构造函数调用时, 如果返回值不是一个对象, 则返回this. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:5:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"异常 抛出: var add = function (a, b) { if (typeof a !== 'number' || typeof b !== 'number') { throw { name: 'TypeError', message: 'add needs numbers' }; } return a + b; } 捕获: var try_it = function() { try { add(\"seven\"); } catch (e) { document.writeln(e.name + \"; \" + e.message); } } 一个try语句只会有一个捕获异常的代码块, 如果有多种异常的情况, 只有通过name属性判断. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:6:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"闭包 通过函数字面量创建的函数对象包含一个连接到外部上下文的连接, 叫做闭包. 因为JavaScript是一个函数式语言, 所以支持返回一个函数. 这样将会导致内部函数比它的外部函数拥有更长的生命周期. 这一特性也让创建私有变量成为可能. var myObj = (function () { var value = 1; return { setValue: function (inc) { value = typeof inc === 'number' ? inc : value; }, getValue: function () { return value; } }; }()); 再来看一个糟糕的例子及其改进: var add_the_handlers = function (nodes) { var i; for (i = 0; i \u003c nodes.length; i++) { nodes[i].onclick = function (e) { alert(i); }; } }; 这个函数的目的是点击一个节点时, 弹出对话框显示节点的序号. 而这个函数的效果却是每次显示节点的数目. 原因在于, 创建onclick函数时, 函数引用的变量i属于add_the_handlers方法, 而i一直在改变, 直到变为nodes.length. 所以, 当所有的onclick方法创建完成后, 引用的i实际上是一个变量, 值为nodes.length. var add_the_handlers = function (nodes) { var helper = function (i) { return function (e) { alert(i); }; }; var i; for (i = 0; i \u003c nodes.length; i++) { nodes[i].onclick = helper(i); } } 这个改进的方法就能达到目的, 原因在于, 返回给onclick的函数是helper的内部函数, 其引用的i是helper函数的i, 覆盖了外部的add_the_handlers的i. 所以, 当循环进行时, helper中的i因为是形参, 而不会收到add_the_handlers中的i的影响. ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:7:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"级联 其实这是一个技巧. 多数的setter方法往往不需要返回任何内容, 这时在JavaScript中, 函数将会返回undefined. 如果我们需要对一个对象设置很多值, 不得不写成: obj.setName(name); obj.setAge(age); obj.setSex(sex); ... 如果我们让这样的函数返回this, 就可以启动级联, 情况就大不一样. obj.setName(name).setAge(age).setSex(sex)... ","date":"2017-03-06","objectID":"/javascript_the_good_parts_function/:8:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--函数","uri":"/javascript_the_good_parts_function/"},{"categories":null,"content":"前两章介绍基础, 没什么笔记好记录. 这是第三章. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:0:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"什么是对象 在JavaScript中, 除了简单数据类型(数字, 字符串, 布尔值, null和undefined), 其他所有的值都是对象Object. 其中number string和boolean虽然拥有方法, 但他们并不是object, 因为他们是不可变的. JavaScript中的对象是可变的 键控集合. 在JavaScript中, 数组/函数/正则表达式/对象本身都是对象. 对象是属性的容器. 属性由K/V组成, 属性名可以是包括空字符串在内的任意字符串, 属性值可以是除undefined之外的任何值. 这意味着对象可以包含对象. 对象是无类型(class-free)的, 对新属性的键值没有限制. JavaScript中包含一种原型链的特征, 允许对象继承另一个对象的属性. 这一特性可以用来减少时间和内存消耗. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:1:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"对象字面量 ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:2:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"标识符 JavaScript中的标识符由字母开头, 可由字母, 数字和下划线组成, 但是不能使用保留字, 如: abstract boolean if等. 一个对象的字面量就是包围在一对花括号中的零个或多个K/V对. var empty = {}; var person = { \"first-name\": \"melo\", \"last-name\": \"Gao\" }; 如果属性名是合法的标识符, 且不是保留字, 则不强制要求使用引号. 如: var auther = { firstName: \"Douglas\", familyName: \"Crockford\" } ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:2:1","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"检索 有两种方式检索到对象的属性: person['first-name'] //1 melo auther.firstName //2 Douglas empty.first_name //3 undefined 作者推荐优先使用方式2和3, 理由是可读性更好. 但方式1可以让我们通过修改参数值而达到动态访问的目的, 如: key = 'first-name'; person[key] //melo key = 'firstName'; auther[key] //Douglas 另外, 对于第三种情况, 要注意判断返回的值, undefined会被判断为false. 如果不做判断, 会抛出TypeError异常. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:3:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"更新 可直接对对象中的属性赋值, 就像对Java中的public属性赋值一样. 当属性不存在时, 该属性会扩充到对象中. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:4:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"引用 JavaScript通过引用传递对象, 他们永远不会被复制. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:5:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"原型 每个对象都连接到一个原型对象, 并且可以从中继承属性. 所有通过对象字面量创建的对象都连接到Object.prototype, 它是JavaScript中的标配对象. (这有点像Java中所有类都是Object的子类.) 原型在更新属性时是不起作用的, 如果对象没有相应属性, 会扩充该属性. 只有在检索属性时, 原型才可能起作用. 如果在对象中没有找到目标属性, 则会在它的原型对象中查找. 如果原型对象中还是没有找到, 再到它的原型对象中查找, 依此类推, 直到找到该属性, 或者在Object.prototype中也找不到为止. 如果该属性不在此原型链中, 则得到undefined. 这个过程叫做委托. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:6:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"反射 使用关键字typeof可以查看任何值的类型. typeof person //'object' typeof 2333 //'number' typeof '2333' //'string' typeof true //'boolean' typeof Object //'function' typeof auther.firstName //'string' 原型链中的任何属性都会产生值. 另一个方法是hasOwnProperty, 用于判断对象是否拥有某个属性, 但不会查找原型链. auther.hasOwnProperty('firstName') ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:7:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"枚举 使用for in循环可以枚举对象中的所有属性. 但是这种枚举是无序的, 而且会遍历整个原型链, 所以需要做判断. var name; for (name in auther) { if (auther.hasOwnProperty(name)) { //do something } } ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:8:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"删除 delete运算符用于删除对象的属性, 但不会触及原型链中的任何对象. delete auther.familyName ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:9:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"减少全局变量污染 无论何时, 使用大量全局变量都不是一个值得推崇的做法. 我们可以定义一个全局的对象, 把需要的全局变量纳入其名称空间, 降低模块间的冲突. ","date":"2017-03-04","objectID":"/javascritp_the_good_parts_object/:10:0","tags":null,"title":"「JavaScript 语言精粹」读书笔记--对象","uri":"/javascritp_the_good_parts_object/"},{"categories":null,"content":"本书第六章的读书笔记，也是我这个系列的最后一篇读书笔记。后面7、8、9章分别讲的“测试、调试与重构”、“设计和架构的原则”以及“使用Lambda表达式编写并发程序”，因为笔记不好整理，就不写了，感兴趣的同学自己买书来看吧。 并行化流操作 关于并行与并发的区别和并行的重要性的讨论这里不做笔记了，直接看Stream类库提供了哪些关于并行的操作把。 如果已经有了一个Stream对象，可以调用parallel方法使其拥有并行操作的能力； 如果想从一个集合类创建一个Stream对象，可以调用parallelStream方法获得一个拥有并行能力的流。 BaseStream提供的sequential方法将会按顺序遍历Stream中的元素 对于同一个Stream对象，如果parallel方法和sequential方法都被调用，最后调用的那个方法将起效，不能同时处于两种模式。 使用并行流，不需要写代码用于处理调度和等待线程池中的某项任务完成，这些工作都交由类库完成。 限制 编写并行流，存在一些与非并行流不一样的约定。 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:0:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"reduce方法的限制 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:1:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"初值必须为组合函数的恒等值 使用恒等值与其他值做reduce运算时，其他值保持不变。比如，使用reduce进行求和运算时，初值必须为0，而进行求积运算时，初值必须为1 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:1:1","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"reduce操作必须符合结合律 因为并行计算时，元素的遍历顺序是不确定的，所以只有符合结合律才能保证结果是确定的。 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:1:2","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"避免持有锁 前面提到过，并行流的操作，是把线程的调度等工作交给了类库解决的，所以不要做持有锁的操作，否则是自找麻烦 性能 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:2:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"主要影响因素 影响并行流的性能的因素主要有5个： 数据大小 因为并行处理会带来分解数据和合并数据的额外开销，所以只有当数据量足够大时使用并行流操作才具有意义，否则就是在浪费资源。 源数据结构 源数据通常是集合，而因为具体的类型不同，造成了分割时的开销不同。 装箱 基本数据类型比装箱类型处理更快。 核的数量 拥有的核数量越多，潜在的性能提升越大。但这里的核是指运行时进程能够使用的核的数量。 单元处理开销 花在每个元素上的处理时间越长，并行带来的性能提升越大。 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:3:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"底层框架 并行流在底层沿用的fork/join框架，fork递归式的分解问题，然后每段并行执行，最终由join合并结果，返回最后的值。 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:4:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"数据结构分解的难易 数据结构对半分解的难易程度，决定了分解的效率。可以将核心类库提供的通用数据结构分为三类： 性能好 ArrayList、数组或者IntStream.range这样的支持随机读取的结构，能够轻易的分解。 性能一般 HashSet、TreeSet这样的数据结构不易公平的分解。 性能差 有的数据结构难于分解，有的结构可能需要花O(N)的时间复杂度来分解。比如：LinkedList，难以对半分解；Streams.iterate和BufferedRead.lines这样长度未知的数据结构也难以分解。 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:5:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"操作的状态 流中的操作，可以分为有状态和无状态。无状态的操作在整个操作中不必维护状态；有状态的操作则有维护状态所需的开销和限制。 避开有状态的操作，可以获得更好的并行性能。无状态的操作包括map、filter和flatMap；有状态的操作包括sorted、distinct和limit。 并行化数组操作 Java 8为数组提供了并行化操作的方法，这些方法在Arrays类中： method description parallelPrefix 任意给定一个函数，计算数组的“和”（任意BinaryOperator） parallelSetAll 使用Lambda表达式更新数组元素 parallelSort 并行化对数组元素排序 ","date":"2017-02-20","objectID":"/java_8_lambdas_functional_programming_note_parallel/:6:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——数据并行化","uri":"/java_8_lambdas_functional_programming_note_parallel/"},{"categories":null,"content":"本章是该书的第五章, 主要讲了方法引用和收集器 方法引用 形如: artist -\u003e artist.getName() (String arg) -\u003e arg.length() 这样的表达式, 可以简写为: Artist::getName String::length 这种简写的语法被称为方法引用. 方法引用无需考虑参数, 因为一个方法引用可以在不同的情况下解析为不同的Lambda表达式, 这依赖于JVM的推断. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:0:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"方法引用的类型 方法引用可以分为四类: 引用静态方法: ClassName::staticMethodName, 比如: String.valueOf 引用特定实例方法: object::instanceMethodName, 比如: str::toString 引用特定类型的任意对象的实例方法: ClassName::instanceMethodName, 比如: String::length 引用构造方法: ClassName::new, 比如: String::new 元素顺序 当我们对集合进行操作时, 有时希望是按照一定的顺序来操作, 而有时又希望是乱序的操作. 有两个方法可以帮助我们进行顺序的操作. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:1:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"乱序 BaseStream.unordered()方法可以打乱顺序, 科技将本来有序的集合变成无序的集合 ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:2:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"排序 Stream.sorted方法有两个签名, 一个无参, 一个有参数Comparator\u003c? super T\u003e comparator 无参的方法要求T实现了Comparable接口 有参方法需要提供一个比较器 收集器 收集器是一种通用的, 从流中生成复杂值的结构. 将其传给collect方法, 所有的流就都可以使用它. 而下面提到的单个收集器, 都可以使用reduce方法模拟. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:3:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"转换成集合 我们可以使用Collectors中的静态方法toList() toSet()等, 将流收集为List或Set stream.collect(toList()) stream.collect(toSet()) 我们不需要关心具体使用的是哪一种具体的实现, Stream类库会为我们选择. 因为我们可以利用Stream进行并行数据处理, 所以选择是否线程安全的集合十分重要. 当然我们也可以指定使用哪一种实现来进行收集: stream.collect(toCollection(ArrayList::new)) ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:4:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"转换成值 Collectors类提供了很多的方法用于转化值, 比如counting maxBy minBy等等, 可以查看javadoc了解. 目前了解到的是, 这三个方法都可以使用Stream中的count max min方法代替, 而不需要作为collect方法的参数 ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:5:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"数据分割 有时我们想按照一个条件把数据分成两个部分, 而不是只获取符合条件的部分, 这时可以使用partitioningBy方法收集. 将它传入collect方法, 可以得到一个Map\u003cBoolean, List\u003e, 然后就可以对相应的数据进行处理了. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:6:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"数据分组 groupingBy方法可以将流分成多个List, 而不仅仅是两个, 接收一个Lambda表达式作为参数, 其返回值作为key, 最后的结果也是一个Map, 形如Map\u003cString, List\u003e. 这一方法类似于SQL中的group by ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:7:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"生成字符串 如果要从流中得到字符串, 可以在得到Stream\u003cString\u003e之后使用Collectors.joining方法收集. 该方法接收3个String参数, 分别是分隔符 前缀 后缀 artists.stream() .map(Artist::getName) .collect(Collectors.joining(\",\", \"[\", \"]\")); ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:8:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"组合收集器 我们可以将收集器组合起来, 达到更强的功能. 书上举了两个栗子🌰 例一 public Map\u003cArtist, Long\u003e numberOfAlbums(Stream\u003cAlbum\u003e albums) { return albums .collect( groupingBy(Album::getMainMusicina, counting())); } 这个方法的目的是统计每个歌手的作品数目. 如果不组合收集器, 我们先用groupingBy得到一个Map\u003cArtist, List\u003cAlbum\u003e\u003e之后, 还要去遍历Map得到统计数目, 增加了代码量和性能开销. 上面的counting方法类似于count方法, 作用于List\u003cAlbum\u003e的流上. 例二 public Map\u003cArtist, List\u003cString\u003e\u003e nameOfAlbums(Stream\u003cAlbum\u003e albums) { return albums .collect( groupingBy(Album::getMainMusician, mapping(Album::getName, toList()))); } 这个方法的目的是得到每个歌手的作品名称列表. 如果不组合收集器, 我们将会先得到一个Map\u003cArtist, List\u003cAlbum\u003e\u003e. 然而, 我们只想得到作品名称, 也就是一个List\u003cString\u003e, 组合mapping收集器可以帮助我们实现效果. mapping收集器的功能类似于map, 将一种类型的流转换成另一种类型. 所以类似的, mapping并不知道要把结果收集成什么数据结构, 它的第二个参数就会接收一个普通的收集器, 比如这里的toList, 来完成收集. 这里的counting和mapping是我们用到的第二个收集器, 用于收集最终结果的一个子集, 这些收集器叫做下游收集器. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:9:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"定制收集器 定制收集器看起来麻烦, 其实抓住要点就行了. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:10:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"使用reduce方法 前面说过, 这些收集器都可以使用reduce方法实现, 我们定制收集器, 实际上就是为reduce方法编写三个参数, 分别是: identity accumulator combiner 关于这三个参数的意义, 如果不太理解, 可以看看这个答案: https://segmentfault.com/q/1010000004944450 我们可以设计一个类, 为这三个参数设计三个方法, 再提供一个方法用于获取目标类型(如果这个类就是目标类型的话, 可以不提供这个方法) ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:10:1","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"实现Collector接口 如果不想显式的使用reduce方法, 我们只需要提供一个类, 实现Collector接口. 该接口需要三个泛型参数, 依次是: 待收集元素的类型 累加器的类型 最终结果的类型 需要实现的方法有: supplier: 生成初始容器 accumulator: 累加计算方法 combiner: 在并发流中合并容器 finisher: 将容器转换成最终值 characteristics: 获取特征集合 多数情况下, 我们的容器器和我们的目标类型并不一致, 这时, 需要实现finisher方法将容器转化为目标类型, 比如调用容器的toString方法. 有时我们的目标类型就是我们的容器, finisher方法就不需要对容器做任何操作, 而是通过设置characteristics为IDENTITY_FINISH, 使用框架提供的优化得到结果. 详细讲解可以参见http://irusist.github.io/2016/01/04/Java-8%E4%B9%8BCollector/ Map新增方法 Java 8为Map新增了很多方法, 可以通过搜索引擎轻松找到相关文章. 这里举几个书中提到的相关方法. V computeIfAbsent(K key, Function\u003c? super K, ? extends V\u003e mappingFunction) V computeIfPresent(K key, BiFunction\u003c? super K, ? super V, extends V\u003e remappingFunction) V compute(K key, BiFunction\u003c? super K, ? super V, ? extends V\u003e remappingFunction) 这三个方法类似, 都是根据key来处理, 只是Lambda表达式的执行条件不同, 从函数名就可以看出来. 不过要注意Lambda表达式的参数, 第一个方法的Lambda只需要一个参数key, 后面两个方法的Lambda需要两个参数key和value, 而compute方法的Lambda中的value参数可能为null. V merge(K key, V value, BiFunction\u003c? super V, ? super V, ? extends V\u003e remappingFunction) 此方法用于合并value, 新value在第二个参数给出. Lambda表达式规定合并方法, 其两个参数依次是oldValue和newValue, oldValue是原Map的value, 可能为空; newValue为merge方法的第二个参数. void forEach(BiConsumer\u003c? super K, ? super V\u003e action) 通过forEach方法, 不再需要使用外部迭代来遍历Map. ","date":"2017-02-10","objectID":"/java_8_lambdas_functional_programming_note_collector/:10:2","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——高级集合类和收集器","uri":"/java_8_lambdas_functional_programming_note_collector/"},{"categories":null,"content":"本书第四章的读书笔记, 本章主要阐述: 如何使用Lambda表达式. ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:0:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"基本类型 考虑到装箱类型过于占用内存, JDK提供了针对基本类型的操作, 以达到优化的效果, 如mapToLong方法. 对基本类型做特殊处理的方法在命名上有明确规范: 如果返回类型为基本类型, 则在基本类型名称前面加To 如果参数类型是基本类型, 则不加前缀只需类型名即可 如果敢接函数使用基本类型, 则在操作名后加To, 再加基本类型名, 如mapToLong ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:1:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"summaryStatistics方法 这些为基本类型进行优化的Stream还有一些额外的方法, 避免重复实现一些通用方法, 比如summaryStatistics方法. public static void printSummary(List\u003cProduction\u003e productions) { DoubleSummaryStatistics priceStats = productions.stream() .mapToDouble(prod -\u003e prod.getPrice()) .summaryStatistics(); System.out.printf(\"max: %f, min: %f, ave: %f, sum: %f\", priceStats.getMax(), priceStats.getMin(), priceStats.getAberage(), priceStats.getSum()); } ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:1:1","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"重载解析 Lambda表达式作为参数时, 其类型由它的目标类型(方法的参数类型)推导得出, 推导过程遵循如下规则: 如果只有一个可能的目标类型, 由相应函数接口里的参数类型推导得出 如果有多个可能的目标类型, 由最具体的类型推导得出 如果有多个可能的目标类型且最具体的类型不明确, 则需要人为指定类型 ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:2:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"@FunctionalInterface 该注解会强制javac检查一个接口是否符合函数接口的标准. 如果该注解被添加给一个枚举类型, 类或者另一个注解, 或者接口包含不止一个抽象方法, javac就会报错. ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:3:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"默认方法 ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:4:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"产生原因 由于集合框架的基本接口如Collection Map等都新增了stream方法, 在以前的版本中, 第三方的类库如果实现了Collection这样的接口, 必须新增stream方法的实现, 否则无法通过Java 8的编译. 为了避免这种情况, Java 8中添加的新的语言特性: 默认方法 ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:4:1","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"编写方法 Java 8中的任何接口都可以添加默认方法, 使用default关键字修饰, 比如forEach方法: default void forEach(Consumer\u003c? super T\u003e action) { for (T t : this) { action.accept(t); } } 因为接口没有成员变量, 所以默认方法只能通过调用子类的方法来修改子类本身. ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:4:2","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"默认方法的重写 假设: 接口A有默认方法a, 接口B扩展了接口A, 并重写了方法a 类C实现接口A, 并重写方法a 类D实现接口B, 并重写方法a 没有重写的情况 一个类实现接口A, 则会调用接口A的实现 一个类实现接口B, 则会调用接口B的实现 继承于C D的类, 无论是否实现了接口A或B, 都将会调用C D的实现 实现A与B, 但没有继承C或D的类将无法通过编译 有重写的情况 无论继承情况如何, 只要重写了默认方法, 都将调用自己的实现 三定律 类胜于接口 子类胜于父类 没有规则三, 如果上面两条不适用, 子类需要实现该方法, 或声明为抽象方法 ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:4:3","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"接口的静态方法 如果一个方法有充分的语义原因和某个概念相关, 那么就应该将方法和相关的类或接口放在一起, 而不是放到另一个工具类中. 基于这个原因, Java 8提供了接口的静态方法的支持. Stream接口中就包含多个静态方法用于生成Stream对象. ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:5:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"Optional Optional是为核心类库新设计的一个数据类型, 用于替换null值. 它可以接收一个泛型参数. 调用get方法获得泛型类型的对象. isPresent方法判断是否为空 orElse orElseGet orElseThrow方法可以自由定制为空时的返回值/抛出异常 ","date":"2017-02-08","objectID":"/java_8_lambdas_functional_programming_note_lib/:6:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——类库","uri":"/java_8_lambdas_functional_programming_note_lib/"},{"categories":null,"content":"本文是「Java 8 函数式编程」第三章的读书笔记，章名为流。本章主要介绍了外部迭代与内部迭代以及常用的高阶函数。 外部迭代与内部迭代 ","date":"2017-02-06","objectID":"/java_8_lambdas_functional_programming_note_stream/:0:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——流","uri":"/java_8_lambdas_functional_programming_note_stream/"},{"categories":null,"content":"外部迭代 过去我们要对一个List进行迭代时，往往会采用如下方式： int count = 0; for (Artist artist : artists) { if (artist.isFrom(\"London\")) { count++; } } 而这种方法的原理，其实是先调用iterator方法，然后再迭代，等效于如下代码： int count = 0; Iterator\u003cArtist\u003e iterator = artists.iterator(); while (iterator.hasNext()) { Artist artist = iterator.next(); if (artist.isFrom(\"London\")) { count++; } } 这样的迭代方式，把迭代的控制权交给了iterator对象，让其控制整个迭代过程，这就叫做外部迭代。 外部迭代需要我们自己编写迭代的控制代码，显得十分繁琐。特别是对于Map对象，繁琐到我都不想给出例子。 外部迭代将行为和方法混为一谈，难以对代码进行重构操作。 ","date":"2017-02-06","objectID":"/java_8_lambdas_functional_programming_note_stream/:1:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——流","uri":"/java_8_lambdas_functional_programming_note_stream/"},{"categories":null,"content":"内部迭代 与之相对的就是内部迭代了。内部迭代就是把迭代的控制权交给了集合本身，让集合自己实现相应的迭代，而调用者并不需要关心如何迭代。 要使用内部迭代，需要使用Java 8中新增的接口Stream。而集合框架都已经包含了一个stream()方法，用于获得Stream对象。 long count = artists.stream() .filter(artist -\u003e artist.isFrom(\"London\")) .count(); 这个例子就是使用的内部迭代。先获取stream对象，然后调用filter方法过滤，最后统计符合条件的个数。 实现机制 在Java中调用一个方法，通常会立即执行操作。然而Stream里的一些方法却不太一样，它们返回的对象不是新的集合，而是创建新集合的配方。我们通过一个例子说明： Stream\u003cString\u003e names = Stream.of(\"Bryant\", \"Jordon\", \"James\") .filter(name -\u003e { System.out.println(name); return name.length() == 6; }); System.out.println(\"counting\"); System.out.println(names.count()); 最终会得到如下输出： counting Bryant Jordon James 2 出现这样的结果，原因是 像filter这样的方法，只会描述Stream，最终不会产生新集合的方法叫做惰性求值方法 像count这样会从Stream中产生值或集合等结果的方法叫做及早求值方法 判断一个操作是惰性求值还是及早求值，只需要看它的返回值 如果返回值是Stream，则是惰性求值 返回的是一个值或null，则是及早求值 在对集合使用流操作时，使用惰性求值方法形成一个惰性求值的链，最后用及早求值方法得到结果，而集合只需要迭代一次。 常用流操作 collect：及早求值，常用于生成List Map或其他复杂的数据结构 map：惰性求值，将一种类型的数据转换成另一种类型，将一个流中的值转化成一个新的流，类似于Hadoop里的map filter：惰性求值，过滤不符合条件的元素 flatMap：惰性求值，类似于map，只是Function参数的返回值限定为Stream，用于连接多个Stream成为一个Stream max \u0026 min：及早求值，reduce方法的特例，返回Optional（第四章介绍）对象 reduce：及早求值，从一组值中生成一个值，类似于Hadoop中的reduce 高阶函数 高阶函数 👉 接收一个函数作为参数，或者返回一个函数的函数。 正确使用Lambda表达式 明确要达成什么转化，而不是说明如何转化 没有副作用： 只通过函数的返回值就能充分理解函数的全部作用 函数不会修改程序或外界的状态 获取值而不是变量（避免使用数组逃过JVM的追杀，应该考虑优化逻辑） ","date":"2017-02-06","objectID":"/java_8_lambdas_functional_programming_note_stream/:2:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——流","uri":"/java_8_lambdas_functional_programming_note_stream/"},{"categories":null,"content":"本文是「Java 8 函数式编程」第二章的读书笔记。 Lambda引入的变化 Lambda表达式，是一种紧凑的、传递行为的方式，从编程思想上来讲，就是代码即数据。 过去的Java中，存在大量的匿名内部类的使用，会新建一个匿名内部类传入调用的方法中。这种传统的方式，会造成冗余的、不易阅读的代码。 于是Lambda诞生了。Lambda的语法简化了使用匿名内部类时的模板代码，让程序员专注于编写想要执行的行为，也让代码更加简洁易读。 Lambda表达式的形式 Runnable runable = () -\u003e System.out.println(\"Hello Lambda\");//1 runable = () -\u003e { System.out.print(\"Hello\"); System.out.println(\" Lambda\"); };//2 ActionListener listener = event -\u003e System.out.println(\"get event\");//3 BinaryOperator\u003cLong\u003e add = (x, y) -\u003e x + y;//4 BinaryOperator\u003cLong\u003e minux = (Long x, Long y) -\u003e x - y;//5 常见的Lambda表达式有以上5种，每个Lambda表达式都可以分为三个部分： 参数部分：() event (x, y) (Long x, Long y) 将参数和表达式主体分开的符号：-\u003e 表达式主体 ","date":"2017-02-05","objectID":"/java_8_lambdas_functional_programming_note_lambda_expression/:0:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——lambda表达式","uri":"/java_8_lambdas_functional_programming_note_lambda_expression/"},{"categories":null,"content":"参数的形式 Lambda表达式可以看作是匿名内部类的简写形式，参数也就是使用匿名内部类时实现的方法的参数。 有的方法不需要参数，如Runnable的run方法，所以使用()代表参数部分。 有的方法只需要一个参数且类型确定，如ActionListener.actionPerformed方法，可以直接使用参数，不需要指定类型，也不需要加括号。 有多个参数时，必须要加上括号，把参数扩起来 当声明参数类型时，无论有多少个参数，都需要加括号 ","date":"2017-02-05","objectID":"/java_8_lambdas_functional_programming_note_lambda_expression/:1:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——lambda表达式","uri":"/java_8_lambdas_functional_programming_note_lambda_expression/"},{"categories":null,"content":"表达式主体的形式 表达式可以只有一行代码，也可以有多行代码；有的表达式有返回值，有的没有。 只有一行代码的表达式不需要{} 如果有返回值，不用写return，表达式会把这行代码的返回值作为返回值 如果使用了{}，则需要显式的写出return 有多行代码的表达式必须使用{} 如果有返回值，需要显式的写return 引用值，而不是变量 匿名内部类中，如果想要引用其所在方法中的变量，需要将其声明为final。这意味着你实际引用的是一个值，而不是变量。 在Java 8 中，虽然可以引用非final的变量，但这个变量必须是既成事实上的final，如果对变量进行修改，将无法通过编译。这意味着Lambda表达式仍然是引用的一个值，而不是变量。 实际上可以通过使用数组来绕开编译器，但是这样做之前应该考虑一下你的代码逻辑是否正确。 函数接口 只有一个抽象方法的接口叫做函数接口。 JDK中最重要的函数接口： Interface Argument Return e.g. Predicate T boolean fliter Consumer T void forEach Function\u003cT, R\u003e T R map Supplier None T factory function UnaryOperator T T modify String BinaryOperator (T, T) T add two instances 类型推断 Java 8为新成员Lambda表达式提供了类型推断的支持，在不需要声明参数类型的Lambda表达式中表现的有为明显。形如： BinaryOperator\u003cInteger\u003e add = (x, y) -\u003e x + y; 的表达式得以通过编译并正确执行，就是因为JVM通过泛型参数Integer推断出了方法参数的类型。 ","date":"2017-02-05","objectID":"/java_8_lambdas_functional_programming_note_lambda_expression/:2:0","tags":["java","lambda","Java Lambda","Java 8","函数式编程"],"title":"「Java 8 函数式编程」读书笔记——lambda表达式","uri":"/java_8_lambdas_functional_programming_note_lambda_expression/"},{"categories":null,"content":"一开始使用spring拦截器拦截请求记录日志,对于请求路径、header这些都很好获取,唯独POST请求无法获取其中的RequestBody. 原因很明显,RequestBody是只能读取一次的,如果在拦截器中读取了,就无法通过@RequestBody注解去获取,因为这个数据是内存中的流数据. 那要如何做呢?明显就需要用到aop了 利用AOP读取RequestBody aop就是用来做切面的,具体概念这里不说了,自行谷歌.这里就讲具体问题的思路. 这里用aop去切入Controller里面所有public的方法,利用JoinPoint获取参数,从而得到RequestBody. 得到RequestBody后使用Spring Boot的日志打印 ","date":"2016-11-20","objectID":"/get_requestbody_by_aop/:0:0","tags":null,"title":"使用AOP获取RequestBody","uri":"/get_requestbody_by_aop/"},{"categories":null,"content":"创建一个切面 @Aspect @Component public class RequestMapAspect {} @Aspect注解表明这是一个切面 @Componet注册为组件,否则无法使用Spring Boot的日志 ","date":"2016-11-20","objectID":"/get_requestbody_by_aop/:1:0","tags":null,"title":"使用AOP获取RequestBody","uri":"/get_requestbody_by_aop/"},{"categories":null,"content":"声明一个logger private final Log log = LogFactory(this.getClass()); 这里的Log与LogFactory均在org.apache.commons.logging包下,不是slf4j,不是logback也不是log4j! ","date":"2016-11-20","objectID":"/get_requestbody_by_aop/:2:0","tags":null,"title":"使用AOP获取RequestBody","uri":"/get_requestbody_by_aop/"},{"categories":null,"content":"声明切点 @Pointcut(value = \"excution(public * cn.gaoyuexiang.controller.*.*(..))\") public void controllerLog() {} ","date":"2016-11-20","objectID":"/get_requestbody_by_aop/:3:0","tags":null,"title":"使用AOP获取RequestBody","uri":"/get_requestbody_by_aop/"},{"categories":null,"content":"编写Before获取参数 @Before(\"controllerLog()\") public void before(JoinPoint point) { logger.info(\"controller aspect begging\"); Object[] args = point.getArgs(); for (Object arg : args) { logger.info(\"arg: \" + arg); } String method = point.getSignature().getDeclaringTypeName() + '.' + point.getSignature().getName(); logger.info(\"aspect finishing\"); logger.info(\"calling \" + method); } JoinPoint.getArgs()可以得到目标方法的参数,返回类型为Object[] RequestBody作为参数时,因为是在调用方法前已经被Spring读取并解析了,所以不存在重复读取的问题.调用目标方法时,Before的aop被执行,拿到传递给目标的参数 ","date":"2016-11-20","objectID":"/get_requestbody_by_aop/:4:0","tags":null,"title":"使用AOP获取RequestBody","uri":"/get_requestbody_by_aop/"},{"categories":null,"content":"最近一个项目要上线了,需要搭服务器,本来是交给同学搭的,结果遇到了大坑,还得自己来,今天把这些坑记一下. 服务器有好几台,都是CentOS6.X,两台6.8,一台6.4. 项目需要的环境是Java+Gradle+MySql+Redis+Nginx 太长不看总结版 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:0:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"顺利的安装 # 准备下载工具 wget http://ftp.tu-chemnitz.de/pub/linux/dag/redhat/el6/en/x86_64/rpmforge/RPMS/axel-2.4-1.el6.rf.x86_64.rpm yum localinstall axel-2.4-1.el6.rf.x86_64.rpm -y # 安装MySql wget http://dev.mysql.com/get/mysql57-community-release-el6-9.noarch.rpm yum localinstall mysql57-community-release-el6-9.noarch.rpm -y yum replist all | grep mysql yum install yum-utils -y yum-config-manager --disable mysql57-community yum-config-manager --enalble mysql56-community yum install mysql mysql-server mysql-devel -y # 安装Nginx echo -e '[nginx]\\nname=nginx repo\\nbaseurl=http://nginx.org/packages/centos/6/$basearch/\\ngpgcheck=0\\nenabled=1' \u003e /etc/yum.repos.d/nginx.repo yum makecache yum install nginx -y # 安装Redis yum install -y tcl axel -n 5 http://download.redis.io/releases/redis-2.8.24.tar.gz cd /usr/local tar zxf ~/redis-2.8.24.tar.gz mv redis-2.8.24/ redis/ cd redis make make test cd src/ make install # 安装Java wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz mkdir /usr/java cd /usr/java tar zxf ~/jdk-8u111-linux-x64.tar.gz echo 'export JAVA_HOME=/usr/java/jdk1.8.0_111' \u003e\u003e /etc/profile echo 'export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar' \u003e\u003e /etc/profile echo 'export PATH=$PATH:$JAVA_HOME/bin' \u003e\u003e /etc/profile source /etc/profile java -version # 安装Gradle axel -n 5 https://services.gradle.org/distributions/gradle-2.14-bin.zip mkdir /usr/local/gradle cd /usr/local/gradle unzip -q ~/gradle-2.14-bin.zip echo 'export PATH=$PATH:/usr/local/gradle/gradle-2.14/bin' \u003e\u003e /etc/profile source /etc/profile gradle -version ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:1:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"坑 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:2:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"Docker的莫名错误 在CentOS6.4上面试过Docker,启动容器后报错,找不到容器 在CentOS6.8上面,Docker的守护进程总是过一会儿自己就挂掉 第二台CentOS6.8上面,因为iptables的配置问题,导致127.0.0.1无法访问,引起Docker映射端口没有作用 第二台CentOS6.8上面使用MySql的镜像,启动配置MYSQL_ROOT_PASSWORD环境变量后面的启动配置不能生效 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:2:1","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"MySql的安装中的坑 CentOS6默认的yum安装的MySql版本是5.1,会出现中文乱码的问题,需要下载mysql57-community-release-el6-9.noarch.rpm来添加yum源 添加后的仓库默认激活MySql5.7,CentOS6并不支持,需要激活MySql5.6 enable与disable哪个包可以使用yum-config-manager进行配置,这个命令包含在yum-utils包中,可通过yum安装 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:2:2","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"localhost,127.0.0.1\u0026iptables 现在的主机默认localhost会解析到127.0.0.1,而127.0.0.1会发送到环回的虚拟网卡,回到主机上 MySql是个特立独行的家伙,默认的mysql连接会访问localhost,但不会映射到127.0.0.1上,而是访问配置文件中描述的sock文件,不会经历网络层的传输 127.0.0.1虽然不会离开主机,但也会经过网卡(虽然是虚拟的),所以也会收到iptables的影响 不要随意删除iptables里面的规则,否则真的会后悔的 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:2:3","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"tips linux上的命令行多线程下载工具:axel,地址:https://pkgs.org/centos-6/repoforge-x86_64/axel-2.4-1.el6.rf.x86_64.rpm.html nginx可通过yum下载最新稳定版:http://nginx.org/en/linux_packages.html 在旧的发行版上安装mysql,优先考虑更新源,而不是下载安装包 在旧的发行版上,就不要尝试docker了,遇到坑伤不起 更新你的服务器,大清亡了 详细版 在CentOS6.4上的折腾 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:3:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"关于Mysql的折腾 我通过MySql官网下载的yum源安装的MySql,在CentOS6.4的系统上执行 yum install mysql mysql-server mysql-devel 没有遇到任何依赖错误,于是顺利安装,然后启动服务 servcie mysqld start 但是得到了失败的结果.于是查看配置文件,找到日志,发现了如下错误信息 2016-11-17T21:44:32.121190Z 0 [ERROR] Fatal error: mysql.user table is damaged. Please run mysql_upgrade. 2016-11-17T21:44:32.121315Z 0 [ERROR] Aborting 于是按照他的提示,执行mysql_upgrade,却又提示没有启动mysqld服务..这是一个死锁啊 😵 然后移除了这个MySql5.7,向系统妥协,用yum装了个5.1的版本,导入建库语句,没有错误产生,暗道庆幸,还以为是自己多虑了. 然而,等我放入数据之后,该来的还是来了. 我在本地开发环境中是用的是docker(后面会讲为什么不在服务器上用docker)拉取的MySql5.7,本地测试一切正常.然后放到服务器上,当我查询一个中文的记录的时候,杀机终于显现——乱码啦 🙀 作为一个对自己代码拥有充分自信的家伙,怎么能是我的问题呢,都怪MySql的版本太低太弱智. 在第一台CentOS6.8上的折腾 所以我决定换一台服务器,从头再来.还好实验室服务器多,够我折腾 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:4:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"先来折腾MySql 基于在上一台服务器上的惨痛教训,这次我决定先装MySql. 首先确定发行版本 $ cat /etc/issue CentOS release 6.8 (Final) Kernel \\r on an \\m 准备下载工具axel wget http://ftp.tu-chemnitz.de/pub/linux/dag/redhat/el6/en/x86_64/rpmforge/RPMS/axel-2.4-1.el6.rf.x86_64.rpm yum localinstall -y axel-2.4-1.el6.rf.x86_64.rpm 竟然连wget也要安装一下 😂 然后安装MySql的yum源并安装MySql,这个文件只有9k,直接用wget就行了 wget http://dev.mysql.com/get/mysql57-community-release-el6-9.noarch.rpm yum localinstall mysql57-community-release-el6-9.noarch.rpm -y yum install -y mysql mysql-server mysql-devel 然后出现依赖报错 Error: Package: mysql-community-server-5.7.34-2.el7.x86_64 (mysql56-community) Requires: libc.so.6(GLIBC_2.17)(64bit) Error: Package: mysql-community-server-5.7.34-2.el7.x86_64 (mysql56-community) Requires: systemd Error: Package: mysql-community-client-5.7.34-2.el7.x86_64 (mysql56-community) Requires: libc.so.6(GLIBC_2.17)(64bit) Error: Package: mysql-community-libs-5.7.34-2.el7.x86_64 (mysql56-community) Requires: libc.so.6(GLIBC_2.17)(64bit) Error: Package: mysql-community-server-5.7.34-2.el7.x86_64 (mysql56-community) Requires: libstdc++.so.6(GLIBCXX_3.4.15)(64bit) You could try using --skip-broken to work around the problem ** Found 1 pre-existing rpmdb problem(s), 'yum check' output follows: tzdata-2016i-1.el6.noarch is a duplicate with tzdata-2012j-1.el6.noarch 我网上找了半天也没找到怎么解决这个错误的解决办法,应该是CentOS6.8不支持,于是放弃自己安装,准备上docker ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:5:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"莫名挂掉的Docker 我平常使用的是daocloud.io的docker镜像,这里也使用这个安装 curl -sSL https://get.daocloud.io/docker | sh chkconfig docker on servcie docker start 然后拉取MySql的镜像、运行,一切正常.然后拉取redis和nginx,如果一切正常的话,很快就能搞定了. 然而可怕的情况出现了,拉取过程挂掉了… 😱 于是查看docker守护进程,发现守护进程竟然挂掉了!我可是什么都没做啊!而且三更半夜,谁会和我一起搞服务器?! ❓ 对于这种莫名的原因,我感觉很是害怕.因为之前这台主机是被动过的,所以我不知道是不是别人动过什么.决定再换一台没动过的主机来搞. 在第二台CentOS6.8上的折腾 我没有在这台主机上尝试docker,我已经不信任在CentOS6.X环境中的docker了,直接来安装MySql吧 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:6:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"还是先来MySql 这次我找到了这篇文章https://segmentfault.com/a/1190000003049498 什么,竟然yum源还有激活这个事情?! 😱 我还是太天真,太孤陋寡闻.于是赶紧查看一下MySql源的激活情况 yum repolist all | grep mysql 果然默认激活5.7,于是我选择降低一个版本,使用5.6 yum install -y yum-utils yum-config-manager --disable mysql57-community yum-config-manager --enalbe mysql56-community yum install -y mysql mysql-server mysql-devel 欣喜的看到安装成功了,但是有之前在CentOS6.4上的惨痛教训,我还是不敢高兴的太早,至少mysql先要能运行起来 service mysqld start 然后根据提示,可以执行 mysqladmin -u root password 'new_root_pass' mysql_secure_installation 然后就是少有的顺利安装完成,然后登录,创建一个数据库和用户并授权,再用新的用户登录建库,都没有问题.我想应该是OK了 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:7:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"用yum安装Nginx-1.10.2 接下来就是nginx了,官网就有教程教你怎么用yum安装http://nginx.org/en/linux_packages.html 于是我就照着教程的样子创建了一个/etc/yum.repos.d/nginx.repo,内容如下: [nginx] name=nginx repo baseurl=http://nginx.org/packages/centos/6/$basearch/ gpgcheck=0 enabled=1 然后安装、配置、启动、测试都正常通过.按照我\"多年\"的nginx使用经验,这个家伙是安装完成了. ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:8:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"安装Redis–异常初现 接着是Redis,只有编译安装这一个方法.因为使用的是Spring框架连接Redis所以下载了一个Redis-2.8 yum install -y tcl # tcl 是 redis 的依赖,可以通过执行 {redis_home}/runtest 了解 redis 需要安装的依赖 axel -n 5 http://download.redis.io/releases/redis-2.8.24.tar.gz cd /usr/local tar zxf ~/redis-2.8.24.tar.gz mv redis-2.8.24/ redis/ cd redis make make test cd src/ make install 然后在make test的时候出现了错误 Executing test client: couldn't open socket: host is unreachable. couldn't open socket: host is unreachable while executing google了好久也没找到解决方案,索性就不管它,直接启动redis-server,然后使用客户端测试连接,却得到错误 Could not connect to Redis at 127.0.0.1:6379: No route to host 这我就想不通了,但是没关系,之前那台CentOS6.4上面还有redis服务,配置一下防火墙就行了,先测试mysql乱不乱码要紧. ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:9:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"异常真凶浮出水面 于是我急匆匆的把代码clone下来,修改一下redis-server地址就跑起来了. 然后访问一下试试.然后Spring Boot内置的Tomcat的数据库连接池就报错了. java.net.NoRouteToHostException: 没有到主机的路由 这次我就真懵逼了:Mysql不是在运行吗?不是能连上吗?这怎么就不行了?难道我必须得用Docker,还要解决daemon离奇死亡的问题? 咦?Docker?MySql? 于是我想到了什么,便在服务器的命令行连接了一次MySql $ mysql -h 127.0.0.1 -u user -ppassword ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (113) 果然,和redis遇到的错误是同一个原因:无法访问127.0.0.1上的服务 我是怎么想到的? 因为我本地的mysql是使用docker运行的,所以我要连接mysql,访问地址必须得是127.0.0.1,所以项目的配置就写的127.0.0.1 为什么呢?因为mysql对localhost的请求做了优化,直接走socket的路线,没有通过127.0.0.1去连接,所以docker运行的mysql如果没做sock文件的映射,是不能通过localhost访问的.而mysql客户端默认使用的地址正是localhost 这就是为什么一开始我能通过mysql命令连接上mysql-server,而加上-h 127.0.0.1参数就不行了的原因. 知道了原因,那就很好对症下药了. ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:10:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"菜鸡的防火墙配置–服务器血崩 127.0.0.1虽然是访问的本机,但实际请求也是会经过网卡(虚拟网卡)的.所以应该是防火墙拦截了我的请求.那么开始与iptables做斗争吧. 对一个安全菜鸡来说,搞防火墙是一件需要小心谨慎的事情;但对一个熬夜的菜鸡来说,理智什么的都已经走远了 😂 这是一开始我看到的iptables Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9000 2 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 3 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 4 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:22 5 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT) num target prot opt source destination 1 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 2 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 3 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited 4 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT) num target prot opt source destination 第一条规则是我设置的,确定没问题(这时我还保有理智) 第二条什么鬼?删了! 于是一条命令执行出去了 iptables -D INPUT 2 然后这台主机今晚就变成一条废咸鱼了,我也像吃了鲱鱼罐头一样清醒了. 我竟然把RELATED和ESTABLISHED的连接都给拒绝了!!! 😱 经过半个小时的头(zi)脑(wo)风(feng)暴(ci), 我想起第一台CentOS6.8的主机,也许我激活一下mysql5.6还有救 回到第一台CentOS6.8的再次折腾 有了前几次的经验总结,这一次搭建变得顺利起来 ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:11:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"顺利安装MySql 先是安装MySql yum-config-manager --disable mysql57-community yum-config-manager --enable mysql56-community yum install -y mysql mysql-server mysql-devel service mysqld start mysqladin -u root password 'new_root_pass' mysql_secure_installation ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:12:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"成功安装并运行的Redis 接下来是Redis yum install -y tcl axel -n 5 http://download.redis.io/releases/redis-2.8.24.tar.gz cd /usr/local tar zxf ~/redis-2.8.24.tar.gz mv redis-2.8.24/ redis/ cd redis make make test cd src/ make install 对于Redis还修改了一些默认配置 daemonize yes bind 0.0.0.0 logfile \"/var/log/redis.log\" 然后使用这个配置文件启动 redis-server /usr/local/redis/redis.conf ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:13:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"同样顺利的Nginx 然后是Nginx,先按照官网的方法设置yum仓库 yum makecache yum install -y nginx service nginx start 这个方法安装的Nginx是默认开机启动的.测试一下,能够正常运行,然后配置了一下,做了个反向代理绕过跨域,重启. ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:14:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"安装Java 接着是Java wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz mkdir /usr/java cd /usr/java tar zxf ~/jdk-8u111-linux-x64.tar.gz echo 'export JAVA_HOME=/usr/java/jdk1.8.0_111' \u003e\u003e /etc/profile echo 'export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar' \u003e\u003e /etc/profile echo 'export PATH=$PATH:$JAVA_HOME/bin' \u003e\u003e /etc/profile source /etc/profile java -version ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:15:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":"安装Gradle 最后是Gradle axel -n 5 https://services.gradle.org/distributions/gradle-2.14-bin.zip mkdir /usr/local/gradle cd /usr/local/gradle unzip -q ~/gradle-2.14-bin.zip echo 'export PATH=$PATH:/usr/local/gradle/gradle-2.14/bin' \u003e\u003e /etc/profile source /etc/profile gradle -version 终于,把所有的东西都装好了,赶快把代码拉下来跑一跑,终于没有乱码了,接口都能正常使用.而此时,天都开始亮了.. ","date":"2016-11-17","objectID":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/:16:0","tags":null,"title":"服务器配置小记","uri":"/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E5%B0%8F%E8%AE%B0/"},{"categories":null,"content":" 运行时注入与硬编码注入是相对的。硬编码注入在编译时就已经确定了，运行时注入则可能需要一些外部的参数来解决。 Spring提供的两种在运行时求值的方式： 属性占位符(Property placeholder) Spring表达式语言(SpEL) ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:0:0","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"注入外部的值 使用@PropertySource注解可以引入.properties文件，使用其中的值。 @Configuration @PropertySource(\"classpath:jdbc.properties\") public class JDBCConfig { @Autowired Environment env; @Bean public DataSource dataSource() { env.getProperties(\"driver\"); ... } } ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:1:0","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"深入了解Spring中的Environment 上例的Environment有如下方法获取属性 String getProperty(String key); String getProperty(String key, String defaultValue); T getProperty(String key, Class type); T getProperty(String key, Class type, T defaultValue); 这几个重载方法的作用顾名思义。其中第一、三个方法获取一个不存在的属性时，会抛出IllegalStateException异常。 可以使用containsProperty(String key)方法查看是否存在某个属性。 其他相关方法： Class\u003cT\u003e getPropertyAsClass(String key, Class\u003cT\u003e targetType) : 将获取的属性转换为类 String[] getActiveProfiles() : 返回激活profile名称的数组 String[] getDefaultProfiles() : 返回默认profile名称的数组 boolean acceptsProfiles(String... profiles) : 如果environment支持给定的profile，则返回true ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:1:1","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"解析属性占位符 使用占位符，可将属性定义到外部的.properties文件中，然后使用占位符插入到bean中。占位符使用${...}包装属性名称。 在Java配置中使用@Value注解。 public BlankDisc(@Value(\"${disc.title}\") String title, @Value(\"${disc.artist}\") String artist) { this.title = title; this.artist = artist; } 使用占位符必须配置一个PropertySourcesPlaceholderConfigurer bean，它能够基于Spring Environment及其属性来解析占位符。 @Bean public PropertySourcesPlaceholderConfigurer placeholderConfigurer() { return new PropertySourcesPlaceholderConfigurer(); } ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:1:2","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"使用Spring表达式语言进行装配 SpEL主要特性： 使用bean的ID来引用bean 访问对象的属性和方法 可对值进行算数、关系和逻辑运算 正则表达式匹配 集合操作 SpEL还可以用在DI之外的地方 ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:0","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"SpEL样例 SpEL表达式要放在#{ ... }中，里面的\"…“就是SpEL表达式。 #{1} 常量，结果始终为1 #{T(System).currentTimeMillis()} T()表达式会将java.lang.System视为Java中对应的类型，然后调用其方法，获取当前时间戳。 #{dataSource.user} dataSource为声明的其他bean，这里可以获取它的属性user #{systemProperties[‘username’]} 通过systemProperties对象获取系统属性 ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:1","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"表示字面量 可表示的字面量有int,float/double,String,boolean，其中浮点值可以用科学技术法表示：#{6.18E3} ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:2","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"引用bean、属性和方法 引用对象 表达式 bean #{dataSource} bean’s field #{dataSource.user} bean’s method #{dataSource.getPassword()} bean’s method’s method #{dataSource.getPassword().toUpperCase()} 如果方法返回值为null，第四种情况会抛出NullPoniterException。可以使用： #{dataSource.getPassword()?.toUpperCase()} 其中的?.运算符能够在访问前确保不为null，否则返回null。 ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:3","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"在表达式中使用类型 使用T()表达式来访问Java类中的static方法和常量，在括号内的是类名，返回一个Class对象，然后调用其方法和常量。 ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:4","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"SpEL运算符 运算符类型 运算符 算数运算 +, -, *, /, %, ^ 比较运算 \u003c, \u003e, ==, \u003c=, \u003e=, lt, gt, eq, le, ge 逻辑运算 and, or, not, | 条件运算 ?: (ternary), ?: (Elvis) 正则表达式 matches Elvis运算符 利用三元运算符来检查场景：#{disc.title ?: 'Rattle and Hum'}，当disc.title为null时，返回\"Rattle and Hum\"。 名称的来历据说是因为'?‘长得像猫王的头发。。。 😲😲😲 正则表达式 正则表达式利用matches来支持正则匹配。 ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:5","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"计算集合 引入一个元素 : #{jukebox.songs[4].title} 随机选取 : #{jukebox.songs[T(Math).random() * jukebox.songs.size()].title} 从String中获得char : #{'This is a test'[2]} 使用.?[]进行过滤，得到符合条件的子集 : #{jukebox.songs.?[artist eq 'Aerosmith']} 使用.^[]和.$[]进行过滤，得到第一个和最后一个匹配项 使用.![]从集合的每个成员选择特定属性放入新集合中 : #{jukebox.songs.![title]} 最后四个表达式有点像lambda表达式 SpEL的表达式可以相互组合使用。 更多Spring学习笔记：https://github.com/kbyyd24/spring.demo.test/issues ","date":"2016-08-17","objectID":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/:2:6","tags":null,"title":"Spring高级装配之运行时注入","uri":"/spring%E9%AB%98%E7%BA%A7%E8%A3%85%E9%85%8D%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%B3%A8%E5%85%A5/"},{"categories":null,"content":"题目 先把题目放上： 链接：https://leetcode.com/problems/sort-colors Given an array with n objects colored red, white or blue, sort them so that objects of the same color are adjacent, with the colors in the order red, white and blue. Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Note: You are not suppose to use the library’s sort function for this problem. Follow up: A rather straight forward solution is a two-pass algorithm using counting sort. First, iterate the array counting number of 0’s, 1’s, and 2’s, then overwrite array with total number of 0’s, then 1’s and followed by 2’s. Could you come up with an one-pass algorithm using only constant space? 解题思路 拿到这个题目，第一个想到的就是遍历，计数，然后赋值。这个方法很容易想到，题目也给出了提示。 然而，能只用一次遍历就得到预期的数组吗？ 当然可以。**按照要求，就是对数组进行遍历，找到0就放到前面去，找到1就放到中间，找到2就放到后面去。**有没有很眼熟？ 没错，这就是一个快速排序，因为只有0，1，2这三种数，所以仅仅需要一次遍历就能完成，连赋值都变得简单了起来。 😄 代码 public class Solution { public void sortColors(int[] nums) { if (nums == null || nums.length \u003c 2) return; int i = 0, j = 0, k = nums.length - 1; final int red = 0; final int white = 1; final int blue = 2; while (j \u003c= k) { if (nums[j] \u003c white) { nums[j++] = nums[i]; nums[i++] = red; } else if (nums[j] \u003e white) { nums[j] = nums[k]; nums[k--] = blue; } else { j++; } } } } ","date":"2016-07-29","objectID":"/sort_solors-%E8%A7%A3%E9%A2%98%E6%80%9D%E8%B7%AF/:0:0","tags":null,"title":"Sort Colors 解题思路","uri":"/sort_solors-%E8%A7%A3%E9%A2%98%E6%80%9D%E8%B7%AF/"},{"categories":null,"content":"在慕课网上看了高并发的课程，准备用spring+Mybaits来开发新的项目。遇到了前端跨域请求的问题。 服务器上nginx+tomcat，其中nginx监听80端口，tomcat监听8080端口。 因为对前端不熟悉，以为用ajax就可以不需要callback，然而前端的同学说不跨域的情况下才不需要callback，让我在返回的json里加上。可是我刚刚学会了最基本的spring-mvc用法，根本不知道怎么加上callback 😂 网上到时找到一些可行的代码，差不多这个样子： 来源：http://quarterlifeforjava.iteye.com/blog/2218530 @RequestMapping(method=RequestMethod.GET,value=\"getProjectStatusList\",produces=\"text/html;charset=UTF-8\") @ResponseBody public String getProjectStatusList(HttpServletRequest request, HttpServletResponse response){ Map\u003cString,Object\u003e map = new HashMap\u003cString,Object\u003e(); try{ String callback = request.getParameter(\"callback\"); //System.out.println(\"token:\"+request.getHeader(\"token\")); List\u003cString\u003e list = ss.getProjectStatusList(); map.put(\"status\", \"success\"); map.put(\"data\", list); ObjectMapper mapper = new ObjectMapper(); //这个拼接是重点。。。 String result = callback+\"(\"+mapper.writeValueAsString(map)+\")\"; //String result = mapper.writeValueAsString(map); return result; }catch(Exception e){ JSONObject jo = new JSONObject(); jo.put(\"status\", \"fail\"); jo.put(\"data\", e.getMessage()); return jo.toString(); } } 然而这样改动对我来说简直是伤筋动骨，因为我有太多的URL映射，修改的成本太大。 所以机智的我想到了nginx，这家伙不就是拿来搞反向代理的吗？真是机智如我 😎 有了这个思路，做起来就简单了。直接在监听80端口的server中添加一个location： location /myApp { proxy_pass http://localhost:8080/myApp; } 重新加载nginx： {NGINX_HOME}/sbin/nginx -s reload 然后就把之前http://site:8080/myApp的跨域请求变成了http://site/myApp的非跨域请求。 好吧，都是我猜的，等前端同学来验证我的想法了 😄 下午实验课自己写了个ajax请求试了下，这个思路是没有问题的 $.ajax({url : \"/myApp/list\", async : true}).success( function (data) { console.log(data); }); 打印结果： Object {data: Array[8], success: true, errorMsg: null} ","date":"2016-06-07","objectID":"/%E5%88%A9%E7%94%A8nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%81%BF%E5%85%8D%E8%B7%A8%E5%9F%9F/:0:0","tags":null,"title":"利用NGINX反向代理避免跨域","uri":"/%E5%88%A9%E7%94%A8nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%81%BF%E5%85%8D%E8%B7%A8%E5%9F%9F/"},{"categories":null,"content":"这个五一折腾了下https，看了加密的建立过程和原理，然后动手实践，把博客从不支持https的阿里云虚机上搬到了新买的腾讯云的主机上，配好了https，这里记录一下。 加密连接建立过程与原理 这个部分不想自己写了，参见 sf 上的这篇文章就很容易理解。 ","date":"2016-05-27","objectID":"/from_http_to_https/:0:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"我的理解 https并不是一个全新的协议，而是一个组合的协议，是ssl与http组合而来的，其模型如下图 不难发现其实https就是在ssl连接的基础上对http报文进行加密后发送。通过抓包，我们不难发现这一点： 以segmentfault.com为例，用wireshark抓取发送的数据包 用http关键字进行筛选： 用ip.addr关键字进行筛选 从截图可以看出： 数据包的抓取并不能获取到http协议的任何信息，哪怕是URL也不行 通过ip地址获取到的数据包，应用层协议是SSL，在协议的报文中包含加密数据包的协议，加密协议的版本等信息，但无法获取原始报文 按照这个逻辑，我们就可以对所有应用层的协议进行加密工作，比如sftp。另外，最让码农感觉幸福的是，只要服务器做好配置，我们不需要修改代码，只需要修改接口的协议，其加密过程对于程序来说是透明的。 ","date":"2016-05-27","objectID":"/from_http_to_https/:1:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"letsencrypt Let’s Encrypt 是一个提供免费SSL证书的项目，托管在github上： https://github.com/letsencrypt/letsencrypt 有了这个项目，我们可以方便的把自己的站点升级成为https 然而，这个项目提供的证书默认有效期是 90 天，我们需要定期的更新证书，或者写个脚本，定时执行。 实战 ","date":"2016-05-27","objectID":"/from_http_to_https/:2:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"准备工作 在获取证书的时候，需要使用 80 和 443 端口，所以需要先关闭占用这两个端口的程序 # systemctl stop nginx.service ","date":"2016-05-27","objectID":"/from_http_to_https/:3:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"获取letsencrypt # git clone https://github.com/letsencrypt/letsencrypt # cd letsencrypt # ./letsencrypt-auto -h 最后一个命令会帮助解决项目的依赖问题 ","date":"2016-05-27","objectID":"/from_http_to_https/:4:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"获取证书 上一个命令结束后会打印出一些使用的帮助信息，可以根据帮助信息选择需要的参数。 因为我的站点部署在nginx上，所以使用如下命令： # ./letsencrypt-auto certonly --standalone --email melo@gaoyuexiang.cn -d gaoyuexiang.cn -d www.gaoyuexiang.cn 如果得到如下信息则说明证书已经正确获得了： IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at /etc/letsencrypt/live/gaoyuexiang.cn/fullchain.pem. Your cert will expire on 2016-08-01. To obtain a new version of the certificate in the future, simply run Let's Encrypt again. - If you like Let's Encrypt, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le ","date":"2016-05-27","objectID":"/from_http_to_https/:5:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"配置nginx nginx的配置能够在网上找到很多资料，这里我就把我的与ssl相关的配置贴出来 #https配置 server{ listen 443 ssl; root /opt/wordpress; ssl_certificate /etc/letsencrypt/live/gaoyuexiang.cn/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/gaoyuexiang.cn/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers AES256+EECDH:AES256+EDH:!aNULL; ... } # 强制使用https server { listen 80; server_name gaoyuexiang.cn; #强制将 http 访问转发到 https rewrite ^/(.*) https://$server_name$1 permanent; } 配置完成，启动nginx和php-fpm # systemctl start nginx.service # systemctl start php-fpm.service 然后访问自己的站点，发现已经能够强制使用https了。 wordpress还有一些设置需要调整，把以前使用http链接本站资源的地方给改过来，如站点URL、主题使用的图片等信息 ","date":"2016-05-27","objectID":"/from_http_to_https/:6:0","tags":null,"title":"从http到https","uri":"/from_http_to_https/"},{"categories":null,"content":"记录一些想写的东西。 GitHub: https://github.com/kbyyd24 Email: blog@gaoyuexiang.cn 欢迎关注公众号： 在 /thoughtworks 工作，有兴趣找我内推 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"友链还没加。。。 ","date":"0001-01-01","objectID":"/links/:0:0","tags":null,"title":"","uri":"/links/"}]